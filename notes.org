#+TODO: TODO | DONE NOPE
#+BIBLIOGRAPHY: refs


* Tasks
** DONE Setup export of manuscript [5/5]
CLOSED: [2015-10-13 mar. 15:24]
- [X] Reorganize outline (exported) vs. meta-concerns (not-exported)
- [X] Generate PDF in separate directory
- [X] Create makefile for batch export
- [X] Use memoir or classic-thesis template
  Book will do for now.
- [X] Export citations to biblatex refs

** DONE Acknowledgements as front matter
CLOSED: [2015-10-13 mar. 17:09]
Bibliography should also be unnumbered.

** DONE Babel does not process in batch
CLOSED: [2015-10-13 mar. 17:20]
DOT source blocks are exported rather than their output.  Probably should allow
Babel to execute DOT without confirmation.

** DONE PNG generated by DOT are too large
CLOSED: [2016-05-12 jeu. 17:01]
Maybe related, but I should probably use SVG instead if I want diagrams at all.

** DONE Re-organize contents [1/1]
CLOSED: [2016-05-12 jeu. 17:01]
Table of contents is a mess.  Should refactor the tree to redistribute weight.

*** DONE Organize state of the art
CLOSED: [2016-02-01 lun. 16:59]
Any partition will be lacking in some respect, but no partition looks worse than
an unsatisfactory one.

Here’s a partition I can live with:
- Une chronologie de la modularité dans les langages de programmation

  Ratisse large pour contextualiser et retracer les étapes.

- Moyens techniques pour l’instrumentation

  Instrumentation, ou un meilleur terme pour désigner ce qu’on veut faire avec
  un programme: l’étendre après sa conception.

  Partie focalisée sur les moyens existants d’attaquer le problème.

- Travaux connexes

  Peut-être pas directement utilisables pour l’instrumentation, mais devraient
  être mentionnés parce qu’ils ont des désirs similaires.

** DONE Translate ‘Listings’ captions to French
CLOSED: [2015-10-29 jeu. 15:17]

** DONE Fix word choice program/process/source code
CLOSED: [2016-07-01 ven. 13:41]
Dijkstra uses “program” to mean program text (source code), but Naur rather uses
“program” to mean what Dijkstra calls “process” (what the machine actually does
when running the binary).

In the Art of the Intepreter [[cite:SS-78][SS-78]], the distinction is explicit:

#+BEGIN_QUOTE
This is the first place where we find it crucial to distinguish the three ideas:
(1) the /program/ — the text describing a procedure, e.g. in the form of an
S-expression; (2) The /procedure/ which is executed by the computer; and (3) The
mathematical /function/ or other conceptual operation computed by the execution
of the procedure.
#+END_QUOTE

For casual usage, “program” denoting at the same time the text and the process
is fine.  The text /describes/ the process, and the two are the same.  But,
sometimes, we want do distinguish them.  I like the program/process distinction
the most, but it would be best to avoid using “program” at all, since it can be
ambiguous.

code/process would work.

Or, I can explicitly state that “program” refers to the program text.  But then
I lose the ability to express both meaning at once.

** DONE Explicit distinction between late binding and dynamic binding
CLOSED: [2016-08-05 ven. 17:27]
For me, dynamic binding is what happens for ~B.m~ to be called here:

: A a = new B()
: a.m()

Or is that dynamic dispatch?

I take late binding as encompassing any mechanisms that delays the evaluation of
names (methods, but also identifiers, or even control flow operators).

Dynamic binding \in late binding.

Dynamic scoping is late binding, because the meaning of a dynamically-scoped
identifier cannot be known until the code is run (and can change multiple times
in the same execution).

A meta-object protocol is late binding in the sense that how an "Object" behaves
is not statically fixed, but can change at runtime.

There is a real difference in dealing with languages that have late binding
features.  Calling ~eval~, or having late-bound closures.

But maybe it's not the right term.

** DONE Do we have enough epigraphs?
CLOSED: [2016-09-14 mer. 13:25]
At least two quotes, one from a proponent, one from an opponent?

- Structured programming:  Dijkstra to nuance Knuth derision?
- OOP: Kay?

** DONE RSVG does not export text on paths
CLOSED: [2016-08-02 mar. 17:02]
There seems to be an [[https://bugzilla.gnome.org/attachment.cgi?id=270767&action=diff][unmerged patch]] to add basic support.  Might test that.

Or do not use text paths.  That would require changing the "similar" diagrams
of the section to keep the common theme.

** DONE Check ~with~ trick with ES6 implementations
CLOSED: [2016-09-14 mer. 10:55]
Chrome is up-to-date.  Babel might also be worth a look (i.e., shouldn't change
a character).

And change the note in the manuscript to indicate whether that still works or
not (or necessitate function expressions).

Works in Chrome 53.0.2785 now.  HAHA!

The [[https://bugs.chromium.org/p/v8/issues/detail?id=686][related Chromium issue]] was not updated, but at least I can say it works.

** DONE Use memoir instead of KOMA
CLOSED: [2016-06-10 ven. 18:46]
** DONE No first indent with RaggedRight?
CLOSED: [2016-06-10 ven. 18:46]
Paragraphs are not indented, which is confusing.  Might be caused by
RaggedRight.

Will probably disappear when switching to memoir anyway.

** DONE Fix interprète/interpréteur mix
CLOSED: [2016-07-28 jeu. 16:28]

** DONE Define "module"
CLOSED: [2016-08-12 ven. 19:38]
An independent unit of behavior in a program?

** DONE Define "recomposition"
CLOSED: [2016-08-12 ven. 19:38]
Re-arranging a program's functional units within the composition units
(i.e. module) of the programming language.

** NOPE Fix French typography in HTML export
CLOSED: [2016-09-01 jeu. 17:41]
There is currently no space before ':' or '?'.  There should be space.

The issue is that under LaTeX, Babel will take care of adding the space.  The
documentation mentions that it is better to stick to English typography
conventions even when writing in French, and leave the typography to Babel.

Trouble is, HTML is not that smart.

Not fixing because it would be too much work for a little gain.  If you care
about typography, read the PDF!

** DONE Add section to present JavaScript and its features
CLOSED: [2016-07-18 lun. 17:41]
Netscape hired Brendan Eich under the promise of “doing Scheme” in
the browser.  Working at Silicon Graphics, he had been introduced
to SICP by Nick Thompson.  However, upper management imposed that
the language had to “look like Java”, so Perl, Python, Tcl and
Scheme went out of the window.

Netscape wanted to embed a programming language in source form in
HTML.  When Java came out, Sun and Netscape agreed to include Java
in the browser.  Java would be used by component authors, and
JavaScript (or “Mocha” as it was then named) would target
scripters.

Prototyped in 10 days in May 1995, the language shipped with
Netscape Navigator 2.0 in March 1996.  It quickly gained
popularity, and Microsoft added a retro-engineered version in IE 3
(August 1996) called JScript.

Later, standardization as ECMAScript in June 1997.

Now, lingua franca of the web.  Ranked #10 on TIOB as of July 2013, but first on
Github.  Also most searched for on StackOverflow, but does this translate to
popularity or obscurity?  See [[http://www.itworld.com/cloud-computing/364194/wait-maybe-javascript-top-programming-language][this article]] for more numbers.

(though increasingly seen as a compilation target, not a language you write in)

Inspired by Awk, C, HyperTalk, Self (prototypes), Scheme (first-class
functions), Perl (for regexps).

Management wanted it to “look like Java”, but the Date class came
along, as well as distinction between primitive vs object.

Dynamic typing
Objects as dictionaries
Prototype delegation (compare to Dony classification, have notes on that)

** DONE Word choice: client code / code interprété
CLOSED: [2016-07-21 jeu. 17:41]
Client/host code is the terminology I've gotten from Narcissus.  Client code
refers to code that is executed /by/ Narcissus, and host code to the code /of/
Narcissus that is executed by V8 (or any other JS engine, including Narcissus,
which gets confusing fast).

The distinction is only useful when discussing the metacircularity.  In other
cases, readers might be more familiar with "interpreted code", without
(hopefully) confusion between program code and Narcissus code.

Look out for that when discussing Narcissus and faceted evaluation
instrumentation.

** DONE Describe interpreter pattern from GOF?
CLOSED: [2016-08-05 ven. 17:27]
** DONE "environment" notion undefined?
CLOSED: [2016-08-12 ven. 19:38]
** DONE Appendix: full code for FOAL example
CLOSED: [2016-07-28 jeu. 16:23]
To be able to see that it is append only.  Also, link to s3c.

** DONE Appendix: why "proto: this" in FOAL examples
CLOSED: [2016-08-09 mar. 18:42]
Because it's of little consequence for the FOAL argument.

But it's interesting.

And necessary if you wanted to fully understand the code.

And shows my interest in object construction.

And gives another reason for an appendix.

** DONE Add motivation for using dynamic flow analyses to JS interps
CLOSED: [2016-07-28 jeu. 19:09]
aka, what we were paid to do.

** DONE Context needs a recap section
CLOSED: [2016-08-04 jeu. 17:33]
The conclusion should be that all mechanisms for more flexibility revolve around
indirection and context manipulation.

Therefore, we want to see how they hold up when extending a JS interpreter.
That's the next chapter.

** DONE Add sources for citations where they are not evident
CLOSED: [2016-09-16 ven. 00:02]
Done up to reflection.

** DONE Remove em dash in quotes, they are superfluous
CLOSED: [2016-09-01 jeu. 17:54]
And they cause us to use non breaking space that LaTeX does not understand.  (We
could use an nbsp macro).

** DONE Hunt "permet" and superfluous "peut permettre"
CLOSED: [2016-08-03 mer. 18:21]
** DONE Sectioning: maybe more unnumbered nodes
CLOSED: [2016-08-05 ven. 17:47]
Make problem/mechanisms distinction more proeminent in Context chapter using
unnumbered sections.

** DONE Quell annoying python warning when exporting
CLOSED: [2016-08-05 ven. 17:54]
Can probably set the damn variable in export-setup.

** DONE Acronyms: good use for margins
CLOSED: [2016-08-05 ven. 17:32]
CLOS, ORM, AOP, RPC.

** DONE Epigraphs can be a bit opaque without an explicit name
CLOSED: [2016-08-05 ven. 17:44]

** DONE (pdf) minimal syntax highlighting with listings
CLOSED: [2016-08-30 mar. 18:55]
Just distinguish comments from code, as in the HTML version

** DONE (pdf) figures always take the full margin width
CLOSED: [2016-08-29 lun. 19:16]
CSS is max-width.

** DONE Proper TOC in introduction
CLOSED: [2016-09-12 lun. 16:00]
** DONE (pdf) multi-pages listings are not okay with two-sides
CLOSED: [2016-08-31 mer. 18:33]
Seem to be ok.  Will have to
** DONE ensure there are no dangling FIXMEs in the document
CLOSED: [2016-09-14 mer. 10:52]
** DONE appendix should be backmatter
CLOSED: [2016-08-31 mer. 18:32]

** DONE GO TO, goto, or GOTO?  COME FROM or COMEFROM?
CLOSED: [2016-09-01 jeu. 18:01]
** DONE remove HRules, they are too weird for the PDF
CLOSED: [2016-09-01 jeu. 17:46]
** DONE (pdf) "Références" appears as empty chapter followed by "Bibliographie"
CLOSED: [2016-08-31 mer. 18:25]
** DONE (pdf) Nofirstindent after section, figures?
CLOSED: [2016-09-12 lun. 16:00]
That's actually the french typographical standard.  I'm fine with leaving as is.

** DONE (pdf) listing captions should go in the margin
CLOSED: [2016-08-31 mer. 18:32]
** DONE (pdf) covers in the doctoral school fashion
CLOSED: [2016-09-01 jeu. 15:52]
** DONE (pdf) cut code examples that go over the textwidth
CLOSED: [2016-09-01 jeu. 18:45]
** DONE (pdf) English babel for epigraphs?
CLOSED: [2016-09-01 jeu. 17:42]
Maybe turn off the quotes export for LaTeX, to let LaTeX handle it properly.

** DONE (pdf) Bibliography header not aligned with the main matter
CLOSED: [2016-09-01 jeu. 19:40]
Have to fiddle with geometry.

** DONE (pdf) fix order of frontmatter
CLOSED: [2016-09-01 jeu. 19:34]
Should go:

cover -> frontmatter -> acknowledgments -> abstract -> toc -> mainmatter

Requires to extract the abstract (or better yet, skip it).

** DONE (pdf) Fix headers
CLOSED: [2016-09-14 mer. 14:16]
Don't need chapter number.  Maybe change font.

Introduction has "Table des matières" in headers...

** DONE Add colophon
CLOSED: [2016-09-15 jeu. 13:47]
HTML and PDF?

** DONE (page-words) Put page words in biblatex backrefs
CLOSED: [2016-09-15 jeu. 14:47]

** DONE Check MathJax config for HTML
CLOSED: [2016-09-16 ven. 14:55]
The first rendering is nice enough, but the second one is ugly.  Can't I just
get the first one?

** DONE (page-words) Sort by proper lexicographical order
CLOSED: [2016-09-16 ven. 10:57]
That handles accented letters.

** TODO (pdf) multi-cites are not fused together
Probably because we do not use ox-bibtex at all there.  But maybe it will break
everything.

** DONE (html) remove TODO tags
CLOSED: [2016-09-19 lun. 11:30]

** DONE (pdf) go over each page one last time and fix everything
CLOSED: [2016-09-16 ven. 00:02]

** DONE (page-words) Add a notice on page words above colophon
CLOSED: [2016-09-16 ven. 13:40]

** DONE Tag version sent to reviewers
CLOSED: [2016-09-19 lun. 11:34]

** DONE Build a print-ready version (no hyperlink color)
CLOSED: [2016-09-19 lun. 11:40]

** TODO (hmtl) Bibliography style is the same as in PDF
Namely, the HTML style downcases titles.  Grrr.

** DONE Add SecCloud & Cominlabs acknowledgements
CLOSED: [2016-09-19 lun. 22:10]
** TODO Explain diagram language when they first appear (chapter 4)
** TODO Explain the grey in diagrams in 6.2
** DONE (defense) fix slide numbers in overview
CLOSED: [2016-11-06 dim. 17:28]
** DONE (defense) incremental overlay should start with first item, not empty
CLOSED: [2016-11-06 dim. 17:28]

* Log
** [2015-10-08 jeu.]
*** Getting serious
Organizing the Org file manuscript.  I initially wanted only one document mixing
meta-concerns and content, but it seems less troublesome to just have one Org
file for the exported manuscript.  I’m sure that taming the Org export for /one/
file only will have its fair share of challenges.

Using the classic-thesis.sty file is not sufficient.  It seems all kind of
broken with the book document class.  It seems the preferred way would be to use
the full bundle of classic thesis.  However, it requires splitting the document
into many TeX files.

Looking into how to split the Org file in several LaTeX files during export.
This is difficult, because Org 8.3 errs when a subtree links to other sections.

** [2015-10-12 lun.]
*** Deciding on the pipeline to produce the PDF
I just want a simple pipeline: write only the Org document.  Worry about the
final details of the presentation /later/.  But, I still need to export to get
an idea of how it fits in terms of length, and to send drafts to my advisors.

So, a working export system would:
- export to PDF
- hyperlinks point to the right location
- bibliography is exported using biblatex
- overall document layout is “good enough”
- generate export files in an =output= directory

all with minimal (ideally none) LaTeX-specific hacks into the Org document.  And
of course, I’d like to avoid editing the TeX /at all/.

*** HTML export
HTML export seems mostly fine.  I do not actually intend to use it, since the
preview of Org on Github is alright (especially with the inline TOC generated by
Org).  Couple of issues:

- SVG generated by LASSY’15 code is not recognized (missing XML namespace
  maybe?)
- Citations link nowhere.

*** LaTeX export
Haha.  First trouble is UTF-8.  The verbatim environment barfs on UTF-8 chars.
Changing to ASCII is not a solution.  Trying with =xelatex=, but Org outputs
packages for =pdflatex= (inputenc, fontenc ...).  Manually editing the TeX to
remove the offending packages works.  Only ~fontspec~ is needed.

Org tries to include SVG with the ~includesvg~ I assume comes from the ~svg~
package.  Does not seem to work outright even including the package.  I comment
the offending SVG files in the Org document, as I’m not sure I need them right
now.

Aside from overfull/underfull hboxes, it compiles.  UTF-8 chars show up in
verbatim environment with a monospace font that supports them (e.g., Ubuntu
Mono).

Quirks:
- Included PNGs are too large.
- Bibliography is missing.

I created a Makefile to produce the TeX from Org, and the PDF from the TeX
with Latexmk.  This saves me from regenerating the whole document when I want to
change the LaTeX (around 6 seconds right now mostly due to Babel I guess).

** [2015-10-13 mar.]
*** Setting the bibliography properly
Use a style that works for now.

*** Setting up the structure for processing the PDF
Trying to specify “a4paper” option outside of the documentclass macro, to put
all changes in =preamble.tex=.  Actually, I can omit documentclass in
~org-latex-classes~ and put it in =preamble.tex= directly.  That saves me from
messing with =export-setup.el= for LaTeX-y stuff.

Setting the front/main/back matter correctly without polluting (too much) the
Org document.  The compromise I’ve opted for is to create =frontmatter.tex= and
=backmatter.tex= to take care of finer presentation details.  That leaves room
to grow for a custom title page.

Two downsides to the approach:
1. Acknowledgments must be defined outside the Org document.  Just because I
   cannot set it as a chapter in LaTeX without setting it to a level 2 heading
   in Org, which would be absorbed by the previous level 1 heading.

   Maybe I can define it as a level 1 heading, no export, but then do a subtree
   export for this node only in a separate file.

   Ok, I just did that instead.  Works, with the issue of linking to other parts
   of the document.  Don’t need that for now, and I prefer the Acknowledgments
   stay in the document.

2. Three lines of LaTeX in the Org document.  The first two at the top are okay,
   the problematic one is the third.  It should be the last line of the Org
   document, but is part of the last heading instead.  When moving headings, it
   can be forgotten.  There is probably a better way.

Allowing Babel to run in batch fixed the issue with non-loading images.

*** Trying out classicthesis
Trying out the ‘classicthesis’ package; it’s all kind of broken.  Going to stay
with the default book class for now, then maybe later look into KOMAScript or
memoir for further customization.

** [2015-10-16 ven.]
*** Organizing the background section
Since the section is intended to be a map of manipulating programs, present the
pipeline from source to side effects in the computer.

** [2015-10-19 lun.]
Starting from the start.  What is a program?

** [2015-10-20 mar.]
*** Monologue about scope of introduction
- Why do you talk about programs?  I thought this was about extending interpreters.

- An interpreter is a program like any other.  To extend an interpreter, we can use
  the same mechanisms we would use for extending a program.

- Ok, go on.  Wait, why talk specifically about interpreters then?  Doesn’t your
  work apply to any program?

- Well, the mechanisms would apply to any program, yes.  But they are more
  tailored for use in interpreters.  However, I find it important that I relate
  the mechanisms used in the specific instance of interpreters to the larger
  family of mechanisms used for extending generic programs.  The background
  section deals with this larger family, while the core section focuses on the
  instance of interpreters.

- Will you not lose time discussing related work that could not apply to
  interpreters, although they are viable for generic programs?

- Such work can be dismissed on the grounds you highlight.

*** Monologue, again
- So, where are you going with this explanation of what a program is?

- I want to show how a program is executed.  Because we want to modify programs,
  we need to know how they run.  There’s a whole pipeline, from source code to
  machine code, that takes the program and transforms it into an increasingly
  larger file with a lower instruction count.  But it’s still the same program.
  So if you want to modify the program, you now have several places where you
  can do it.  They all have compromises: the source is easy to modify, and
  corresponds to the object the programmer knows, but you might not have access
  to it.  The binary is always accessible, but it might be too low-level as it
  has lost structuring information that the programmer knows (names, modules,
  classes ...).

** [2015-10-22 jeu.]
*** Using listings for exporting Org source blocks
Captions on source blocks are broken.  The source block is inserted in a
verbatim environment, and the caption with a ~captionof~ command.

Maybe trying to export with the ‘listings’ package will take care of captions?

It did.  I just had to enable listings as the environment for source block via
an ELisp variable, and add the package in the preamble.  The Org export sets the
‘language’ option of listings environment to the language of the source block,
so I have to define these languages for listings in the preamble as well.

And source blocks can contain references to line numbers through Org, cleanly
(see [[info:org#Literal%20examples][info:org#Literal examples]]).

*** Neologism
Oh yes, ‘[[https://fr.wiktionary.org/wiki/exot%25C3%25A9rique][exotérique]]’ is a word.  Not that I would not use it if there was no
agreed-upon definition.

*** Trying out side footnotes
They might need a bit more space.

** [2015-10-23 ven.]
*** Footnotes in the margin, figuring out the style
Margin footnotes using =footmisc= have two issues:
1. Ragged side does not follow side of margin (always ragged right).
2. There’s an indentation at the start of the footnote.

Trying the =marginnotes= package.  The fact that footnotes are not floats is a
feature, but will not do for my use case.  Also, they are not numbered.

After toying for the last hours, I think I’d like to have both footnotes and
(numbered) margin notes.  Margins for most of side information (notes, but also
small images, Listing captions, snarky remarks ...).  And footnotes for URLs?

Or URLs as citations, which make more sense.  All in the bibliography, and avoid
the need for footnotes.

** [2015-10-26 lun.]
*** Footnotes in the margin
Let’s try to put everything in the margin, and then URLs as cites.

- Notes ragged left or right might look better.
- Need a number or symbol.

Ok, done.  Had to write my own LaTeX command and use counters.  As I’m using
~marginpar~, there may be an issue down the road if the number of margin notes
exceeds 256.  The package =etex= apparently solves that issue.

*** URLs as citations
Since the document must be printed, external URLs are not very useful when
presented only as hyperlinks.  Rather, I treat them like other citations, which
gives a unified presentation.

The author-year citation style needs an author and a year, information that
might not make sense for a website.  I could switch citation styles, but in the
meantime I can use the ~label~ field to provide a referencing key.

*** Captions in the margin
Now I’d like to have captions of figures and listings to appear in the margin as
well.  This allow for easy parsing, and I don’t like that caption break the flow
of paragraphs.

Wow, so that was complicated, as anticipated.  Turns out that the =tufte-latex=
package does what I want.  But it is also strongly opinionated on the layout of
the text.  So I took the code, and commented out most of what I did not need,
and let it redefine the ~figure~ environment for captions in the margin.

I tried to use it for footnotes in the margin as well, but the code seems
bugged?  I kept getting an error with the ~sidenote~ command, but the
~marginnote~ worked alright.  I kept my custom margin note command.

Next step is to override the caption of ~listings~ environment.  The [[http://tex.stackexchange.com/questions/46137/combining-listings-and-tufte-book-listings-caption-on-tuftes-margin][suggested
workaround]] is to wrap a ~lstlistings~ into a ~figure~ and not skip the caption
handling of =listings=.

** [2015-10-27 mar.]
*** Listing environment caption margins
Ok, listings, captions, margin.

Got the caption in the margin.  Now I need to tell the Org LaTeX exporter to
spew out a ~figure~ environment around the listing.

** [2015-10-28 mer.]
*** Got listing environment caption margins to work
Advising ~org-latex-src-block~ did the job.

For now I just add the LaTeX commands I need in this function.  A neater
solution would be to create a new environment that takes care of wrapping the
listing in a figure, and setting the ~figurename~.  That will do for now.

** [2015-10-29 jeu.]
*** Fix figures without a caption
Empty captions caused tufte-latex to put the figure in full width.  This was
caused by the minipage for side captions having nothing in it.  I just put a
non-breaking space to fill the page as a workaround.

*** Monologue for rethinking the background introduction
— Where are you going with the compiler explanation?

— That’s how you get a program to the machine.

— Wouldn’t it be simpler to just have said “programs have different forms, from
source code to binary.  To modify a program, we can modify any of these forms.
People have done that; each form has compromises; here they are.”

— Sometimes it’s more about the journey...

— Yes, but know your audience.  For sure the two people reading your manuscript
will know all that stuff.  Your take on the basic of compilation, while
interesting, is not very relevant.  They rather want to know what you did.  The
background section is for surveying work related to the problem you want to
solve, not for getting lyrical.  Focus.  A focused manuscript is easier to write
and to read.

— But I /care/ about this stuff, and I want to ensure we all have the same
understanding of what the basics are.

— This material has a place, but maybe not in the document.  A companion, the
“lost chapters” perhaps?  Or for a course?  The fact that a program is for
machines /and/ humans is certainly relevant however, as the contributions are
certainly about the human side of programming.  But this is context, and context
is for the introduction.

— You have good points, and I will certainly heed them.  A synthetic, focused,
to-the-point manuscript has its virtues.  I think I’m afraid that going this
route will also lead to dryness.  I was also taken aback by Adrien’s defense,
where he went headfirst to solve a problem when, for me, the whole premise of
finding a “best” set of keywords was dubious.  I think I expect a researcher to
argue the merit of the problem before exposing a solution.

*** A brief history of my PhD
In my case, the task set by Jacques was “Find a better way to instrument
Narcissus than ad-hoc copy-pasting”.  That fit into the larger problem, that was
a motivation for pursuing a PhD in programming languages: how to reduce
complexity in software, and achieve Modularity (with a capital M)?

Back then, I understood clearly that the instrumentation of Narcissus was not
modular.  I had to decide what would a modular instrumentation look like, to set
a goal.

Turns out, modularity is a tricky thing.  I was in team that had ridden on the
wave of AOP, promoting “separation of concerns” and “modularity” at every turn
of a sentence, but trying out AOP for my use case I could see that it was not
quite the silver bullet.  You gained separation of concerns, but traded clarity
of code, and ease of debugging.  Was the end result more “modular”?  The meaning
of this word started to elude me.  Every solutions to the problems of AOP were
like putting makeup on a pig.  The core COMEFROM concept was both the solution
to the separation of concerns, and the cause of all the other problems.
And, AspectScript did not feel like a proper tool that anyone would use outside
of a proof of concept.

If AOP did not fit the bill, maybe another paradigm would?  Functional
programming was en vogue, and I agreed that immutability and avoiding side
effects tended to make programs clearer.  Éric and Ismael had put out an AOP
library for Haskell powered by magical monadic dust, and I was supposed to work
with them for a month.  Besides, I always wanted to take a deeper look at Haskell.

So, I tried to wrap my head around monads.  Even around category theory, as I
gathered the theory would provide a mental model for understanding monads.  I
fought with the type system daily, battling ever-stranger and increasingly
opaque error messages from the compiler.  In the end, the type system won.  I
never managed to type even a modest lambda calculus interpreter that could
change its meaning at runtime.  The near-solutions were already so convoluted
that I judged them not worth the hassle.

I turned to static code transformation.  That worked, but then it had nothing to
do with functional programming or monads.  So it was not a new solution.  And it
did not solve the problem with Narcissus.

I had still not produced any scientific contribution, and was entering my third
year.  Getting back to JavaScript, I needed to produce a working solution to the
original task.  Not an ideal solution to the problem of modularity.  Just, a
better way to instrument Narcissus.  A better way that would also be worthy of a
publication.

In JavaScript, the most elegant and simple mechanism I could find that would do
the job was dynamic scoping.  Except JS had no dynamic scoping.  Also, I found
the idea that dynamic scoping could be legitimately /useful/ rather amusing,
especially as it is often viewed as conflicting with modularity.  I found a way
to simulate dynamic scoping in JavaScript using ~with~.  Since that was mostly a
trick, I put a workshop paper showing how to build an interpreter with dynamic
layers.  Then I tried to use the same technique for Narcissus, and ended up
using another trick, still based on ~with~ and dynamic scoping.  It’s not
particularly novel.  It’s still a trick.  I’m not sure it warrants publication.
But it did solve the problem in a new way.

For some definition of solved.  What I gained was the ability to change the
interpreter dynamically, with separation of concern and single point of truth.
However, modularity is violated since we are touching private code from outside
the module.

Turns out, there is no silver bullet alright.

So I’ve contemplated, and found that maybe trying to find a solution using the
language was not the right idea.  Separation of concerns is a presentation
issue, something well put by Akram’s poster.  Every language raises the issue of
primary decomposition (the expression problem being a prime example).  Solving
it /through/ the language only complicates the code.

That’s why something like literate programming and WEB is appealing.  It’s on
top of any language.  You can build the program from named blocks, that can
appear in multiple places.  So you have full flexibility of presenting the
program structure in any way you like for a maintainer, while generating a
“linear” version to the machine.  The program has dual, conflicting roles, and
we cannot resolve this tension by manipulating only one view of the code.  With
multiple views, we can satisfy all interested parties.

It just has to be as natural and painless as writing navigating plain text.

** [2015-11-03 mar.]
*** On the COMEFROM trail
What I’ve been trying to do, extend interpreters from other files, amounts
essentially to a COMEFROM statement.

The COMEFROM statement has originated as a joke statement in FORTRAN and other
programming languages of the early ‘70s, as an answer to the “GOTO considered
harmful” paper by Dijkstra.

[[cite:Cla-73][Cla-73]] proposes COME FROM.  [[cite:SM-78][SM-78]] goes to 11 and proposes UNLESS, DON’T, UNTIL,
IF MAYBE...  Both are humorous.

With the same title of “Structured programming considered harmful”, but in
definitely serious tone (coincidentally 3 years earlier in the same
publication), [[cite:Abr-75][Abr-75]] attacks the sociology of structured programming.

*** Auto spacing in Babel frenchb
Making a note of the fact that then frenchb package of Babel by default inserts
spaces before the punctuation signs =; : ? !=.  It means I should /not/ put any
space (follow English typographical conventions) in the Org document.

** [2015-11-04 mer.]
*** Abrahams’ pragmatic defiance against structured programming
He argues that blindly following the “dogma” of structured programming does not
lead to clear and correct programs.  Structured programming is based on false
assumptions, and goes through them.

First assumption is the belief that one can write a correct program in a
top-down fashion, in stepwise refinement of a tree structure.  But programs are
not trees, and even in those exhibited by Wirth or Dijkstra we find lattices.
From my own experience, it is certainly a dual approach.  A pure top-down does
not work unless you have already solved all the problems of representation.
Abrahams argues that a readable and efficient program is more the product of a
skilled programmer than the result of following the rules of structured
programming.

#+BEGIN_QUOTE
If the prime number program, for instance, had been spontaneously invented by
one of Dijkstra's weaker students after taking a course in structured
programming rather than by Dijkstra himself, the case for structured programming
would be more convincing.
#+END_QUOTE

In the “Pragmatics” section, the attitude of Abrahams is opposed to that of
Dijkstra:

#+BEGIN_QUOTE
But we must accept the fact that we live in the world that is, not the world
that we would wish to live in or even the world that we will in the future live
in.
#+END_QUOTE

He then goes on to say that alternative constructs to GOTO (like LOOP) may not
be available in the language you must use.  GOTOs are efficient, while a
procedure call is not.  Adding a GOTO is more convenient than rearranging all
your punched cards.  And some programs are not meant to be “graven onto bronze
tables, to be preserved for future generations”; “fastest is best”.

He finishes by quoting Knuth [[cite:Knu-74][Knu-74]]:

#+BEGIN_QUOTE
There has been far too much emphasis on GO TO elimination instead of the really
important issues; people have a natural tendency to set up an easily understood
quantitative goal like the abolition of jumps, instead of working directly for a
qualitative goal like good program structure.
#+END_QUOTE

*** Knuth’s reasonable position on GOTO
Re-reading [[cite:Knu-74][Knu-74]] (at least I think I’ve skimmed through it at least once, but
some parts seem new to me).

#+BEGIN_SRC js :results output
var A = [1,2,3,4,5,6,7,8,9]
var B = [0,0,0,0,0,0,0,0,0]

function findOrAppend(x, A) {
  var i = 0;
  var m = A.length;
  while (i < m && A[i] !== x) ++i
  if (i < m) // found
    ++B[i]
  else {
    A[i] = x
    B[i] = 0
  }
}

findOrAppend(1, A)
print(A, B)
findOrAppend(3, A)
print(A, B)
findOrAppend(-1, A)
print(A, B)
#+END_SRC

The paper seems to argue that some GOTOs are harmless and even useful, if you
care about counting cycles.  Using labels can make getting out of a loop more
readable.  Syntactic sugar can take the place of GOTOs, the LOOP ... THEN
construct is a nice example that somehow never made it into mainstream
languages.  Error exits are also a legitimate use case, and I believe exception
throwing and catching is the non-GOTO replacement.

But, it touches also on the more general concern of structured programming, in
the sense of crafting programs that must be simple to read and understand, but
must also be efficient when it counts.  Focusing on GOTO is the wrong issue.
Finding better abstractions, better way to deal with large programs is.

Comments notes invariants.  Of loops, but also of variables (“m is the number of
items in the table”, ...).

The whole thing still rings true after 40 years, especially the conclusion
section.

“The Future” is also prescient:

#+BEGIN_QUOTE
We will perhaps eventually be writing only small modules which are identified by
name as they are used to build larger ones, so that devices like indentation,
rather than delimiters, might become feasible for expressing local structure in
the source language.
#+END_QUOTE

We essentially do that with modules.  And languages like Python and Haskell use
only indentation to delimit blocks.

#+BEGIN_QUOTE
Although our examples don't indicate this, it turns out that a given level of
abstraction often involves several related routines and data definitions; for
example, when we decide to represent a table in a certain way, we simultaneously
want to specify the routines for storing and fetching information from that
table.  The next generation of languages will probably take into account such
related routines.
#+END_QUOTE

Again, this is abstract data types.  Though for that one maybe at the time some
people were already toying with the idea.  I thought Reynolds was credited with
the concept around that time.

Then there is this vision of “program manipulation systems”:

#+BEGIN_QUOTE
Program manipulation systems appear to be a promising future tool which will
help programmers to improve their programs, and to enjoy doing it.  Standard
operating procedure nowadays is usually to hand code critical portions of a
routine in assembly language.  Let us hope such assemblers will die out, and we
will see several levels of language instead: At the highest levels we will be
able to write abstract programs, while at the lowest levels we will be able to
control storage and register allocation, and to suppress subscript range
checking, etc.  With an integrated system it will be possible to do debugging
and analysis of the transformed program using a higher level language for
communication.  All levels will, of course, exhibit program structure
syntactically so that our eyes can grasp it.
#+END_QUOTE

While enticing, one part is over enthusiastic.  We do have piles of languages
now, but we have certainly lost control over the code executed by the machine in
the process.  Even C code directives for inlining or register allocations are
routinely discarded because “the compiler knows best”.

But maybe Knuth has realized that himself.  The language of the future was
slated for 1984.  This is the year Knuth published the literate programming
system.  And it seems literate programming is the solution he prefers, to
organize parts of the program in the order he wants to expose them, rather than
adhering to a strict top-down or bottom-up approach.  In particular, he can
still touch the low-level stuff, but put them in a dedicated section.

*** The thesis I wanted to write
It is evident that finding better ways to organize programs has motivated
programmers for a long time.  AOP, in this larger history, may look like a
passing fad.

AOP has two parts: the motivation, and the mechanism.  They realized that some
programming concerns were orthogonal to the main purpose of the system.  Thus,
they argued this orthogonality should be reflected in the structure of the code.
The mechanism is essentially a powerful COMEFROM statement.

Programs have two roles: to be executed by the machine and to be read by other
humans.  Program for the machine, we write assembly code with an optimal use of
registers and without redundant instructions.  Programming for the humans
however, we make our code match the problem domain by using the right level of
abstraction.

These two roles are at odds with each other.  Higher level of abstraction have
meant lower control on the exact produced code.  And focus on machine code means
using very basic constructs (jumps) that humans have difficulties making sense
of.

I’ve come to realize the thesis I wanted to write was about how to organize
programs.  I’ve reviewed AOP, type systems, model driven engineering... in the
hope of finding the true path to well-crafted programs.  I did not find that
path, and was enlightened.

** [2015-11-05 jeu.]
*** Drafting the introduction
But not finding the path is not a very interesting story to tell.  There are
ways to organize programs in order to separate concerns.

- Introduction
  - Programs are for machine to execute, but also for humans to read.
  - Machines only need bits.  We optimize for program size and program
    efficiency.  Programs should have just the minimal number of instructions
    required to carry their job, and make the most use of system resources
    (registers, cache, IO, ...).
  - Humans need words, structures, and value readability and extensibility.  The
    program should be decoupled into small, re-usable components that can be put
    back together.  Because correctness is important, and creating correct
    programs is hard, it is common sense to reuse correct that is proven to
    work.  Code is correct only with respect to a set of assumptions, implicit
    or explicit.  If a code assumes less, it can be used in more situations.
    Therefore, small components are preferred to build software.

  - Program, procedure, function (conceptual operation) distinction [[cite:SS-78][SS-78]].

    A program has a function, a utility.  It makes something happen.  This is
    what the user of the program cares about, and the job of the programmer is
    to instruct the machine to make things happen for the user.  From a user
    point of view, correctness, reliability and efficiency matter most.  The
    software should make things happen without surprises, not crash, and react
    to user input.

    Software is an incarnation of a conceptual system.  A text editor acts on
    text.  A browser displays web pages and executes scripts.  A paint
    application fills a canvas with brushes.  These are conceptual operations.
    Not working on a machine.  But act on an abstract model of the software.  A
    text editor provides operations to manipulate a list of characters.  A paint
    application manipulates a 2D array of pixels.  Conceptual operations can get
    away with infinite resources, and not worry about the ‘how’, but focus on
    the ‘what’.

    A program is just a text string.  A collection of text files that must be
    processed by an interpreter or compiler.  From the program, the instruction
    carried out by the computer are generated.  But the program is not the
    procedure.  Trivially, using a different compiler would yield a different
    procedure.  Or compiling for a different machine.  The point is, the
    programming language usually abstracts the machine hardware, because we want
    to write code that matches the conceptual operations more closely than it
    matches the machine code.  If the code matches the conceptual operations, it
    becomes easier to check that it does indeed make the right things
    happen—that it’s a correct translation of the conceptual operation into
    computation.

    The procedure is what is really executed by the machine.  Usually, the
    programmer has only an indirect control over the procedure.  That’s what
    makes debugging fun.  You are trying to understand why the machine blurted
    out an error, and errors are usually reported at one level of abstraction
    below the one you express your program.  You get segfault without dealing
    with segments.  You get syntax errors about a missing token, but deal with
    functions and classes.  But the procedure is the ultimate truth.  The
    conceptual operations, the program, none of them are executable directly.
    Only the procedure is.  A common analogy of a program is cooking recipe.
    You can’t taste recipes.  Even if the recipe is well-written and easy to
    follow, the end meal might be atrocious.

    So a program is really part of all that.  And a programmer must take all
    those aspects into account.

    Even if we are mainly concerned with simplifying the programmer’s job, it
    must be with respect to all the other aspects of programming.

*** Retaining performance control with language abstractions
Could not quite put the finger on why abstraction could not be reconciled with
precise control over performance while reading [[cite:Knu-74][Knu-74]] yesterday.  Take numbers.
In C, we declare what type of number we want, depending on their expected use.
ints, floats, shorts.  Operations on those numbers always have a clear and
simple translation to assembly.  The compiler can even do some assembly tricks
for you, like turning a multiplication by 2 into a left shift.  This is no
abstraction at all, and you have a fine control over performance (even the
ability to revert to assembly).

In Haskell, we want to deal with numbers that behave like those from
mathematics.  By default, all numbers are infinitely large integers (provided
memory is sufficient).  With this abstraction, no need to worry about int size.
But no way to tell the compiler either that we only need 4 bits for a number,
and that we can directly use ADD to sum them without worrying about overflow.
To regain control over performance, you can use Int types, but that requires
foregoing the abstraction.

In JavaScript, all numbers are floats.  No integers.  And no way to tell the
compiler it can safely use an int.  So all operations must be use the slower
floating point module.

Using an abstraction, you avoid specifying the details that were abstracted
away.  But you cannot, at the same time, have fine control over those details.

Maybe, however, this is not a fatality.  In Common Lisp, annotations are
sufficient to help the compiler with optimizations.  I guess, if the annotations
are not good enough for your purpose (and that’s always the case for someone),
you would want the compiler to be extensible enough that you can add your own.

** [2015-11-09 lun.]
All this reflection indicates is that a synthetic manuscript would be much
easier to write, and to follow.

Sketching a new, synthetic, outline.

** [2015-11-09 lun.]
Synthetic outline.

*** Introduction
**** Problème: étendre un interprèteur par de multiple analyses
- Contexte de sécurité web.
- Scripts de pages web passent par un interpréteur.
- Sécuriser un script = analyser ses fonctionnalités
  - runtime monitoring
  - access control
  - logging
- Une analyse dynamique = une modification de l’interpréteur
- Modification du code source en conflit avec la séparation des préoccupations
  - perte d’extensibilité, perte de lisibilité, difficulté de maintenance...
- Ajouter une analyse devrait être simple
  - sans requérir des modifications invasives de l’interpréteur
  - maximiser la flexibilité, minimiser le coût d’adoption
- Ajouter une analyse ne doit pas impacter la lisibilité du code de l’interpréteur
  - séparation des préoccupations
- Analyses peuvent se composer sans effort particulier (lorsqu’elles
  n’interfèrent pas entre elles)

**** But: mécanismes pour étendre simplement un interpréteur en préservant la séparation des préoccupations
- Trouver des mécanismes, des constructions (patterns) pour étendre des interpréteurs
- Améliorer la situation

**** Portée: interpréteurs en JavaScript
- Mécanismes et patterns génériques, pas nécessairement liés à un langage
  particulier.
- Software engineering
- Point de vue du programmeur
  - Travail sur le code source, l’éditeur de programmes, les outils du programmeur

**** Survol

** [2015-11-10 mar.]
Introduction is focused.  Now on to the background.

*** Background
**** État de l’art
- Un interpréteur = un programme
- Techniques pour modifier un programme -> techniques pour modifier un interpréteur
- Comment modifier un programme?
- Modifier le code source
  - Mais aussi modifier son interpréteur
  - Ou son code binaire
- S’insérer n’importe où dans le pipeline
  - différentes représentations du même programme
  - compromis pour la modification

De là où je suis parti, vers les confins du raisonnable.

- OO
  - Smalltalk
  - Self
- Expression problem
- Visitor, extensible visitor
- Building from modules
- Building with monads (FP trail)
- Customization (core + extensions)
  - Emacs
  - Plugins (browser/Eclipse)
  - Lua
- Open implementation
  - double interface: client et implémenteur
- Reflection
  - proxies
  - meta object protocol
- Dynamic binding
- AOP
  - AspectJ
  - CaesarJ
- HyperJ
- COP
- FOP (software product lines)
- Bytecode instrumentation
- Static transformation
  - preprocessor
  - semantic patches

**** Contributions
***** Étude de cas: extension ad-hoc de Narcissus
***** Variations (lamfa)
***** Construire un interpréteur par modules (LASSY)
***** Modifier un interpréteur par dynamic scoping (DLS/SAC)
***** Étendre Narcissus par dynamic scoping (DLS)

**** Synthèse
***** Séparation des préoccupations: pourquoi?
- Séparation difficile à obtenir
- Difficile de définir les frontières
  - une préoccupation est rarement isolée du reste du programme
- Cause d’autres problèmes
  - appels implicites, perte de compréhension du flot de contrôle
- Pointcut ~ dynamic scoping ~ COMEFROM
  - mécanismes puissants mais peu connus
  - usage difficile à justifier hors cas vraiment spécifiques
- Mauvais sens des priorités
  - Un programme doit d’abord être correct.
  - Puis il doit être maintenable -> bonne documentation des choix de structure
  - Séparation des préoccupations = cerise sur le gâteau.  Mais où est le gâteau?

*** What the hell did I just do?                                      :emacs:
There is a ~last-command~ variable that holds the last command.  Useful for
these moments I do something unexpectedly useful but have no idea what I typed.
Also, ~command-history~.

** [2015-11-12 jeu.]
*** The multiple dimensions of the background section
The *chronological* axis is evident, and somehow not particularly revealing.

There is a trend for higher-level languages, but it seems to have stagnated.
Though I don’t know how one would /measure/ the ‘higher-levelness’ in order to
produce this curve.  Taking popularity numbers is easier now with Github, but
getting numbers from past decades might be harder.

A timeline of all the cited works might be interesting to look at.  Not only the
dates, but the genealogy between works also, to understand trends.

There is a dimension of the nature of *tools*.  Languages, methodologies,
patterns, mechanisms.  All are tools in the programmer’s toolbox, but they
are not interchangeable.  Mechanisms may be independent of languages, languages
can depend of the problem domain...

In paradigms like OO and FP, works present how to *build* extensible programs
(or interpreters).  Reflection and AOP allow one to *extend* existing programs,
mostly regardless of how they are constructed.  These are different approaches,
as in FP the latter is somehow difficult.  But the distinction can be fuzzy, as
AspectJ could be argued to fall into both ends (building with aspects, and
extending with pointcuts).

We could place all these works on the map of the *pipeline* from source code to
machine code.  This provides an applicability chart: if I want to modify source
code, a bytecode transformer is not the right tool.

Ideally, the background section should highlight a blank space, an uncharted
territory in some ways, which would motivate the contributions.  In my case it
was rather the contrary: there was a wealth of solutions, but no consensus.  The
territory is well-trodden, but no-one seem to agree.  How to extend interpreter?

In the end, like always, all of those dimensions are relevant.

*** Is modularity relevant?
I want to extend programs.  One of the virtue of a modular program is that it
should be easier to extend.  So, a “modular” interpreter would be extensible.
However, an extensible interpreter is not necessarily modular, because the
extension mechanism may introduce coupling (like dynamic scoping).

But I want to refrain from dealing with modularity, as it is an “eel”
concept—ill-defined and too slimy to capture.

** [2015-11-13 ven.]
*** Paste in insert mode
=C-r register= courtesy of http://stackoverflow.com/questions/2861627/paste-in-insert-mode.

** [2015-11-17 mar.]
*** Rewriting Git history
Had committed a bunch of things with the username/email info of the Github
account I used for the double-blind submission to SAC.  D’oh.

Luckily, Github has a [[https://help.github.com/articles/changing-author-info/][handy script]] exactly for this purpose, based on ~git
filter-branch~.

Now I just have to remember to make a fresh git clone of those projects from my
other computers.

** [2015-11-18 mer.]
*** Making the diagram for tangled concerns in Narcissus
Already had done the ground work of stitching the code into a single picture,
and labeling the diff with the concerns.  But it’s a large PNG, and I want only
a ‘big picture’ that shows the fragmentation of concerns over the code.

**** Aligning rectangles to the grid
Since I want a vector diagram, I loaded the PNG into Inkscape.  Now, I wanted to
represent one line of code by a rectangle of solid color.  Each color
corresponds to a category of change, out of the 3 main categories I present in
the case study.

To line up the rectangles with each line of code, I used a custom-sized grid.
Grid step height equaled the line height (I eyeballed it), and grid step width
equaled the column width of my text.

Here are the values I used, in case I need them again:
: Preferences > Interface > Grid
: Spacing X = 372px
: Spacing Y = 1,54920px
: Major grid line every: 5

Then, I created a rectangle of solid color covering a line.  Copy-pasting this
rectangle with the grid activated automatically snapped the copy onto the line
under my cursor.  Then, I could resize the rectangle using the black arrow and
holding Alt, for covering multiple lines at once (holding Alt does /not/ make
the resizing snap to grid, but it resizes in increments of the original height,
which in my case amounted to the same).

**** Recovering the underlying hidden image
I used some alpha value in the solid colors in order to still see the code
underneath while putting the rectangles.  Now that this part is over, I can
deactivate the alpha and hide the underlying image.  To get the image back, I
can go through Edit > Find/Replace, and search for ‘img’, including hidden tags.
Or I can go through Edit > XML Editor and go through the tree to look for it.

**** Choosing a palette
Went to [[http://colorbrewer2.org/][ColorBrewer]] to find a qualitative palette for 4 classes.  Might change
that later, but for the moment it is not offensive and does the job.

**** Exporting in batch mode
I put the SVG into an svg folder.  Then I add a rule to the Makefile to produce
a PDF from an SVG using Inkscape in batch mode.

: inkscape --file svg/narcissus-diff.svg --export-pdf=img/narcissus-diff.pdf

** [2015-11-19 jeu.]
*** Images in caption
So, I needed to put the legend of yesterday’s diagram in the caption.  Making a
file link in the Org CAPTION works (though it does not seem to be recognized as
a link by org-mode).

On export, an ~includegraphics~ is included in the caption.  However, LaTeX
complains.  [[http://tex.stackexchange.com/questions/54049/placing-graphics-inside-figure-captions][Turns out]] that you have to ~\protect~ that command in a caption, or
put something into the optional argument of the caption (short name for the list
of figures).

You /can/ specify the short name in Org:

: #+CAPTION[short]: long

But!  It’s not sufficient in my case, since ~figure~ is actually redefined by
tufte-latex, and only the ~\protect~ will do.

So, I bring out the big guns and I advise ~org-latex--inline-image~ to add a
~\protect~ in front of ~\includegraphics~ for image links.  I do this regardless
of whether I am in a caption.  I don’t know enough LaTeX to know whether this
will cause me headaches in the long run, but it works for now... (famous last
words)

*** Troubles of listings in figures
Argh.  Listings environment by default can be broken over a page.  But not
figures.  So, wrapping a listing in a figure gives the figure behavior.  Might
be problematic down the road, as I intend to include many code.

** [2015-11-20 ven.]
*** Fixing space around figures
Smaller space via ~\intextsep~ (cf. [[http://tex.stackexchange.com/questions/38393/spacing-around-table-and-figure-environments][SO]]).  Listings had an extra space above,
even though I put ~aboveskip~ and ~belowskip~ both to 0... Putting a negative
value in ~aboveskip~ did the trick.

*** Adding a toggle for ~\centering~ in figures
I already override the function to export inline images in [[file:tex/export-setup.el][export-setup.el]],
so...  Just another case of getting the argument and formatting.

Tricky thing though.  I went for an option ~:center~, like this:

: attr_latex: :center nil

So, nil removes the ~centering~, but absence of the option leaves it.
~plist-get~ gives you whatever was written after the option.  But it returns nil
if the option was absent.  Apparently, a “nil” value will be turned into the
symbol, which makes the absence of value indistinguishable from the presence of
a “nil” value.  I guess it’s understandable.

So I went for ~:no-center~ instead.  But!  Giving no value to the option seems
to still return nil.  So you can’t have a simple boolean option that is either
present or not.  I must write:

: attr_latex: :no-center t

which is redundant.  Oh well.

** [2015-11-25 mer.]
*** Reading by following leads from reviewer #3
“Homogeneous embeddings” [[cite:Tra-08][Tra-08]] seems like a fancy academic word for “DSL”, or a
distinction thereof that I do not quite grasp.  The paper is obscure and I don’t
quite understand the problem it tries to solve.

“Growing a language” [[cite:Ste-99][Ste-99]].  Is there ever a Steele paper I disliked?  Argues
that a well-designed language is often small, too small to be of use for the
real programs of today.  Large languages provide many words to be used for the
real programs.  But large languages are too hard to design, and too hard to
learn.  Hence, one must design /growable languages/.  Start small, but give
users the tools to extend the vocabulary of their language.  Matches content to
form by giving a talk made of one syllables words, and defining longer words as
he goes.  Behind this general advice, says that he wants generic types and
operator overloading in Java.  One of them did not make it; this was in 1999.
Steele went on to develop Fortress, which he considers as an example of growable
language.

“Modular Domain Specific Languages and Tools” [[cite:Hud-98][Hud-98]].  DSLs are good, because
they are domain-specific.  Programs written in them are more declarative, and
easier to learn (HTML and LaTeX users rarely deal with control flow, for
instance).  But DLSs are hard to write and their vocabulary is tricky to get
right.  Hence, domain-specific /embedded/ languages (DSELs), which are DSLs
built inside an existing language.  Reuse the underlying language facilities,
but build a domain-specific vocabulary inside it.  The term is new, but what it
describe is old: think Lisp macros.  Of course, Hudak prefers to use Haskell.

Goes off to give examples of DSELs: geometry regions, reactive animations,
monadic interpreters.  Referential transparency, higher-order functions and
laziness are the key ingredients to declarative DSELs.  Referential transparency
is necessary for equational reasoning, like showing associativity or
distributivity.  Higher-order functions are necessary for composing existing
behavior into new words for the DSEL vocabulary.  Laziness is necessary for
writing recursive definitions that we can compute.  Although, maybe those are
just /sufficient/ for DSELs and not /necessary/.  Hudak only says that without
those ingredients it would be cumbersome and unnatural, not impossible.  But he
developed Haskell, so might be a bit biased in this respect.

The nugget, for my topic, is:

#+BEGIN_QUOTE
The design of truly modular interpreters has been an elusive goal in the
programming language community for many years.  In particular, one would like to
design the interpreter so that different language features can be isolated and
given individualized interpretations in a “building block” manner.  These
building blocks can then be assembled to yield languages that have only a few, a
majority, or even all of the individual language features.  Progress by Moggi,
Espinosa, and Steele [[[cite:Mog-89][Mog-89]], [[cite:Ste-94][Ste-94]], Esp93, [[cite:Spi-95][Spi-95]]] laid the groundwork for our
recent effort at producing a modular interpreter for a non-trivial language
[[cite:LHJ-95][LHJ-95]], and basing modular compiler construction technology on it [LH96, Lia98].
The use of monads [PJW93, [[cite:Wad-90][Wad-90]]] to structure the design was critical.

Our approach means that language features can be added long after the initial
design, /even if they involve fundamental changes in the interpreter
functionality/.  [...] At each level the new language features can be added,
along with their semantics, /without altering any previous code/.
#+END_QUOTE

An example is briefly given.  An interpreter is just a function:

: interp :: Term -> InterpM Value

But these types are just aliases.  To build an interpreter incrementally, redefine
the aliases.

: type Term = TermA
: type Term = OR TermF TermA

I feel this is cheating a bit, since you still have to change existing code.
Unless you can load the definitions of the types from external files.

All this composition has a performance cost.  But partial evaluation to the
rescue!  Hudak mentions speedups, but no baseline for comparison; e.g., a
hand-written instrumented interpreter or program.  This read like an ad with
very little criticism and basically no word on alternative approaches.

[[cite:LHJ-95][LHJ-95]] goes into lengthy details of how to build this interpreter.  Mostly, it’s
about using the Either type for composing terms, and monad transformers for
composing interpreter functions.  Writing monad transformers is a lot of work,
and some boilerplate.  Lifting is a thorny issue which creates complex code.
While it seems the interpreter is indeed composed from modular pieces, the
language is definitely not natural.  Swierstra’s [[cite:Swi-08][Swi-08]] solution is easier on
the eye.

** [2015-11-26 jeu.]
*** Catching up on OOPSLA’15
More reading, while I’m at it.

[[cite:FR-15][FR-15]] questions the folklore that functional programming is better for
modularity.  Mainly, all proponents cite [[cite:Hug-89][Hug-89]] as proof, and recently the
claims have become bolder without any empirical evidence.  Authors suggest to
look for this evidence, and settle the question.  As usual, I do not believe in
their methods based on large repository mining.  Statistics are devil’s tool.
And since “modularity” is a highly subjective property, I don’t think any
measurement of coupling will move proponents of functional programming either
way.  Still, it’s good someone else cares.

“Binding as Sets of Scopes” gives a new model for hygienic macro replacement.
Food for thought, as the Open Scope pattern is mostly binding manipulation.

** [2015-12-04 ven.]
*** Additional reference for building interpreters with monads
While searching for a “seminal” reference on the Free Monad, came across [[cite:SP-04][SP-04]].
In the history section, the authors attribute their knowledge of the Fix
datatype to a talk from Erik Meijer in 1996.

They also give pointers for building interpreters from composing re-usable
blocks. [[cite:Ste-94][Ste-94]] and [[cite:LHJ-95][LHJ-95]] I was already aware of, but [[cite:SBP-99][SBP-99]] is a new one in
MetaML.

** [2015-12-15 mar.]
*** A plan for the talk
Gramofarou.

One word to describe my thesis.  I put everything that I want to convey in this
word.  For me, it describes perfectly what I did and learned during these 3
years.  The perfect essence.

But for you, it means nothing.

It means nothing because it’s actually a pointer to my brain.  Your brain is
wired differently than mine.  Giving you my memory pointers would just point to
a random place in yours.  Brain segfault.  You can’t use the pointer.  I need to
/serialize/ the data out of my brain, into interchange words that you can digest
and mount in your memory.

But that has never been easy, writing.  There are agree-upon schemas, but since
all brains are wired differently, no-one has the same implementation of
serialization.  Luckily, computers programs are much more clear-cut.  There are
no ambiguities: the code is absolute truth.

There is a tension between speakers that is also present in programming.  I want
to tell you _THIS MUCH_ information in the fewest words possible.  Maximize
entropy, minimize noise.  I could tell you everything I know, but it would take
far too much time.  You need many words in order to make sense of what I’m
trying to tell you, each words would add a few bits of information, painting a
clearer picture in your head.  I’d rather use the fewest words possible in order
to save time.

Programming is a kind of dialogue.  The same tension exists and is even a
principle of software engineering: DRY.

Or, parsimony.  Pragmatism.  AOP ideas are interesting.  COP also.  But
frameworks/libraries for those are terribly convoluted and driven by salami
publishing.

** [2015-12-21 lun.]
*** Searching for a diagram language
The JavaScript examples are not very telling.  Because they only give the
syntax, they are useless to a reader who doesn’t know the semantics of the
language.  They only tell him that the results are indeed what we wanted to
obtain.  Even I sometimes gloss over the code when re-reading.

That is why I tried to use diagrams in the DLS submission.  To focus on the
/effects/ of the code, rather than focus on the code symbols.  We can see JS
programs as giving instruction to build a running system.  By showing how
objects relate to each other in the heap, we focus on the configuration itself,
not how we express it.  This frees us from thinking solely about JS: the same
could be done in Scheme, Python, Ruby ...

The code itself is part of the contribution of course.  /How/ we achieve a
specific heap configuration is of interest.  There are /infinite ways/ to obtain
a configuration.  Not all of them are equal in code size and clarity.

But the heap diagrams lack an important information: how to go from one diagram
to the next one.  They are just snapshots of state.  There is nothing that tells
a reader how to construct the next diagram.  This is implicitly gathered from
following the examples.

I think a representation of the code stack is missing.  What current code is
being executed?

** [2016-01-04 lun.]
*** Reading Programming as Theory Building
On the occasion of Peter Naur’s death.  [[cite:Nau-85][Nau-85]].

Programming is not the production of a program.  Programming is building a
theory of the matters at hand.

The first example resonates strongly: group A builds a compiler, group B wants
to extend it.  Group B proposes extensions.  Group A reviews them:

#+BEGIN_QUOTE
In several major cases it turned out that the solutions suggested by group B
were found by group A to make no use of the facilities that were not only
inherent in the structure of the existing compiler but were discussed at length
in its documentation, and to be based instead on additions to that structure in
the form of patches that effectively destroyed its power and simplicity.  The
members of group A were able to spot these cases instantly and could propose
simple and effective solutions, framed entirely within the existing structure.
#+END_QUOTE

Case in point, the program text is insufficient to access the theory:

#+BEGIN_QUOTE
This is an example of how the full program text and additional documentation is
insufficient in conveying to even the highly motivated group B the deeper
insight into the design, that theory which is immediately present to the members
of group A.
#+END_QUOTE

By programming, group A developed a theory, which is not accessible to group B
through the program.  The program is a by-product of building the theory, which
resides mostly in the mind.

In the second example, there is again evidence of critical knowledge held by
programmers about a large program; without this knowledge, modifying the program
is hopeless.

He means by theory what I often call “mental model”:

#+BEGIN_QUOTE
What characterizes intellectual activity [...]  is the person's building and
having a theory, where theory is understood as the knowledge a person must have
in order not only to do certain things intelligently but also to explain them,
to answer queries about them, to argue about them, and so forth.

The notion of theory employed here is explicitly /not/ confined to what may be
called the most general or abstract part of the insight.  For example, to have
Newton's theory of mechanics as understood here it is not enough to understand
the central laws, such as that force equals mass times acceleration.  In
addition, as described in more detail by Kuhn, the person having the theory must
have an understanding of the manner in which the central laws apply to certain
aspects of reality, so as to be able to recognize and apply the theory to other
similar aspects.
#+END_QUOTE

Why modify programs rather than building new ones from scratch?  Costs.

#+BEGIN_QUOTE
The question of program modifications is closely tied to that of programming
costs.  In the face of a need for a changed manner of operation of the program,
one hopes to achieve a saving of costs by making modifications of an existing
program text, rather than by writing an entirely new program.
#+END_QUOTE

But is modifying an existing program always cheaper?

#+BEGIN_QUOTE
First it should be noted that such an expectation cannot be supported by analogy
with modifications of other complicated man-made constructions.  For many kinds
of such constructions, such as cars, or television receivers, modifications are
entirely out of the question, in practice.

Second, the expectation of the possibility of low cost program modifications
conceivably finds support in the fact that a program is a text held in a medium
allowing for easy editing.  For this support to be valid it must clearly be
assumed that the dominating cost is one of text manipulation.  This would agree
with a notion of programming as text production.  On the Theory Building View
this whole argument is false.
#+END_QUOTE

Modifying text is cheap.  But a program works because of hundreds of design
decisions, hundreds of contextual assertions.  If even one or two assertions
change, then the whole program may have to be rebuilt.

What about flexibility?  Anticipating changes in the structure of the program,
allowing for easy extension?

#+BEGIN_QUOTE
flexibility can in general only be achieved at a substantial cost.  Each item of
it has to be designed, including what circumstances it has to cover and by what
kind of parameters it should be controlled.  Then it has to be implemented,
tested, and described.  This cost is incurred in achieving a program feature
whose usefulness depends entirely on future events.  It must be obvious that
built-in program flexibility is no answer to the general demand for adapting
programs to the changing circumstances of the world.
#+END_QUOTE

To modify a program, one must know the theory:

#+BEGIN_QUOTE
What is needed in a modification, first of all, is a confrontation of the
existing solution with the demands called for by the desired modification.
[...]  The point is that the kind of similarity that has to be recognized is
accessible to the human beings who possess the theory of the program, although
entirely outside the reach of what can be determined by rules, since even the
criteria on which to judge it cannot be formulated.
#+END_QUOTE

Essentially, there is no shortcut.  The program text is not sufficient.

#+BEGIN_QUOTE
the very notion of qualities such as simplicity and good structure can only be
understood in terms of the theory of the program, since they characterize the
actual program text in relation to such program texts that might have been
written to achieve the same execution behaviour, but which exist only as
possibilities in the programmer’s understanding.
#+END_QUOTE

The program is simple only when considering the theory, when taking all the
design choices into account.

I agree with Naur that programming is more than just writing the program text,
and the theory building view is an apt description of the activity.  However,
unlike Naur, I think that the theory can be described to a programmer, to some
extent.  If the theory is being able to answer questions about how the program
works, and how it should be extended, then it is only a matter of writing a FAQ.
A top-down description, à la literate programming, can also be used to describe
the overall design, and explain the trade-offs.  Maybe the whole knowledge is
not transferable, but the rest can be rebuilt by a willing programmer working
with the program text.

An important point about training programmers (and education in general):

#+BEGIN_QUOTE
This problem of education of new programmers in an existing theory of a program
is quite similar to that of the educational problem of other activities where
the knowledge of how to do certain things dominates over the knowledge that
certain things are the case, such as writing and playing a music instrument.
The most important educational activity is the student's doing the relevant
things under suitable supervision and guidance.  In the case of programming the
activity should include discussions of the relation between the program and the
relevant aspects and activities of the real world, and of the limits set on the
real world matters dealt with by the program.
#+END_QUOTE

He calls “program revival” the act of rebuilding the theory of a program text by
a new team of programmers (reverse-engineering the source).  Modifying a program
written by other programmers is akin to reviving it, since you need to rebuild
its theory.  But using dirty tricks that do not conform to the original theory
can be enough to make your changes.  Program necromancy sounds much better than
unanticipated extension.

He argues that modifying a program text is hopeless without modifying the
theory.  The proper way is to rebuild, and that may have a lower cost even in
the short term.

#+BEGIN_QUOTE
The point is that building a theory to fit and support an existing program text
is a difficult, frustrating, and time consuming activity.  The new programmer is
likely to feel torn between loyalty to the existing program text, with whatever
obscurities and weaknesses it may contain, and the new theory that he or she has
to build up, and which, for better or worse, most likely will differ from the
original theory behind the program text.
#+END_QUOTE

A consequence of the Theory Building View is that the choice of programming
language or tools is inconsequential.  All that matters is the theory.  It helps
understand why one should choose the tools the team is the most comfortable
with, without prejudice.

#+BEGIN_QUOTE
Where the Theory Building View departs from that of the methodologists is on the
question of which techniques to use and in what order.  On the Theory Building
View this must remain entirely a matter for the programmer to decide, taking
into account the actual program to be solved.
#+END_QUOTE

The Theory Building View implies that programmers are not interchangeable.  And
that we should focus on theory building skills to train new programmers:

#+BEGIN_QUOTE
While skills such as the mastery of notations, data representations, and data
processes, remain important, the primary emphasis would have to turn in the
direction of furthering the understanding and talent for theory formation.  To
what extent this can be taught at all must remain an open question.  The most
hopeful approach would be to have the student work on concrete problems under
guidance, in an active and constructive environment.
#+END_QUOTE

** [2016-01-27 mer.]
*** Separation of concerns is the scattering of control flow
Dijkstra’s original argument against GOTO was that you lost sight of the control
flow of a program when using GOTO statements.  Structured programming argues for
a single entry point to a function, and a single exit point from it: no
surprises of control transfer in the middle of a function, or an exit from a
routine from the middle of a loop.  Hence the more declarative loop
constructions (do-while, for) to replace the common use-cases of GOTO.  So, for
Dijkstra, clarity of code means that the control flow is explicit and not
tortuous.  The locality of the control flow is matched by the locality of the
code.

Separation of concerns argues for code clarity as well, by putting code related
to the same concern in a common place in the source code.  Code that can happen
basically at any point of the control flow.  Following the separation of
concerns, locality of source code does /not/ match the locality of control flow,
but rather matches the locality in the programmer’s brain.  Code pertaining to
the same concept is all in the same place, neatly arranged.  But then, the code
is full of non-local transfer of control!  Trying to make sense of the control
flow, we are back to GOTO, or even worse, COMEFROM.

COMEFROM, events, inversion of control.  From this lens, they all have the same
consequence: increasing modularity by decreasing coupling, at the cost of a
decreased ability to follow the control flow.

There are recourses.  Inversion of control is only a change in point of view.
You can always transform code that use observers to use direct calls.  And
convert GOTOs to and from COMEFROMs.  These paradigms are only two sides of the
same coin.  To get the full picture, one needs to look at the two-sides of the
coin at the same time.  Get some perspective.

The source code is a description of the program, but it is only one description.
We can arrange the code in many ways using views as done in literate
programming.

** [2016-02-03 mer.]
*** Figures aligned on the wrong side of the page
Tufte LaTeX nightmares ...  Figures are floats.  Floats can be pushed to the top
of the next page.  If I include a figure in an even-numbered page, and it is
pushed to the top of the next (odd-numbered) page, then instead of aligning to
its destination page, it aligns to the original page it was included in.

I’m considering switching to HTML export for drafting, and tinkering with LaTeX
only at the last moment, for publishing.  When publishing, I can modify the TeX
source directly and won’t care about using dirty hacks.  And drafting with HTML
I actually know how to put things where I want them to be.

Also, I can use SVG natively, and even add some interactivity, like running the
JavaScript examples.

** [2016-02-04 jeu.]
*** Splitting SVG using rsvg-convert
Splitting SVG using Inkscape on the command line was not as frictionless as
exporting to PDF.  The SVG are not cropped, leading to wonky work-around of
scripting the Inkscape GUI with --verb commands.  But that takes many seconds.

So, maybe exporting to PNG would be better for drafting.  But no.  PNG are not
cropped correctly, even with the --export-area-drawing option set.  Looking on
the Inkscape bug tracker for this surprising behavior, I see that rsvg-convert
does seem to do that correctly.  So I switch to rsvg-convert.

Mostly it’s smooth sailing.  I’m just wondering whether the PNG/PDF outputs can
different than the one from Inkscape.  I /hope/ that just cutting SVG out of a
larger file does not alter the image.

I also ditch Inkscape even for getting the IDs of objects to export, since it’s
so long to startup.  xmlstarlet with a simple XPath query is enough to get the
information I need.

*** Customizing the HTML output
This is covered by the [[info:org#CSS%20support][Org info manual]].

I need:
- max-width for content
- set the font

*** Getting figures in the margin
Again.  I should try to toy with that over the week-end.  If I can get full
control of figure offset in the exported HTML from the Org, that would be good
enough.  Because I can always tweak the values in the Org to match what I want.

Even better would be a few sane defaults like figures in the margin, full-width
figures with below caption, and text-width figures with margin caption.

*** Getting the bibliography back
There is a bibliography extension to org-export that uses bib2html.  Might use
that at start.

Otherwise, I might be able to roll my own.  It’s just a matter of:

- Getting all the refs I need to include by parsing the Org.

  Might be able to do that by logging each ref when creating citations.

- Create links for each citation.

  Easy, already done in export-setup.el.

- Extract all the needed info for each ref from the bib.

  Call a standalone tool that can slice bibtex files for a given ref.  Then
  template that into HTML.

Famous last words.

** [2016-02-05 ven.]
*** How to design diagrams for the manuscript
I design diagrams by imagining myself explaining the point of the whole piece of
text next to it.  I would explain the main points by pointing at different parts
of the diagram(s) on the right, and they would get it immediately because I
don’t have to draw a picture.  The picture is there.

Then I try to build the diagram that can explain the main points.

** [2016-02-08 lun.]
*** Software is soft
How many times you see construction for adding a lane to a bridge?

Software is soft.  It’s just text.  It can be written from scratch by one
person, using easily-accessed tools at a very low material cost.

But programs are built with a structure.  The programmer made design decisions
according to the requirements he had, and foreseeing the potential changes.

If you know you will need to service a bathtub regularly, leave an easy access.
When you need to change faucet in your kitchen, your are thankful that the
plumber put a valve at the door, so you can cut the water just for the kitchen
without disturbing the rest of the house.

The program is expressed as plain text, and plain text is easily changeable, so
we are led to think that the program can be easily changed as well.  But the
program is not just the text; the text is the result of many design decisions
that do not appear in the source text.  These decisions were made according to
the assumptions, the knowledge the designer posses of the application domain and
tools.

The program is extensible only if the designer made it so.  Extensibility is a
feature that must be planned in advance.  And as a feature, it adds complexity
to the system.  A good designer would try to minimize the overall complexity of
the system, and hence would only make the system modular if the trade off is
worth it; only for foreseeable changes that do not bear too much weight on the
system.

** [2016-02-10 mer.]
*** There is no “zero-cost” abstraction
The less you specify, the more control you relinquish.

When you delegate a task, you lose control over the finer details of how this
task will be performed.  The delegate has this control.  In exchange for this
loss of control, you gain time, you are to focus on other matters.

Delegation is very useful when you do not care about /how/ the task is carried
out, as long as it’s done.

If you need a custom-made chair, then ask a woodworker.  Give approximate
dimensions, style, fitting guidelines and a few days later tadaa, you got a
chair.

But if you have very specific needs for a chair, it’s probably best to be
involved in the decision process.  Pick the exact fabric, the exact wood to be
used, specify the exact shape, etc.  You might even do it yourself.  Then you
have more control over the end result.

Abstraction in programming is delegation.  If you abstract, you have less
control.  And for programs, less control is less control over the performance,
over /how/ exactly is the machine stirred by your code.  That’s the price you
pay, and there’s no way around it.

To give an example, a “number” construct in a language is an abstraction.  Let’s
say your programming language only offers the “number” type to hold any integer,
no matter how large it is.  Of course, there’s no machine capable of holding an
integer of arbitrary size.  But, dealing with 32-bit integers, or 64-bit
integers, or moving them to heap space using an efficient representation is
extra work.  So, you just say “number” and happily build your program with the
confidence that your additions will never overflow, provided you do not outgrow
your machine’s memory limits.

But then, you have absolutely no idea how efficient your program really is.  You
/hope/ the designers of the “number” abstraction have done their best to
optimize the common cases: if your numbers can all be represented in 64-bit or
less, that they use adequate registers and opcodes.  But you also know that /no
compiler/ will ever be optimal, so there’s always a potential loss there.

** [2016-02-11 jeu.]
*** Code tagging for language-agnostic multiple-views
Multiple design decisions are made when creating software.  It’s a very good
idea to document these decisions, and leave that somewhere with the program, for
future source code readers and maintainers to better understand why the program
was written the way it was.

Something you often need to communicate, is the knowledge of modifying some
parts of the program, if it was built with some flexibility.

Literate programming is a nice solution for exposing the structure of a program,
by focusing only on one part at a time.  It kind of solves the separation of
concern, since you can expose the code that pertains to one concern at a time.

But literate programming requires a weaving phase to produce the final document
as well as the code.  So you are editing something but are not directly
manipulating the object of interest.

When editing code, you might want to recall a few related functions, to
understand the control flow, or show how they can be modified in order to extend
the functionality of the program.  Grouping these related pieces of code
together in the source is not always a solution, since they can be part of
multiple separate concerns at once.  There is never only one dimension to the
program.  As Knuth noted, it’s more of a web.  But literate programming produces
a little friction when developing.

One low-cost solution is to put #tags in comments.  This is language-agnostic,
and does not require a specific tool.  All related pieces (tagged with #t, for
example) can be brought up with grep.  Best part is editor integration that can
show you views for each tag (very much like tree slicing in Org, or Occur).
Much like TODO, FIXME tags, this a very lightweight solution to give pointers to
future programmers (including future self).

** [2016-02-17 mer.]
*** Reading Patterns of Software
[[cite:Gab96][Gab96]].

The foreword by Christopher Alexander, I’m assuming an established architect who
as written several books on architecture, is rather insightful:

#+BEGIN_QUOTE
But still a fundamental question of practicality must lie at the forefront.
Does all this thought, philosophy, help people to write better programs?  For
the instigators of this approach to programming too, as in architecture, I
suppose a critical question is simply this: Do the people who write these
programs, using alexandrian patterns, or any other methods, do they do better
work?  Are the programs better?  Do they get better results, more efficiently,
more speedily, more profoundly?  Do people actually feel more alive when using
them?  Is what is accomplished by these programs, and by the people who run
these programs and by the people who are affected by them, better, more
elevated, more insightful, better by ordinary spiritual standards?
#+END_QUOTE

That’s a very important question, that puts the betterment of human beings as
the first priority of the endeavor of programming (and, I assume that Alexander
holds architecture to the same standard).  I already like what I’m reading,
especially since the answer is “probably not”:

#+BEGIN_QUOTE
Here I am at a grave disadvantage.  I am not a programmer, and I do not know how
to judge programs.  But, speaking only about what appears in this book, I must
confess to a slight—reluctant—skepticism.  I have not yet seen evidence of this
improvement in an actual program.  Of course my ignorance is such that I would
not have good instincts, at first anyway, about a given bit of code, not even
enough to be able to say “This is a beautiful program, this one less so.”  I do
not therefore ask these probing questions in a negative or hostile spirit at
all.  I ask them, because I hope, and believe it may propel readers of this
book, programmers themselves, into trying to do better.  But I cannot tell, as
yet, whether the probing questions asked in this book, will actually lead to
better programs, nor even what a better program is.
#+END_QUOTE



Gabriel takes over and also expresses skepticism over cure-alls:

#+BEGIN_QUOTE
We’ve tried to make programming easier, with abstraction as a toll, with
higher-level programming languages, faster computers, design methodologies, with
rules of thumb and courses and apprenticeships and mentoring, with automatic
programming and artificial intelligence.  Compilers, debuggers, editors,
programming environments.  With structured programming and architectural
innovations.

With object-oriented programming.

But programming still requires people to work both alone and in teams, and when
people are required to think in order to achieve, inherent limitations rule.
Object-oriented programming—which is merely a set of concepts and programming
languages to support those concepts—cannot remove the need to think hard and to
plan things, to be creative and to overcome failures and obstacles, to find a
way to work together when the ego says not to, that the failures are too many
and too pervasive.
#+END_QUOTE

He is, like Alexander, someone who cares deeply about fellow humans above
technology:

#+BEGIN_QUOTE
My overall bias is that technology, science, engineering, and company
organization are all secondary to the people and human concerns in the
endeavor.  Companies, ideas, processes, and approaches ultimately fail when
humanity is forgotten, ignored, or placed second.  Alexander knew this, but his
followers in the software pattern langauge community do not.  Computer
scientists and developers don’t seem to know it, either.
#+END_QUOTE



One desirable criterion for software is /habitability/, rather than clarity.
Here we see the influence of Alexander’s writing on Gabriel:

#+BEGIN_QUOTE
Habitability is the characteristic of source code that enables prorgammers,
coders, bug-fixers, and people coming to the code later in its life to
understand its construction and intentions and to change it comfortably and
confidently.  Either there is more to habitability than clarity or the two
characteristics are different.

Habitability makes a place livable, like home.  And this is what we want in
software—that developers feel at home, can place their hands on any tiem without
having to think deeply about where it is.  It’s something like clarity, but
clarity is too hard to come by.

Most programming languages are excellent for building the program that is a
monument to design ingenuity—pleasingly efficient, precise, and clear—but people
don’t build programs like that.  Programs live and grow, and their
inhabitants—the programmers—need to work with that program the way the farmer
works with the homestead.

Software needs to be habitable because it always has to change.  Software is
subject to unpredictable events: Requirements change because the marketplace
changes, competitors change, parts of the design are shown wrong by experience,
people learn to use the software in ways not anticipated.
#+END_QUOTE

** [2016-02-18 jeu.]
*** An abstraction is not always clearer
Abstraction can create more trouble than its worth:

#+BEGIN_QUOTE
If one abstraction is used in many places and that abstraction’s interface is
wrong, then repairing it forces repair of all its uses.
#+END_QUOTE

(Reminded of the [[http://www.sandimetz.com/blog/2016/1/20/the-wrong-abstraction][Wrong Abstraction]]).

An abstraction is often a compression: a new word for designing a common
pattern.  But new words are only useful for those who know them.  Gabriel asks
if the right abstraction:

#+BEGIN_SRC lisp
(mismatch sequence list :from-end t
          :start1 20 :start2 40
          :end1 120 :end2 140 :test #’baz)
#+END_SRC

is clearer than using more commonly-known constructs:

#+BEGIN_SRC lisp
(let ((subseq1 (reverse (subseq sequence 20 120)))
      (subseq2 (reverse (subseq list 40 140))))
  (flet ((the-same (x y) (baz x y)))
    (loop for index upfrom 0
          as item1 in subseq1
          as item2 in subseq2
          finally (return t) do
          (unless (the-same item1 item2)
            (return index)))))
#+END_SRC



The quality without a name is a key concept to Alexander’s work, especially to
his pattern language.  Gabriel tries to understand this quality as it pertains
to programming software.

One aspect of the quality is a match between the solution and the problem:

#+BEGIN_QUOTE
One of the key ideas in this book was that in a good design there must be an
underlying correspondence between the structure of the problem and the structure
of the solution.
#+END_QUOTE

Quoting Alexander:

#+BEGIN_QUOTE
structural hierarchy is the exact counterpart of the functional hierarchy
established during the analysis of the program.
#+END_QUOTE

** [2016-02-19 ven.]
*** The process is as important as the design
Gabriel relates the experience of Alexander of trying to first build a pattern
language for constructing buildings that have the “quality without a name”.
This is difficult to relate to software, since the quality without a name is
also without a clear definition.

But there is an interesting twist: people start designing buildings following
Alexander’s patterns, and the results do not have the desired quality.

The root cause, according to them, is to fail to pay attention to the /process/
of buliding, of mortagage, of the economy around buying and building a house to
live in.

#+BEGIN_QUOTE
One of his reactions was to consider the process of building: the mortgage
process, the zoning process, the construction process, the process of money
flowing through the system, the role of the architect, and the role of the
builder.  By controlling the process, you control the result, and if the control
retains the old, broken process, the result will be the old, broken
architecture.

This resonates with what we see in software development: The structure of the
system follows the structure of the organization that put it together, and to
some extent, its quality follows the nature of the process used to produce it.
The true problems of software development derive from the way the organization
can discover and come to grips with the complexity of the system being built
while maintaining budget and schedule constraints.
#+END_QUOTE

At the end of this chapter, I have an overwhelming impression that Gabriel is a
hopeless romantic.  Searching for a “quality” without a clear definition of it
is just seeking a specific perfection.  When you do not define clearly the goal,
it will always elude you.  And that’s begging for unhappiness.  Alexander also
comes out as a nostalgic: he opposes the modern “funky” constructions to the
“beautiful” traditional ones.

*** Messy perfection, ordered chaos
In “The Bead Game, Rugs, and Beauty”, Alexander gives a more constructive
definition to the elusive quality using Turkish rugs.  In the rugs, symmetries
and subsymmetries contribute to their beauty.  But, the symmetries are rarely of
the perfect geometrical variety.  They evoke symmetry, but some details change.
For Alexander, this crude symmetry contribute to their /wholeness/:

#+BEGIN_QUOTE
In our time, many of us have been taught to strive for an insane perfection that
means nothing.  To get wholeness, you must try instead to strive for /this/ kind
of perfection, where things that don’t matter are left rough and unimportant,
and the things that really matter are given deep attention.  This is aperfection
that seems imperfect.  But it is a far deeper thing.
#+END_QUOTE



The chapter on languages is short and only mildly interesting.  There are no
strong points in the essays; only the opinions of one programmer.  Alexander’s
vision is grander than Gabriel’s, who has reasonable positions on programming
and software engineering, and ultimately, more interesting.

** [2016-02-22 lun.]
*** Energize and Lucid Emacs
Getting close to the end of the book.  Part IV and V are autobiographical
accounts of the events that led Gabriel to Stanford, and to the creation and
ultimate demise of Lucid Incorporated.

The chapters are quite personal, and light on technical content or advice on
software, though not uninteresting.  But one part that caught my attention is
the description of the C++ interactive programming environment codenamed
/Cadillac/:

#+BEGIN_QUOTE
Cadillac was an attempt to build an environment with same the degree of
integration as the single-address-space Lisp and Smalltalk environments had in
the 1970s and 1980s while separating the tools from the application.

The idea was to reduce the number of physical tools as much as possible and to
layer information onto those few tools.  Basically we had a text editor and a
grapher.  These tools were able to handle descriptions of the sorts of things they can
display along with descriptions of annotations.  Annotations are a generalization
of hypertext.  An annotation is an object along with an associated set or sequence
of other objects.  An annotation acts as a link among those other objects.  Each
annotation is an instance of a class to which methods can be attached.  An
example annotation is the simple link associated with a sequence of two objects.
When both objects are text, the annotation is merely hypertext.

Annotations and other information about the program under development
are kept in a database and managed by a kernel.  The kernel and database act as the
single address space where everything is known.  The tools know nothing.  An
annotation is active: Methods are attached to annotations by being associated
with a class.  When someone clicks on an annotation, the tool asks the connected
kernel to look up what actions are available, which are passed back for the tool
for display and selection.

With annotations you can take an ordinary textual display of a program and
decorate it with error messages, with breakpoints, with connections to related
code, to classes, to methods, and so forth.  Instead of a number of browsers
there is only one textual browser and one graphical browser.  These tools
display the same objects, and all action takes place in the kernel.  The objects
in the kernel are in a persistent object store, so information lasts over
multiple sessions.

When you click on the textual representation of any object, you get a pop-up
menu with a list of actions.  This is nice because it makes you feel that you
are interacting with objects directly instead of through the intermediary of a
tool.

The kernel learns about the programs by listening to a stream of data about the
program sent by the compiler which annotates the source with language elements,
a sort of generalization of programming language constructs.  This implies that
the environment is language independent.
#+END_QUOTE

Based on what is described there, it sounds like a very interesting project I’d
like to see in action.  Annotations of code linking to diagrams, explanations,
documentation, all integrated into the programming environment is something that
we still have not realized well.

More details are in [[cite:GBD+90][GBD+90]].  Now, unfortunately, it does actually contains any
tangible details; only abstract descriptions of how the systems is built and
vague promises about what the system will do if the products takes off (it
didn’t).

It seems to be a full-featured IDE for C++, for Lisp programmers.  Not only
there is the annotations part, integration with man pages, documentation, call
graphs ..., but it also turns C++ into a live interactive environment, like
Lisp.

This [[https://www.youtube.com/watch?v=pQQTScuApWk][demonstration video]] shows off the actual product.  It’s a modified Emacs
(Lucid Emacs, aka XEmacs) with a mouse-oriented interface where you can click on
any function, class, variable and search for its uses, included files, etc.
There’s also a “grapher”, a window to display class hierarchies or call graphs.
All the views are kept in sync: modify a function name, and the graph will
reload after compilation.  Compilation is incremental: only the changed
functions/classes/enums are recompiled and linked into the final program.  So
it’s a thoughtful IDE for C++.

But the annotations are not exactly what I had in mind.  [[http://www.codersnotes.com/notes/a-constructive-look-at-templeos/#hypertext-doldoc][Temple OS]] did it
better.

** [2016-02-29 lun.]
*** The ideal platonic programming language
Say you have a specific program idea in mind, you want to create a specific
system.  You start seeing the system working in your head, you see the
interactions, how that would work, you can already toy with it even if it’s not
real.

Let’s take a concrete example to avoid talking about clouds.

Say you want to build a Tetris-like game.  Interactive, realtime, traveler’s
backpack puzzle game where you fill a well with tetraminoes.  You already see
how it should work, where the well goes in the UI, how the tetraminoes should
fall, how do they rotate when the user presses a button, how the next tetraminoe
to fall is previewed in the corner...

All of these elements form the /domain language/: the vocabulary that you can
use to talk about the running system when you describe it to someone.  In this
language, you can already specify rules for the system: the game is over when
the last tetraminoe was put outside the screen; after a tetraminoe is put in
place, any full-width line is cleared and all lines above it are shifted one
line down.

When it comes to implementing the game in software, you just want to express the
design constraints as plainly as possible.  What you really want to is to give
the above rules to the machine, after you have defined the vocabulary.  This
gives you the greatest flexibility in toying with design ideas: once the
vocabulary is in the machine, you can just try new rules or alter existing ones
easily.

Now the problem of course is that while the domain language might be expressive
enough for designing rules, it borrows a lot from its context and the ability of
the readers to understand English, and their knowledge about the world (what’s a
line, a tetraminoe, a screen, or a GUI?).

When it comes to teaching the vocabulary to the machine, you don’t have all this
context to lean on.  You have to recreate this context first, when porting the
vocabulary to the machine.  And then it gets awkward, because English will allow
you to combine words in many ways, but programming languages are less flexible.

And programming languages are there to allow you to /program/ the machine, to
express computations that the machine will carry away without supervision.  You
cannot leave details out that a human computer would fill in for you; everything
has to be specified at some point.  So it becomes very hard to dissociate the
rules from the specific choices you have to make in order for the system to work
on real hardware.

The point is, a designer would very much like a programming language that is as
close to the domain language as possible.  That is the ideal programming
language for them.  But, as the domain language conveys no information about
real hardware, there is a tension between expressing only what is relevant to
the model, and stating out every step the machine must take in order to realize
this vision.  This tension is also present in English itself:

#+BEGIN_QUOTE
In Zipf’s view, ambiguity fits within the framework of his unifying principle of
least effort, and could be understood by considering the competing desires of
the speaker and the listener.  Speakers can minimize their effort if all
meanings are expressed by one simple, maximally ambiguous word, say, /ba/.  To
express a meaning such as “The accordion box is too small,” the speaker would
simply say /ba/.  To say “It will rain next Wednesday,” the speaker would say
/ba/.  Such a system is very easy for speakers since they do not need to expend
any effort thinking about or searching memory to retrieve the correct linguistic
form to produce.  Conversely, from the comprehender’s perspective, effort is
minimized if each meaning maps to a distinct linguistic form, assuming that
handling many distinct word forms is not overly difficult for comprehenders.  In
that type of system, the listener does not need to expend effort inferring what
the speaker intended, since the linguistic signal would leave only one
possibility.

Zipf suggested that natural language would strike a balance between these two
opposing forces of unification and diversification, arriving at a middle ground
with some but not total, ambiguity.  Zipf argued this balance of speakers’ and
comprehenders’ interests will be observed in a balance between frequency of
words and number of words: speakers want a single (therefore highly frequent)
word, and comprehenders want many (therefore less frequent) words.  He suggested
the balancing of these two forces could be observed in the relationship between
word frequency and rank frequency: the vocabulary was “balanced” because a
word’s frequency multiplied by its frequency rank was roughly a constant, a
celebrated statistical law of language.
#+END_QUOTE
([[cite:PTG-12][PTG-12]], p. 2)

This tension is a the core of what programmers deal with everyday.  Programming
is mainly taking sentences in the domain language, and translating them to
executable machine instructions.  It’s more than that: it’s mapping the whole
domain language to the target computer.  The difference is the same as the
difference between translating a book and translating a hundred isolated
sentences.  A book is a coherent collection of sentences, so the translation
should be coherent as well.

So maybe an ideal programming language, one that gives you all the expressivity
of the domain language, but can be executed as is by the machine, is really
ideal in the platonic sense—that is, an elegant construction that exists only to
the mind, totally not happening in real life.

Coming up with a coherent domain language in the design phase goes already a
long way toward making the right choices at a low cost.  Designing a game on
paper, or /with/ paper cut-outs can help make some problems immediately apparent
without writing a single line of code.

But any such design is incomplete until it is running on a machine as expected.
So we should not strive for a perfect design upfront.  Being able to answer
questions and try out variations on paper is already helpful.  I am not
convinced that aiming for the ideal language will bring tangible benefits to the
process.

** [2016-03-01 mar.]
*** Exporting bibliography in HTML
There was the right way:
1. Parse the Org file to get all cited keys
2. Call an external tool to generate a .bib file containing only cited entries
3. Parse the bib and generate HTML

Now, it’s not /so/ trivial.  Existing tools are a bit bizarre and convoluted.
Last time I tried to do that, I ended using bib2x, and it’s not that flexible.

All I want for step 3 is a bibtex to JSON.  After that, I can generate HTML
using anything.  It /seems/ there are such converters ([[http://home.in.tum.de/~muehe/bibtex-js/demo/bibtex.html][there]], [[https://github.com/mikolalysenko/bibtex-parser][there]] and [[https://www.npmjs.com/search?q=bibtex][there]]),
but they are all version 0.0.1, and I don’t have time to test them all.

So, there is the (sort-of) fast way:
- Use ox-bibtex, and tweak it until I get a good-but-not-perfect output.

I knew ox-bibtex /could/ get me 3/4 of what I wanted.  And as it turned out,
there were only minor changes to make.

Just requiring ox-bibtex at export time is enough (since I already had the
BIBLIOGRAPHY keyword in the manuscript).  With the ~limit:t~ option it exports
only the cited entries.  Each entry has a link with the key bound to its name,
so querying #Ari-35 in the page works directly.

The references are included at the position of the BIBLIOGRAPHY keyword though.
Since I want them at the end, I put the keyword in its own (unnumbered) section.
It’s a minor inconvenience to have to bury this keyword at the end of
manuscript, but hey.

Also, since ox-bibtex calls bibtex2html to generate two files, read one of them,
and insert the content in the exported manuscript, there was no reason to keep
the files around.  So I [[file:html-src/export-setup.el::(defun%20org-bibtex-process-bib-files%20(tree%20backend%20info)][tweaked]] the main function of ox-bibtex to do that.

Then, what took the most time was an unexpected (of course) annoyance with file
encoding.  I suspected that bibtex2html would not handle my unicode bib file
correctly.  Actually, it leaves the accents unchanged when creating the HTML,
but mangles the key names when an author has an accented name, e.g.:

: [�T09a] 	Éric Tanter.

But!  Since the resulting HTML is /not/ a full utf-8 file, when ox-bibtex reads
it with [[help:insert-file-contents][insert-file-contents]], the automatic encoding detection opts for
japanese-shift-something, and I get katakanas instead of é.

Helpfully, the documentation of insert-file-contents mentions that you can set
[[help:coding-system-for-read][coding-system-for-read]] locally to override the automatic detection.  I did that,
and nothing changed.  I tried to understand and debug it for a good half an
hour, when I realized I had only changed the coding system for one call to
insert-file-contents, but it was the second call that created the string that
went into the exported manuscript.

The key names are still mangled in the HTML, but it’s only cosmetic, as the
links internally use other, correct key names.  Maybe changing the bibtex style
from alpha to something else would fix it.

Oh!  And for some reason, bibtex2html returned an error initially because it
could not find a crossref.  I put the crossref before the entry that made a
reference to it.  Apparently, bibtex2html does not like that, as putting the
crossref /after/ the entry that calls it worked.

** [2016-03-08 mar.]
*** Visualizing JavaScript snippets
So I’ve got a couple contributions that are mainly based on particular pieces of
code.  The straightforward thing to do would be to just put these snippets into
the manuscript alongside some text explaining what it does.

The issue I have with that is that I seldom read snippets of code in papers,
articles, or books.  If I’m learning the language, or a specific pattern, or
coding trick, then of course the code is relevant.  The code is what is being
talked about.  But when exposing data structures, control flow, algorithms, or
optimizing techniques, the explanation is better served by a diagram or an
animation.

Code is truth.  But the code can be full of details irrelevant to the message at
hand.  Maybe you are trying to explain how your algorithm handles text
sequences, but are showing your code in Python which deals with dozens of corner
cases, error handling, and tangential unicode workarounds.

The fallacy of thinking that the code can be clearer than a diagram is that we
live in our own code.  We build it, we improve it, we construct a working model
of it in our mind.  So when we read that code, we actually reason abount it
using our mental model.  And then we are tempted to include it verbatim
alongside prose, because we are comfortable reading it.

But another reader would not be comfortable with it at all.  Another reader has
no mental model of this code.  He must work through it line by line.  And that’s
where the accidentals, all the extraneous information is harmful for their
understanding.

And more importantly, the reader might not even /know/ the language semantics.
Without them, he might as well be reading Chinese.

That’s why, in the manuscript, I don’t want code to be the main way of
explaining the things I do in JavaScript.

Opening the module pattern, for example, is essentially a process of
manipulating scopes.  Visualizing how this manipulation takes place, and how it
differs from the standard module pattern and alternatives should be enough to
understand it.  The code can then be derived once the idea is understood.

To that end, I find that the presentation in the DLS paper was pretty clear.
Describe the code problem in terms of scope diagram.  Sketch the solution in
these scope diagrams, and present a way in code to achieve the sketched
solution.

But the diagrams are still somewhat suboptimal, in that they need to be
accompanied by code to know how they fit in the /timeline/ of execution.  A
scope diagram is just a snapshot at a fixed point in this timeline.  So, when
giving code snippets, the reader is still relying on me to choose the relevant
snapshots and not mess up my translation from the JS semantics.  The only way
that I have to link these snapshots together is to go back to code.

I wish I could find a way to work /directly/ with this diagrams, and be able to
express modifications of these diagrams visually, step by step.  Say, “set the
property ‘a’ of ‘m’ to the number 1” would have an unambiguous counterpart in
the diagram language (change /that/ box to 1).

The interactive visualizer I was building in January would give me part of the
solution.  I could manipulate directly diagrams as if it was code.  But those
diagrams are still /static/, they do not convey any information to make a
computer /do/ something, to animate, to make it beep and blink.

I guess the best way to describe it is: I can give instructions to turn a module
pattern into an open scope pattern, by adding “with” and a scope object around
the inner code of the function.  But how would I give /visual/ instructions to
turn the scope diagram of a module pattern into the open scope diagram?
Instructions that would work for any module, regardless of the specific names,
values and functions it contains?

If the diagram can be self-sufficient to describe the computation, then I can
dispense of the code entirely.  The diagrams would be a different, simpler
language that would help me get the point across more directly than bringing the
full weight of JavaScript.

** [2016-03-09 mer.]
*** Setting up auto-deployment of HTML export on Github pages
I’ve been using the HTML export locally for weeks.  It would make sense for
others to be able to read the same thing in their browser.  Pushing it to
a gh-pages branch is a no-brainer.

Of course, I didn’t want to build, commit, and push the gh-pages manually.  I’ve
done that for other projects, and it’s terribly error-prone.  You have to keep
track of which files to add, which files to delete...

So, git hooks!  I figured that a post-commit hook to master would automatically
make the HTML export, stage and commit the changes in the gh-pages branch with a
generated message.

Then, a post-push to master would push the gh-pages branch to the Github
remote.

First things first, the html folder was not standalone: the img folder was just
a symlink to ../img, where PNG and SVG are created by the Org export and
svgsplit during make.

Solution?  Just tell the makefile to copy all files in the img folder to
html/img after they are created.  But there was a dependency problem.  The files
in img need to be copied /after/ the Org export took place.

So I tried to add the HTML output as a dependency, but as I was using the
wildcard function for creating img targets, the wildcard would only be run once
on the images that were present /before the export/.  After a make clean, there
would be nothing in img, and the rule would only trigger in a second call to
make.

I wanted one call to make, and idempotent calls: “Nothing to be done” means that
every target is up to date.

I tried to RTFM.  I learned how Make processed the variables (learned about [[ but I had no way of telling make to wait for that process to end /and/ still copy each file in img without
 https://www.gnu.org/software/make/manual/make.html#Flavors][recursively expanded variables versus simply-expanded]], as I’d been using the
former all along), but that did not help me find a solution.  Tried to use
wildcards in prerequisites, using both the wildcard function and the asterisk
syntax.  Nope.

In the end, the manual mentionned that directories would have their timestamp
updated whenever their contents changed.  So I figured that instead of copying
every file of the img folder, I could just copy the img folder itself!  Make
would use its timestamp to know when to process the rule, and this would be much
cleaner.

And it worked.

Now, I had a standalone HTML folder which was updated every time I called make.
I cd’ed into it, git cloned the phd-manuscript repository, checked out the
orphan gh-pages (following the [[https://help.github.com/articles/creating-project-pages-manually/][Github doc]]), clean tree, add, commit, and push.
This was the usual process, except that the html folder, where make builds, is
always checked out to gh-pages.  Rather than switch between master and gh-pages,
I have two folders that correspond to these branches.  And the gh-pages branch
folder is inside the master branch folder.

#+BEGIN_EXAMPLE
.
├── html
│   ├── img
...
├── Makefile
├── Makefile.latex
├── manuscript.org
...
#+END_EXAMPLE

Then, I could finally write the hooks.  Rather straightforward, as they are just
shell scripts.

As a final touch, I put the hooks into a bin/ directory and checked them into
the repository.  Makes more sense to keep track of this stuff if I work on a
different machine.  Or if I want to use the same kind of setup for another
project.  I was initially thinking of symlinking .git/hooks/pre-commit to
bin/pre-commit, but this is not always a [[https://stackoverflow.com/questions/3462955/putting-git-hooks-into-repository][good idea]], security-wise.  Better to
take the habit of copying them and updating them manually.

** [2016-03-11 ven.]
*** Smart pagination in HTML
Not having pages in the HTML export is a /good thing/.  It tremendously
simplifies the placement of figures and text.  Everything flows linearly on a
single page.

However, it does not eleminate the need to think about figure placement.  If
some paragraph describes a figure, it makes sense that the reader will want to
look at both at the same time.  The figure and the paragraph should both be in
the window.

We could define virtual pages, bounding boxes of content that is better shown
together.

[[file:svg/smart-pages.svg]]

Here for instance I have four paragraphs and one figure.  The paragraphs above
and below the image refer to the figure, so I put a bounding box around them.

We can then use this bounding box as friction point when using page down/page up
and when scrolling in the browser.  Like window snapping in a window manager.

This is to prevent paging down inside the top paragraph.  We can keep the top
paragraph whole.

Same thing for the second bounding box.

Actually, maybe only the top anchor is really needed.  But the full bounding box
can still be useful when thinking about publishing on different screen
resolutions.  For mobile readers, we could imagine that some reflowing happens
to try to keep all the content of the bounding box is visible.

*** Keeping track of pages without page numbers
Page numbers are very useful to identify a page uniquely in a printed document.
When copy editing the document, or referencing a particular paragraph, or line,
having the page number at the bottom is necessary.  Imagine having no page
number at all, you would have to say: “The third page after the beginning of
section 2.3.  No wait, fourth page, top-half”.

But there is an unfortunate side effect of page numbers: you can use them as a
proxy to determine if a thesis manuscript is “good enough”, or “too much”.

Is there a way to uniquely identify pages without being able to know how many
pages there are (beyond manual counting)?

To identify the pages uniquely, you just need a unique symbol at the bottom of
each page.  A word can be used.  A word can be pronounced, while a random
graphical symbol (unicode or emoji) might not have an obvious pronunciation.

But a word is not as useful as a number.  If I tell you “page dolphin, 3rd
paragraph”, where is page dolphin?  Unless you know the page words by heart, you
wil probably have to shift through the pages to find exactly where it is.  There
has to be a /meaningful order/ between the words.

A total order has no ambiguity at all, and serves the purpose well.  Now if you
are looking at page elephant, you know page dolphin is before, but after page
cheetah.  It’s as efficient as numbers, if you know your lexicographical order.

But it as advantages that numbers don’t.

First, you cannot compare books based on the page count anymore.  Unless you
count all the pages manually.  But the goal of a proxy like page numbers is to
/avoid/ working too much in order to produce a superficial judgment.  Of course,
you could go by book weight instead, but oh well.

Also, words can give a sense of progress through the book.  Like 11/150, but
books never give you the total count on the bottom of each page.  But if you use
words that begin with ‘a’ on the first few pages, going to words beginning with
‘z’ to the last few pages, and using lexicographical order, then looking only at
the bottom of the page you have an idea of where you are.

You could even instill further conventions, like using the same first letter for
all pages belonging to the same chapter...

And the best part is, you can choose the words!  As long as there is a clear
order for the reader, then they retain their usefulness as page locators.  But
you can be subversive, poetic, clever, or descriptive while choosing the words
to match a page.

One thing I can see working is to built words out of three or four random
consonants, then add random voyels in-between.  Order them by lexicographical
order, and map them to the sequence of pages.  Pronunceable, potentially fun
words.

Otherwise, you can choose among a list of dictionary words that are rarely used
in common speech.  There are tons of those, and some can be really recombulant.
Just chose words that you like, order them, and voilà.  It gives you a perfect
excuse to learn of new words that are outside your domain.  And it gives those
words a purpose.

Idea of the year right there.

** [2016-03-18 ven.]
*** Finding duplicates in code
As I translate the DLS paper, it is evident that the core motivation of the
whole endeavor is to eliminate (or, rather, minimize) duplicated code.

Duplicated code is more code to maintain.  A fix in one part will have to be
mirrored into the duplicates.  Factorisation is often the prescribed medicine.
Once duplications are factorized, there is only one path to maintain.

So, in DLS, we find ways to structure the intepreter and the instrumentations
without repeating as much code as the previous instrumentation.  Benefits
abound.

But there are drawbacks.  Using ~with~ is a dirty hack. And the control flow of
the whole program is not necessarily cleaner.  The base interpreter is simpler,
because devoid of instrumentation-specific code, and has no facility for
injecting code (other than ~with~).  But to understand instrumentation code you
still need to understand how the interpreter works.  And to understand
concurrent instrumentations, ... well.  You need to consider the order in which
they are activated, and look at what pieces of the interpreter they override.
The point is, it’s not free.

By now I must face the evidence that there are no free solutions, only different
compromises.  It’s interesting that way too, chasing the nuances.

There is an alternative to duplicate code elimination.  What if we could live
with this duplication?  After all, it’s the /downsides/ of the duplications we
don’t want to live with, the burden of maintenance.  But what if this burden was
eased by tools?  We would not have to pay the price of inadequate abstractions
in clarity and performance.

My main grief with duplicated code is the need to replicate changes to all
duplicates.  If I have the /exact same lines/ repeated over and over, then
factoring it into a function is a no-brainer.  But often, there are a few
variations, and then a few others, and then it turns out you need the code to do
something else entirely.  And you are left with a function that is called not
twice, but only once.  Do you keep it?  It’s an indirection.  When you read the
code, it’s just a verb, so you have to jump to definition just to know what it
does.

So sometimes you feel a pattern, a /groove/ that warrants abstraction.  But then
later you need to undo this abstraction to get its components out, and rewire
them.  But still, you feel it is part of a coherent unit.  Is this unit a
function, or just a paragraph?

In Inkscape you can group SVG objects together and treat them as one unit.  But
then you can always access the individual components, even if they are part of
the group, to copy them elsewhere, or change them.  And you can as easily
ungroup them.

That could alleviate the cost of going back and forth between abstracting and
destroying these abstractions.  “Group code”/”Ungroup code”.  But that opens the
problem of /saving/ those groups.  In the source file itself?  In a metadata
file?

To avoid the manual replication of changes to duplicated parts, the editor can
take over.  “Do you want to replicate those changes to these parts, which have
stayed in sync so far?”.

Detecting duplicate changes seems computationally intensive.  But, it’s
fortunately a problem that has been [[https://en.wikipedia.org/wiki/Duplicate_code#Detecting_duplicate_code][investigated]].  [[http://pmd.sourceforge.net/pmd-4.3.0/cpd.html][PMD]] mentions using [[http://www-igm.univ-mlv.fr/~lecroq/string/node5.html][Karp-Rabin]]
string matching after having tried two other algorithms.  And the [[http://students.cis.uab.edu/tairasr/clones/literature/][literature]]
over University of Alabama is absurdly long.

[[http://www.iam.unibe.ch/~scg/Archive/Papers/Rieg98aEcoopWorkshop.pdf][One technique]] that is intuitive is to compare all the lines of code of a
project, and draw a matrix.  A dot indicates identical lines.  A diagonal is a
repeated sequence of lines.  So you can see at a glance the amount of duplicates
in a project (and then, jump to the file to inspect further).

*** A leitmotiv for the manuscript
THE theme that has been going through all my work during these three (plus)
years, that has been my constant adversary, the greatest difficulty to overcome,
is the tension between static, safe, predictible programs and dynamic,
unknowable, but reconfigurable programs.

The thesis started with AOP.  AOP allows one to extend a program, statically or
dynamically, in a way that makes the running behavior of the program difficult
to predict.  If you have a ~cflow~, you cannot say anything about the code
unless you know exactly what happens at runtime.

Then extending interpreters in Haskell, wanting to mix AOP with strong static
guarantees thanks to Ismael’s AOP library and the type system.  Turns out, the
evident ways to extend the interpreter are also hard to type, because if you
want full flexbility you are turning an AST -> Value function into an AST -> M
Whatever, so you need to be free on the return type and effects.  So the
static/dynamic duality only leaves you with cumbersome ways to deal with that.

Not sure if the free monad changes that perspective.

Then it’s clear in the JavaScript works, that the dynamic nature of the language
allowed greater flexbility.  Of course, the cost is paid in static guarantees,
and possible optimizations.

Behind that theme was the question: why can’t we have modular programs first?
Why is it so hard?  And the answer is, primarily because there are multiple
tensions at work, and when you design a solution, you have to take all those
tensions into account; they are the dimensions of your design space, and any one
solution can never satisfy all the tensions at once.

** [2016-03-24 jeu.]
*** SVG export not cropping to the selected element
Rogntudju.

This one has been driving me nuts.  rsvg-convert, /sometimes/, decided to leave
huge whitespace around exported SVG when using the ‘export-id’ option.

I found a minimum example: a simple SVG path won’t be cropped correctly.

Diving into [[https://github.com/GNOME/librsvg/blob/master/rsvg-convert.c][the source]], I found that to crop, it first draws the selected object
through Cairo, and then gets the bounding box of the render area.  And it turns
out that the bounding box of the render is /not/ the bounding box of the
object.  The render always starts at (0,0) when drawing a path, then executes
the path commands.  The bounding box is then always (0,0)--(end of path).

The render bounding box is the union of all the bounding boxes of the objects
that are rendered.  And, in ~cairo_render_path~, there was a special case to
take into account the bounding box of the filled area of a path, /even if the
fill wasn’t set/.  This was made to accommodate a use-case for GNOME SVG icons.

Discarding this bounding box when no fill is set fixed the issue.

While I’m in the code, the bounding box was one pixel too narrow, because the
computations are done with double precision, and rsvg-convert those to ints,
without rounding.  I just added 2 pixels to the final width and height, to
ensure we don’t lose any part of the SVG (1 pixel for the ‘width’ rounding, and
1 pixel for the ‘x’ coordinate of the top left corner).

I made a [[file:bin/librsvg-bbfix.patch][patch]], to ensure I can reproduce the build at a later point.

** [2016-03-25 ven.]
*** Interactive SVG diagrams
I can highlight parts of the SVG diagrams just by hovering the mouse over words,
just like Org already highlights line numbers for code examples.

It’s a matter of selecting the element in the SVG, and toggling a class.  But
for that, I need to be able to target the right element.

I should use id, but rsvg-convert discards all the meta data from the extracted
SVG :(  Since this a side-effect of rendering to SVG using Cairo, there’s not
much I can do; there are no guarantees that a path in the original multi-SVG
will end up as a path in the extracted SVG, and I can’t find any way to leave
labels intact after rsvg-convert.

That would, of course, work if extracting through Inkscape.  So it’s not out of
reach for a final version.

But I would also need to add the necessary syntax to Org; probably as a custom
link type.  And then add a simple JavaScript function to be called on mouseover
and on mouseout, just like code highlighting.

** [2016-03-31 jeu.]
*** Publish hooks do not use the commit version
The hooks I’ve written do not really use the working tree.  If I have pending
changes that are not committed yet (WIP), the hooks will ‘make’ the file in the
directory, /with/ the uncommitted changes.  That means that when I push, I push
the version with WIP.

The correct thing to do would then be to build the thing for each commit.  But
that would take more time.  And also that would complexify the hook scripts.

For now, I just need to remember that pushed version = WIP version, not HEAD.

** [2016-04-11 Mon]
*** Setting up a new machine
My Dell laptop motherboard is fried.  So I got an old Core2Duo with HDD that was
lying on a shelf, and put ArchLinux on it.  Another Dell; hopefully it will last
for the remaining months.

And now the manuscript exports to HTML!  The repository is /mostly/
self-contained.  Other than installing the tools to extract the SVGs, there are
hidden dependencies to get a correct output, like the right fonts.  Not sure
about bundling the fonts, even if they are under an open license.  Will just
add them in the README for now.  Same for the body font.

-----

Ah, forgot the hooks.  And to setup the html folder as a git repository pointing
on the gh-pages branch:

: git clone manuscript.git html
: cd html
: git checkout gh-pages

** [2016-04-12 Tue]
***  Also forgot the Inkscape palette
But I actually [[http://goinkscape.com/custom-color-palettes-in-inkscape/][found]] a better way than putting a GIMP-format palette of RGB
values in ~.config/inkscape/palettes~: using swatches.

For the fill (or stroke) color of an objet, instead of “Flat”, use “Swatch”.
This defines a “swatch” with a unique identifier.  You can change the color of
this swatch using the wheel or RGB values.

Then, any new object can be assigned this swatch as fill or stroke color.  This
is like an indexed color in GIFs, so you can just edit the swatch and change all
the object that have it at once!

And bonus, all swatches of the current document appear in the “Auto” palette.

** [2016-04-21 Thu]
*** Small typographic changes
Finally noticed what had been bugging me with the font rendering of the HTML
manuscript: the x-size changed from letter to letter!  This was only apparent at
some font sizes.  I fixed the various font size in pixels to make sure the
rendering was right for these sizes.

And I switched fonts for Charter, which is more legible.  I like Fira Sans for
the headlines.  And Source Code Pro is good for the source blocks.  It keeps me
on my toes, having a fresh font I’m not used to.

** [2016-04-25 lun.]
*** In other news...
The V8 team is [[https://groups.google.com/forum/m/#!msg/strengthen-js/ojj3TDxbHpQ/5ENNAiUzEgAJ][dropping strong mode]].  While there were semantic difficulties in
"locking down" the language (especially forbidding mutation of class
properties), one reason that caught my eye was the implementation complexity:

#+BEGIN_QUOTE
Strong mode tweaks many small bits of the language semantics (see the list at
the end). Consequently, it requires special-casing all over the compiler(s), run
time, and libraries. In V8, it amounts to literally hundreds of new code paths,
and that does not even include libraries. As you can probably imagine, VM
implementors are not happy about such added complexity, and it gets in the way
frequently. While some of that was to be expected, it was worse than we had
anticipated.
#+END_QUOTE

** [2016-05-04 mer.]
*** How to recompile librsvg in Archlinux
To apply the patch to the latest version.

Update ABS (just for this package):

: sudo abs extra/librsvg

Copy the folder to a build dir:

: cp -r /var/abs/extra/librsvg/ librsvg

Copy the patch to the folder.

Modify the PKGBUILD by adding this before the ~make~ line in the ~build~
function:

: # Bounding box fix
: patch -p1 < ../../librsvg-bbfix.patch

Then build and install the package:

: makepkg -sri

*** RG image processing was looking for functors?
[[cite:MKL-97][MKL-97]] was an image processing system in Lisp.  Each filter was a pure function
that took one or two images as input, and returned a new image.

#+BEGIN_SRC haskell
or = map ||
and = map &&
remove a b = and a (not b)
#+END_SRC

While the filter were nice to compose, they were inefficient because of
redundant allocations and iterations.  So they created a custom language that
did loop fusion and object pooling.  Essentially, a custom compiler.

Now if we look at how the filter compose, it looks like the property they rely
on is that ~map~ distributes on function composition:

: map (f . g) = map f . map g

Which is the [[https://en.wikibooks.org/wiki/Haskell/The_Functor_class#The_functor_laws][second functor law]].  And based on that equivalence, GHC for
instance is able to [[https://wiki.haskell.org/GHC/Using_rules][rewrite]] code and generate only one loop when this is
advantageous to do so.

** [2016-05-05 jeu.]
*** Cleaning fonts in the diagrams
Found a few tricks to inspect and change the fonts all at once.

To list the fonts used by the diagrams:

: grep -io 'font-family:[^;]\+' svg/*  | sort | uniq -c

In the font selection droplist, Inkscape has a "Select all objects with this
font".  Then you can change the font for the selected objects.

Otherwise, there is Edit -> Find and Replace to match on XML attributes, like
font-family, and replace them all at once.

But, for some reasons, I have text objects that hold only paths, not text.  So
the text object node is redundant, but I don't know of any way inside Inkscape
to change that.  Especially since the text objects hold the matrix transform to
place the paths...

Maybe it doesn't matter anyway because rsvg-convert traces all text to paths.
So the redundant node might disappear in the end SVG.

** [2016-05-09 lun.]
*** Mixed concerns in the state of the art
In the state of the art, I want to introduce the different mechanisms that could
be used to solve the problems that arise when trying to instrument Narcissus in
a clean way.

But, just listing the mechanisms out of context is missing a big piece of the
puzzle.  These mechanisms all have a history, a reason for existing, and for
being devised.  They were proposed for solving specific problems.

I don't want to just bring a bag of tools, take each of them out of the bag and
say "nay that's no good for this problem", "neither is this", "this could work,
but let's keep looking"...  There's no insight to be gained from that way of
explaining things.  That's no explaining at all!

I feel there is more to be gained by looking at the problem with a larger angle.
Insight, and putting the problems and solutions in context is the more important
contribution.  The technicalities of using ~with~ are not.

So, the first "chronological" part introduces most of the context as well as the
mechanisms.  And the second part focuses on the mechanisms that we keep as
eligible.  Then the second contribution chapter goes through the mechanisms and
shows how they can solve the problem, or not.

** [2016-05-10 mar.]
*** Odds and ends
HyperJ is an instance of superimposition mechanism.  Mention that in the
mechanisms part.

Methodology of HyperJ goes further than AOP: it wants morphisms between
requirements and source code, whereas AOP merely wanted to maintain a
correspondence between design units and program units.  This leads to
model-driven engineering, where requirements and documentation are specified in
many ways, and code is synthesized.

Expression problem warrants a section in state of the art, because it will come
up often.  Wadler named it, many solved it (Oliveira, [[cite:OZ-05][OZ-05]], trivially
@Modularity16).  Interesting point of view is not to say Scala > all, but show
how different language mechanisms interact to first make this a problem (static
function and type definitions), and the compromises required to make it go away.

Last chapter in the "chronology" might be functional programming.  Even though
it's certainly a parallel evolution, it fits the trend of presenting paradigms à
la mode for solving our modularity woes.

Parnas and Naur fit because they eschew source code, and that's something to
keep in mind all along.

There's nothing that prevents the AOP mechanism to be used without following the
methodology.  One can use aspects to provide functionality.  Whether that's a
good idea depends on the specific requirements.

** [2016-05-11 mer.]
*** Tightening the state of the art
The chronological part can be seen along the following themes:

- Taming control flow (structured programming)

  Procedures as units of logic, no arbitrary jumping in or out.  Foundation for
  reasoning about programs locally.

  A picture of [[https://en.wikipedia.org/wiki/The_School_of_Athens#/media/File:Sanzio_01.jpg][Eiichi Goto]] and Furomu Komo (let's see who gets this one).

- Modeling the world (OOP, smalltalk & self)

  Matching the code structure with the problem domain.  And encapsulation,
  information hiding, abstract data types.  Philosophical tangents.

  Raphael's [[https://en.wikipedia.org/wiki/The_School_of_Athens#/media/File:Sanzio_01.jpg][The School of Athens]].

- Reifying languages (reflection, open implementation, AOP)

  Programs alter the language semantics.  Intercept lower-level events to
  implement custom behavior.  Reflection is unbounded, AOP is more disciplined.

  [[https://commons.wikimedia.org/wiki/Category:Illustrations_of_Baron_von_M%25C3%25BCnchhausen_by_Gustave_Dor%25C3%25A9#/media/File:Gustave_Dor%25C3%25A9_-_Baron_von_M%25C3%25BCnchhausen_-_037.jpg][A Munchausen illustration]].

- Weaving multiple views (literate programming, HyperJ, information transparency)

  Admit that a program has too many dimensions to be captured by any formalism.
  Organize the thing the way you want, and recompose using tools.

  [[https://commons.wikimedia.org/wiki/File:Holbein_XVI_02.JPG%0A][A Turkish carpet]].

Damn, now I guess the SVG folder is not aptly named anymore.

** [2016-05-12 jeu.]
*** Preview inline image in Emacs
is a nice feature.  But some images are too large.

There's an custom variable to override the width of inlined image:
[[help:org-image-actual-width][org-image-actual-width]].  But if set to a number of pixels, all inlined images
have /exactly/ this width, even smaller ones.  And it rasterizes SVG through
imagemagick, which is slower and messier.

I can change the code to use ~max-width~ instead of ~width~, but the most
important change is to not use imagemagick for SVGs, which can be done by
looking up the image-type before hand with [[help:image-type][image-type]].

** [2016-05-13 ven.]
*** Thematic resonance
In the mechanisms review of the state of the art, it would be great if color in
diagrams always had the same meaning.  Blue for concerns, for instance.  That
way we can see at a glance how the mechanisms differ.

*** Modern compiler construction
[[https://channel9.msdn.com/Blogs/Seth-Juarez/Anders-Hejlsberg-on-Modern-Compiler-Construction][Good talk]] by Hejlsberg on how modern compilers must interact with IDE first.
Compilers from the dragon book make optimistic assumptions about the code.  It
is basically syntactically correct, and there are no types errors.

But if programmers want accurate code completion and quick error feedback, the
compiler should not waste its work every time a character is changed in the
source.  Lexing, parsing, type checking is costly.  When a token changes, only
recompute what is needed, and reuse what did not change.

The C# and TypeScript approach is to provide a language server that can interact
with any IDE.

** [2016-05-20 ven.]
*** A simple overview
State of the art right now is more a "context" part.

Then should come the detailed description of the problem.  Begin with a study of
Narcissus to motivate and ground the thing in a real use-case.  Then generalize
to give an overview of the approach:

Spec -change-> Spec'

A change adds a variation to the Spec of the program.  If we have a formal spec,
we can apply this change on it and have a changed spec.  Change is the delta
between the two specs.

The program is implemented.  We have some code that generates a runtime process
that behaves according to the spec.

Spec -> Code -> Program -> Spec

The change causes a change in Code.  Interestingly, a small change in Spec can
have a large impact on Code.

One the goal of the programmer is to make sure the program is correct: acts
according to the spec.  The programmer writes Code, to create Program.  When a
change of Spec arises, it's best if the change to Code is self-contained.

The question is then: how to minimize the impact of change?  How to construct
Code to minimize impact?  What patterns to use?  Do languages make a difference?

Spec changes can be anticipated, or not.  Anticipated Spec changes can be
factored into Code design.  Unanticipated Spec changes cannot; they might have a
small or large impact, depending.

We are not tacking just any Program or Spec, but interpreters.  And a specific
kind of change: adding analyses.  How is that more specific?

Some solutions will be specific to that use case.  Some will apply to any
program.  Lessons learned will be valuable for all.

/Then/ should come a review of the related work.  When the problem is defined.
That cuts the State of the Art chapter (needed), and makes more sense.

I don't know whether I would merge the "Variations" with Related work though.
On the one hand, it grounds the discussion of the related work and provides
concrete arguments to judge a mechanism.  On the other hand, it blurs the line
between contributions and related works.

** [2016-05-26 jeu.]
*** Fixing citations in caption not exported
ox-bibtex was not exporting cite links in CAPTION for source blocks or figures.
This was caused by the use of [[help:org-element-map][org-element-map]] which, by default, does not map
over elements in CAPTION.  This was [[http://thread.gmane.org/gmane.emacs.orgmode/94369][raised]] on mailing list, and the solution is
to use the last optional argument ~WITH-AFFILIATED~.

I had to change the /two/ uses of ~org-element-map~ in ox-bibtex to pass this flag.

** [2016-06-02 jeu.]
*** A coherent thesis
Modularity has always been my concern from the start.  How to write modular
programs?  Is it a question of language?  Of paradigm?  Of mechanisms?  Of data
structures?  Of design patterns?  We all want it, but /what/ is it?  Why do we
need it?

History is replete with people arguing over modularity.  Tanenbaum versus
Torvalds.  Micro-kernels are more modular.  Enthusiasm for functional
programming is fueled by the belief this style leads to more modular, simpler
programs.  The whole context chapter.

First, the why.  The section "From program to binary" shows what a program is,
defines source code versus program versus function.  Different sources can give
the same program and function, so there are many ways to write programs that are
/functionally equivalent/.  If they work the same, why bother?  To a casual
observer, a programmer just pushes symbols around to make the computer jump
through the intended hoops.

For some reason, we didn't stop at assembly.  We had it all.  Turing complete.
Everything that could compute, we could write in assembly.  But, we created
languages, more and more languages.  Even by 1970 (Landin) we had more languages
that we could keep track of.  And the trend is still growing.  New languages
keep popping up.  Why is that?

If you ask any programmer today, pretty much none will program in assembly.
They use higher-level languages.  And most have personal preferences for select
languages.  Why that preference?  What difference does they make?

If we look at all of the languages, what is the main claim they have in common?
They are better than the previous ones.  Better at what?  Better for writing
programs!  But if they are functionally equivalent, how are they better?  They
claim to make your programs clearer, simpler, more readable, more modular,
faster to write, with less defects, and easier to maintain.

Quelques extraits de promesses: [tirées, pêle-mêle, des sites officiels des
langages Scala, Go, Rust, Ceylon, ...]

Scala frontpage:
#+BEGIN_QUOTE
Construct elegant class hierarchies for maximum code reuse and extensibility.

flexible; high-level of abstraction; very enjoyable
#+END_QUOTE

Haskell frontpage:
: Declarative, statically typed code.
: With lazyness, programs can compose well.

Declarative and safe.

Rust:
: blazingly fast, prevents segfaults, guarantees thread safety.

Fast and safe.

OCaml:
#+BEGIN_QUOTE
OCaml is a general purpose programming language with an emphasis on
expressiveness and safety.

powerful; clean and elegant; fast, unobtrusive; portable; sophisticated;
expressive; efficient.
#+END_QUOTE

Go:
: Go makes it easy to build simple, reliable, and efficient software.

Ceylon:
#+BEGIN_QUOTE
Ceylon controls complexity with clarity.

porwerful; readable; predictable; simple, intuitive; regular; elegant; express
more, more easily.
#+END_QUOTE

The functionality of the program is not the sole finality of the programmer.

Programmers have to write the program, and they want the process to be as
enjoyable as possible.  Writing can be a long process, spanning months, even
years, involving one programmer, to teams of thousands, all working on the same
program.  So they want to minimize the pain of this process.  In fact, most
languages claim you will say more by writing less.  Like all things in this
universe, programmers tend to zero entropy; they aim to work with minimal
effort.  Programming languages are sold as productivity tools.

But not only programmers have to write, they have to maintain.  They have to
live with their programs for a long time.  They have to fix bugs, add features,
optimize performance.  And they want maintenance to be as effortless as
writing.  Here as well, each programming language promise to be easier to fix
and refactor than the previous one.

Safety is a good argument for maintenance: the fewer bugs you write, the less
you have to fix.

Efficient, fast: "you won't miss assembly".

Expressive: "you will say more with less code", "you will deliver faster".

Sophisticated, clean, elegant: "your code will be eligible for the Prix
Goncourt" (?)

Extensibility, code reuse, says "you will add features easily".

Modularity is kin to the last category.  A modular program is easier to
maintain, easier to write, easier to understand, easier to fix, and easier to
extend.  A modular program is the grail that minimizes effort.  It's a cure-all,
but an elusive quality.

What is modularity?  What is a modular program?

I don't know if I can define that quality.  But I can tell you how /not to/.
There is no formula for quantifying modularity.  You can't say, "apply this
criterion to the program, and out comes a measure in Milli-Parnas!".  There is
no reliable measure.

Some say a modular program has fewer lines of code that a non-modular one.  This
is not even an appropriate measure for /any/ program.  Any program can be turned
into an equivalent one with more lines of code: just add no-op statements.  But,
you say, only measure the lines of "effective" statements.  That's Kolmogorov
complexity then, not modularity.

Some say that a modular system has fewer /couplings/.  How do you measure that?
Well, you can count the number of dependencies between modules.  Explicit
dependencies that are declared in the code.  But there are often /implicit/
dependencies as well, not written in the code, but assumed by the programmer.
How do you measure that?  If you don't, you risk saying that a program is more
modular, when in effect it has more implicit couplings.

------

The context chapter is a tour of some of the usual suspects for modularity.
Dijkstra was looking for modularity.  So was Knuth.  OOP, AOP.  They all claim
to write better programs, simpler, more understandable.  Is this substantiated?
How is it clearer exactly?

The problem chapter establishes the example, in order to bound the research
area.  If can settle the modularity issue on a non-trivial program, it would be
a start.  The first step is defining what we mean by modularity and
extensibility.  The context showed plenty of examples.  Make a definition we
will use from all that.

The rest of the document is then exploring these questions: what makes a program
extensible?  Modular?  How do languages intervene?  Mechanisms?  Patterns?

Variations tries out different paradigms and patterns to extend an interpreter.

Building with modules leverages late binding to build a extensible interpreter
dynamically.

Open scope leverages late binding again to turn Narcissus into an interpreter
that can switch behaviors dynamically.

Conclusion: late-binding for extensibility?

*** Framing the acknowledgments
There are few people on Earth that bear the qualities of diligence, generosity,
curiosity, that are eager to learn, and who not only value ethics and
intelligent discourse, but have a good sense of humor.  So it was a great
privilege for me to call a few of those people my friends.

** [2016-06-03 ven.]
*** A coherent thesis, continued
Yesterday's plan expanded into an unfocused mess.  Threads that I want to pick
up:

- Why do we still have new programming languages?
- How is modularity linked to extensibility?
- What's my thesis?

**** Why do we still have new programming languages?
Even to a casual observer, C is more readable than assembly.  That's an argument
for the existence of C, but does it still apply to languages that came after?
The casual observer might not feel a sensible difference between C and Haskell
for instance.  But a programmer who knows both will have many things to say
about each of them.  So maybe readability is satisfactory once you go above
assembly.

Languages nowadays claim /safety/ and /easy concurrency/.  It's easy to convince
a non-programmer that a safe language is better than an unsafe one.  The choice
of vocabulary makes this a self-evidence.  Arguably, this is how research into
type system gets funded.

Concurrency is a real issue.  Programming concurrent system is hard, so any
language that ease it is welcome.

Modularity, or extensibility, is not often touted.  Too elusive?

**** How is modularity linked to extensibility?
An extensible program is one that is easier to extend than a non-extensible
one.  A modular program is one that is built with low-coupled modules.
Following Parnas, a module corresponds to design decision that are likely to
change.  Consequently, changes in a modular program are likely to affect only
one module at a time.

The program is likely easier to extend if changes are localized to a module
rather than affecting the whole.  Modularity enables extensibility.

**** What's my thesis?
Yesterday I started with modularity.  But my work has revolved more around
extensibility.  The two are linked: modularity enables extensibility.  It is
thus worthwhile to understand how to build modular programs in order to build
extensible ones.

But "a better understanding" is not quite a thesis.  Though I would be satisfied
to just say "We wanted to better understand modularity and extensibility.
Here's what we found".

"We show a few ways to build extensible interpreters" is underwhelming.

"Better understanding how to build extensible interpreters and how to extend
existing interpreters leads to more code reuse, less maintenance overhead,
better programs."  A conceptual problem with practical costs.  This could work,
but the practical value of my results is low, so it should not be the focus.

More worthwhile is to try to understand bigger problems, deemed more important.
"Understanding how to build extensible interpreters and how to extend existing
interpreters leads to understanding how to design programs and languages for
extensibility in general."

The shared context is obvious: programs are hard to write, hard to maintain.
The practical costs are defects, software that does not work the way we want to,
or is not delivered in due time.  When a program requires new features, we often
tack them on the existing code, without regards to the structure.

Problem: How do we write extensible programs?  How do we extend existing ones?
What are the mechanisms we can use?  The design patterns?  Are some languages
better for extensibility?  Can extensibility techniques be used on any
program?

[I don't feel a /So what?/ here.  These are genuinely interesting questions.]

Solution: We analyze extensibility in specific applications: language
interpreters.  We study different ways to build interpreters, and how they can
be extended.  We also study mechanisms to extend existing interpreters that were
not built with a constraint of extensibility.  We apply these techniques to
extend a JavaScript interpreter at runtime.  We find that late-binding is an
essential ingredient for extensibility.

[Maybe the "more important problems" should go here, to answer "what's next?".]

*** Resuscitating the PDF export
Might have to send a working copy to the higher-ups to appease their wrath.

I've turned off special treatment of side figures for now.  So, they just appear
too large and they bunch at the end for now.

References work.

Need to brush up the front and back covers, so it looks legit.

Put something in the introduction as well.

Made a few changes to the Makefile in order to separate the HTML export from the
PDF.  No shared img folder.  PDF folder is for generated files only, TEX is for
sources.

Should probably simplify the handling of IMG folder for HTML as well.  One
output destination rather than copying things.

** [2016-06-06 lun.]
*** A nifty trick to debug Org
Use a source block of elisp inside the org document, and evaluate it there:

#+BEGIN_SRC emacs-lisp :results verbatim
(org-element-map (org-element-parse-buffer) 'link
  (lambda (link)
    (org-element-property :type link)))
#+END_SRC

*** Cleaning the PDF export
Handling side notes and side figures for now.

There are figures all over the place.  Section pictures are too large, and
listings are too far from where they are used.

Also, text is justified...

But, for Wednesday, what I need most is a cover and back-cover to make it look
legit.

** [2016-06-07 mar.]
*** Adding a cover
I just followed an existing thesis, looked at the (self-contradictory)
specifications, and put the minimal amount of required information.

Just vspace and vfill.

Left space for an illustration.

Same thing for the backcover: vspace, vfill, and minipage to have the two
abstracts side-by-side.

Found out that Fira Sans was too heavy, and used fontspec options to target
lighter faces.

** [2016-06-08 mer.]
*** Coquetteries
I ditched Tufte LaTeX, because it was too opinionated.  KOMAScript allows for
some easy customization, but most of the things I want seem to be covered by
Memoir: side notes, side figures, side captions, section numbers in margin,
custom TOC...

Will take a look when the times come.

** [2016-06-16 jeu.]
*** More rsvg shenanigans
librsvg updated to 2.40.16.  Had to adapt my patch (just moved the dimensions
fix a few lines).

Strange thing is, I /still/ had bounding box miscalculations after applying the
patch on some diagrams.  This was caused by arrows that had a fill color set.
But the fill color was useless, since arrows were straight lines.  Removing the
fill color in Inkscape solved the issue.

** [2016-06-21 mar.]
*** Extensibility related work... again?
The trouble of tackling modularity and extensibility is that you are /never
done/ with related work.  There is a seemingly endless supply of papers,
languages, mechanisms and authors that have tackled the problem.

Looking for jury members, I stumbled upon Alexandre Bergel's PhD thesis [[cite:Ber-05][Ber-05]]
about class extensions.  A class extension is a method that extends a class from
another module.  Classboxes allow class extensions with /local rebinding/, that
is changes made by the extension may only affect the extending class.  He also
talks about unanticipated changes as a motivation for class extensions.

So basically, what I've been trying to do, but in 2005.  And in
Smalltalk/Squeak/Java instead of JavaScript.

The good news is, he's definitely a good candidate for the jury.  The bad news
is he's living on the other side of the world, so he most likely will never be
able to attend.

The worst news is that in my PhD I've attacked modularity and extensibility
because the landscape was foggy, but I only made it foggier.  I wanted to paint
a clear picture of each language's strengths and weaknesses.  All the papers I
read, they all kept creating new ways of doing things that all seemed similar
under the surface.  I wanted to /avoid/ contributing to the noise.  But in
writing a manuscript about idiosyncratic ways of extending JavaScript modules, I
may have just done that.  [[https://xkcd.com/927/][Relevant XKCD]].

Oh well.  I've learned to let go of my inner taxonomist.  Simplifying the
picture was the impulse from an earlier me who preferred to deal with
simplifications rather than the full complexity of reality.  I now embrace it,
but my jury might not.

** [2016-06-22 mer.]
*** From the right perspective, complex can be trivial
Trying to expand the FOAL chapter, I now realize it is misguided.

There's a focus on making it work in JavaScript.  But really, the subset that is
in use here is a very simple language of functions and environments.
Environments are sets of bindings and can be extended by delegation.

Modifying operations is trivial when you realize you are defining a new
(mathematical) operation, and just give it the same name as another one.  So
it's exactly the same as adding an operation, just that delegation lets you
override existing names.

The trick of using multiple ~with~ to change the behavior of the interpreter is
completely backwards when you realize that it only works on expressions that are
declared inside the block.  That's not a realistic use-case for any interpreter
other than a toy one.

In a realistic setting, you would have built an expression object beforehand,
~e~, and you want to interpret it with semantic variations.

Then you could write:

: with (double) { with (double) { e.eval() }}

with the intent that ~with~ alters the semantics of ~eval~.

That reminds of Rust's traits system.  A traits is like a Java interface, a
collection of signatures.  But traits have to be explicitly imported in the
scope.  So you can do something like:

#+BEGIN_SRC rust
trait Eval { fn eval(self) -> u64 }
mod eval1 { impl Eval for Term { fn eval(..) { /* first eval variant */ }
mod eval2 { impl Eval for Term { fn eval(..) { /* second eval variant */ }

fn main() {
  let e = Term(..)
  { use eval1::Eval; e.eval() }
  { use eval2::Eval; e.eval() }
}
#+END_SRC

The trick is to recognize that a method call is just syntactic sugar in Rust for
a function call:

: e.eval() -> eval(e)

And then the ~eval~ function is chosen depending on what is in scope.  We would
have the same result by explicitly writing:

#+BEGIN_SRC rust
fn main() {
  let e = Term(..)
  eval1::Eval::eval(e);
  eval2::Eval::eval(e);
}
#+END_SRC

So, calling two different functions.  Because that's what we are actually doing
here, and in the FOAL paper as well.  It's just a trick to call two different
functions without changing the syntax of the method call.

The motivation really needs to be stated: we don't want to change the client
code, not even one line.  So we find mechanisms that allow to change how it
behaves without actually changing it.

** [2016-06-23 jeu.]
*** FOAL paper was about building incrementally
The FOAL paper then conflates two issues: solving the expression problem, and
changing the behavior of a program without touching its code.

Solving the expression problem is not difficult in a dynamic language, so why
would it be interesting?

Changing the behavior of a program by only altering its environment is more
interesting, and aligns with the goals of the DLS chapter.

On the other hand, there is also the aspect of extending the program
incrementally.  Each module object adds to the interpreter.  Then the client
code can combine all of these extensions.

But the motivation for building incrementally is to avoid touching the existing
code.  The idea is that new functionalities should translate to new code, and
not changes to existing code.

Then, modification of existing functionalities can be seen simply as new
functionalities.  Mathematically, the fact that they resemble an existing one is
irrelevant.  Redundancy is a concern for the programmer, because redundant
behavior is likely to cause redundant code, and redundant code is accidental
complexity.

Missing an "ingredients" section.  Why does it take to make it work in another
language?  What are the mechanisms at hand?  Because the point of the chapter is
not just to show we can do it in JS, but to give some insight of what is
necessary to apply it in other situations.  And the pros and cons of the
/specific way/ we do it here.

The pros and cons of pure extension in general are kept for the synthesis
chapter.

*** Is there a better term than extension?
Modularity, unanticipated extension, extension, building incrementally.  I've
used all of these interchangeably.  But I really need a good term to encompass
what I'm doing here, instead of different sentences.

Extending can be stretching.  We don't so much stretch a program as we add
functionality on it.  But importantly, we aim to add functionality without
touching the existing program.

It's like taking a mathematical function f, that does a certain thing, and then
changing what it does by composition: f' = g . f.  The new f' function is based
on f, but the result has been altered by g.  The function f itself has not
changed.

So we want our extensions to compose like functions do.

One obstacle to composing program extensions is that programs are not pure
functions.  Programs have side effects, reading and writing to the screen, to
the memory.  These side effects can be targeted by an extension as well.

Add is more appropriate than extend: we take something existing, and we /add/ to
it.  Add implies there is no change to the first thing, but the result is
larger.

Now, is there a word to say "To add functionality to a program without touching
the code of that program"?

** [2016-06-24 ven.]
*** Going back on this term
"To add functionality to a program without touching the code of that program".

I've used explicit/implicit modification to mean "with/without touching the
code".

** [2016-06-25 sam.]
*** This term...
Explicit/implicit.  Direct/indirect modification.  I've got this image of a
puppeteer that manipulates puppets through their strings.  Indirect manipulation
is adding strings to make the puppets walk differently.

Maybe "extend" is a decent term for "adding functionality".  But the main point
is to extend /implicitly/, indirectly, not in the usual way.  And I need a word
for that.

"Détournement" I like.  It carries the meaning of distant action on something to
change its course.  So we have the two important pieces: changing the program,
but at a distance.  And it's not an overloaded term in the field.

My thesis is then: we can divert programs to add functionality.  We modify their
behavior but without touching their code.  To divert programs, we act on the
context of execution through late binding and reflection.  Diverting programs is
a lightweight alternative to refactoring: we can add functionality without
the heavy costs of rewriting.

*** Refactoring is related work
Factorization is related to the problem, but not what we are doing.  If
implementing a new functionality requires adding several lines of code inside a
module it should not affect, then there's a problem of module division, and an
opportunity for refactoring.  Refactoring is re-organizing the program structure
so that implementing the new functionality could be done in a separate module.
Refactoring would offer an interface for this new functionality and other
similar additions.  With refactoring, we can integrate a visitor pattern, add
hooks, etc.

Refactoring is just the opportunity to redesign the program structure in the
face of new requirements.  The mechanisms we would use when refactoring are the
same we would use in the first design phase.

Refactoring has a cost in critical resources: time and effort.  But the primary
argument against refactoring is the risk of altering the program behavior
unknowingly.  Refactoring /should/ preserve the behavior, but there are no known
ways of ensuring that the programs before and after the refactoring are
equivalent under all observable executions.

So, rather than changing all the code, we aim to find mechanisms that allow
adding more code without touching the existing one.  We aim to define additional
functionality in layers that can be disabled, so we can always dynamically fall
back to a previous behavior that was known to work.  That way, the costs of
of adding the new functionality are proportional to the complexity of the
functionality itself, not of the whole program.

** [2016-06-27 lun.]
*** Sketching the defense
Case study: Narcissus instrumentation.

The goal was to add faceted evaluation to Narcissus.  They went with modifying
the interpreter all other the place.  You end up with this diagram.

Modifying a program like that does not scale.  It's quick, and it works for this
case.  But now imagine you want to try another evaluation strategy.  You fork
the original interpreter and modify it, again.  Now you have 3 variants of the
interpreter.  And you have a bug that affects all three.  You must now patch the
bug in all three places.  Doesn't scale.

And what happens if you want to merge evaluation strategies?  Combine 1 and 2?
You have to create yet another fourth variant.

There's yet another issue: how do you know what part of the code pertains to the
new evaluation strategy and what part pertains to the original semantics?
Version control tools can help, but they don't work on the /semantic/ of
changes, just on lines of code.  A clear distinction in the code helps.

Let me get one thing clear: that's totally fine, especially considering this is
a research prototype.  Their goal was to apply their evaluation strategy to
JavaScript, and they succeeded.

But, it got us thinking: how would we do it differently?  Given enough time, how
would we add faceted evaluation on Narcissus?

Any software engineer would tell you: refactoring.  We have a typical case of a
program that is being used in ways it what not designed for.  The module pattern
of JavaScript prevents extension.  This calls for taking into account the new
use cases and refactor the program.

But refactoring is difficult to get right and time consuming.  We would have to
think of a new API for extension that would cover all our new use cases.  That
means gathering these use cases, sketching, etc.  We have to review solutions:
e.g., hooks, a visitor pattern.  Then we have to do the actual rewriting.  That
is tricky, because we have to make sure we don't alter the functionality of the
interpreter in doing so.  Maybe this is trivial because we only add code that
visibly does not change the control flow or breaks any (implicit) assumptions.
Or maybe this is hard, because we restructure the whole thing to be completely
extendable.  We can use tests, or rely on types (we have neither here).  We have
people here in ASCOLA trying to prove refactorings that preserve semantics.
It's not quite there yet.

And then, maybe down the road you find out that you missed an important use
case.  And you need to refactor, and do this whole process again.

There is a lightweight alternative to refactoring.  This is what I call
/diverting/ the program.

This is all based on one phenomenon we are all familiar with: context.  Let's
forget programming for a moment.  Take the sentence: "I'm defending".  Today,
you understand perfectly that it means that I am actually defending my PhD
before you.  But, said by a soldier in a war it could actually mean something
else.  There is no ambiguity as long as the context is made clear.

We could even go so far as saying that the sentence is a bundle of meaningless
symbols without the context of an English speaker during a PhD defense.

Back to programming.  Programs have context too.  Like a sentence, the meaning
of programs is determined by their context.  *Changing the context changes the
meaning of the program*.

What does that entails for our case?  We want to change how the interpreter
behaves, and we can do so by changing its context.

Now, what is the context for a program?  Based on our answer to this
question, it yields different ways to carry out a diversion.

One answer is to say that the context is the interpreter of the program.  The
interpreter is what gives meaning to the program, and the program is written
with a given interpreter (semantics) in mind.

So, that means that a first approach is to modify the program is to write a
different interpreter for it.

There are many ways to do that, but I'm just going to show an interesting one
from functional programming: the Reader monad.

This is pattern where the program you write has no meaning until it is
interpreted.  It is a reification of the program, and the interpretation can
easily be changed.

[missing piece of the puzzle here]

We can also see that as creating a DSL with multiple custom interpreters.

Ok, that's interesting.  But it still doesn't scale.  We cannot write an
interpreter for each variant of the language.  We would have the same issues.
We have just shifted the problem.

There's another interpretation of 'context' in a program: whatever the names
of the program refer to, the set of bindings, the /environment/.  This
interpretation leads to looking at dynamic scoping.

Lisp example.

See: dynamic scoping let us manipulate the bindings of the names used by the
function.  In one context, 'plus' is addition, in the next context, 'plus' is
subtraction.

Now, JavaScript does not have dynamically-scoped variables.  But it has an
interesting construct: ~with~.

~with~ example (canvas context object).

~with~ was intended to make writing this kind of code shorter.  By using ~with~,
you don't have to repeat the ~context~ object anymore.

But what ~with~ /does/ is really to push a binding environment on the stack, an
environment that we can control.  We can then give different results to the
same body, just by changing the object passed to ~with~.

~with~ example (from FOAL paper)

Basically, ~with~ takes a context, and the code inside it can change its meaning
based on the context.  If we control the context, we can divert the code.

We can apply this insight to Narcissus.  Just wrap the code inside a ~with~, and
expose the context.

Now, we can add the faceted evaluation strategy without modifying the code of
the interpreter.  Modifications are outside.  We can immediately see what the
changes are. And we can toggle the evaluation strategy dynamically.

We write multiple evaluation strategies and combine them.  That's very
lightweight.  No need for refactoring the interpreter.

There are practical downsides to this approach: performance overhead of 20% in
execution time, and ~with~ is deprecated in the "strict" mode of the language
(that was opt-in in ECMAScript 5.1, but now enforced by default by most new
constructs of ECMAScript 2015).  Also, the exact behavior of using ~with~ that
way is left undefined by the standard.  YMMV (Your Mozilla might vary).

But that's not critical.  ~with~ is just /a way/ to replicate a form a dynamic
scoping in JavaScript.  The point is that dynamic scoping is a lightweight way
to change the meaning of the program.

Recap: so what have we done?  Diverting programs by manipulating their context,
not their source text.

There should be a table recapitulating "Ours" vs. "Theirs" cons and pros of
extending a program.  I think these are always dishonest, because often you are
measuring them to the standard that fits you.  They did not have your goals in
mind, and you consider that their approaches cannot be modified to take your
goals into account.  I can always add silly goals like "Written by me".  So
there won't be such table.

What you want to know is: is diverting programs better?  That would be a great
to know that you can ditch refactoring and always use dynamic scoping for
extending programs.  The answer is, as always, it depends.

In these matters what is important is to /think/.  Thinking allowed us to see a
different way to extend programs.  We questioned the viability of refactoring
and of AOP for diverting.  There was a simpler solution, and here it is.

Diverting may not always be the right solution.  It is /a/ solution that could
be taken into account.  It has the benefit of being lightweight in languages
that support dynamic scoping mechanisms, but it may not be suited to statically
typed programs, and you could argue that it obscures reasoning about what the
program does.  There are situations where diverting may be useful, and I've
illustrated one of them.

But what is more important question is: what insight can be gained by this?  The
takeaway is that modifying a program can be achieved by modifying the way it is
interpreted as well.  Programs, like sentences in a natural language, have
meaning only with respect to a context.  Manipulate the context, manipulate the
program.  Always keep both sides in mind.

**** How is this related to X?
I understand that, when a claim to a new approach is made, the claimant must
provide proof that it is indeed novel.

Now here's the rub: I don't claim it is novel.  The Reader monad is not new.
Dynamic scoping is not new.  I only claim I have not seen these mechanisms
applied in this way.  It may just reflect my poor knowledge of the domain.  I
have tried to look for competing approaches.  The fact is, there are simply too
many papers and too many programming languages.  Proof of novelty must be by
exhaustion.

So, it was novel to me.  Maybe it is novel to you too.  Isn't that enough?

**** You talked about interpreted languages, but what about compiled ones?
Ah!  Well, a compiler, is just an interpreter, but instead of executing a
process, it generates a program in another language.  So a compiler actually
provides us with yet another way to divert the program: we can modify the
generated code itself.

*** Diverting the program by changing its interpreter
We can certainly divert a program by changing its interpreter.  But the
difficulty is that the interpreter might be at too low a level to be able to
capture the semantics of the program.

Let's take an example.  If your language is one of arithmetic operations:

: <term> ::= <term> + <term> | <int>
: <int>  ::= 0 | 1 | 2 | ...

Then you can easily change the meaning of ~+~ to something else, like
subtraction.

Now your language is a lambda-calculus.  You want to write the same program, so
you encode numbers and addition following Church.  But now, if you want to
change addition to subtraction by changing the rules of the lambda-calculus, you
are in a drag.  The calculus "sees" only lambdas and their application.  It does
not distinguish between lambda terms that represent addition and lambda terms
that represent numerals.

To target precisely the addition term, we would need to recognize it.  That
means pattern matching the AST for custom case handling.

: lambda term of addition -> evaluate this term (subtraction) instead
: _ -> standard lambda calculus

That's quite brittle, since there are infinitely many ways to encode addition.
You cannot capture them all in this fashion.

What you need is a more direct way to tell the compiler you want addition here.
What about the keyword 'addition' ?

: addition -> do subtraction instead
: _ -> standard lambda calculus

So now you "teach" the interpreter about addition directly, and it handles it
for you.  You "push" the functionality down, and it can be changed once it's out
of your hands.

That step of interpretation can even be done by a simple rewriting step,
separate from the interpreter itself.

: program (lambda + add) -> rewriting add to sub -> standard lambda calculus

**** By preprocessing
That's not very different from how macros work in C.  Macros are expanded before
the compilation actually takes place.  You can actually use the macro
preprocessor of the C compiler on any language.  There is nothing that tie it to
C (save the cumbersome syntax).

So you could use a ~#define add~ in your lambda program, use the C preprocessor,
to expand the uses of ~add~, and use your standard lambda calculus interpreter.
No difference to the first situation.  Except now, the preprocessor step /can/
easily change the definition of ~add~ to something else.  It gives this step a
hook it can catch on.

This is of course not limited to the C preprocessor.  We can use any system of
macros, or template generator.  Ruby, Rastache...  In fact, the C syntax for
macro calls is not applicable to any language (see [[https://stackoverflow.com/a/18473606][this SO answer]]).  A proper
templating engine is best.

: function plus() { ... }
: plus(1, 2)

becomes

: #define plusdef function plus() { ... }
: plusdef
: plus(1, 2)

One issue with this technique is that you still have to /tell/ the template
system of the parts of you program that can be changed that way.  You have to
/declare/ them explicitly as macros.  You don't want to do that pre-emptively.

But it is lightweight.  When you need the flexibility to change the definition
at a later time, outside the source code, just declare the function as a macro
with a default value of its current definition.  Then you can use the template
engine to override the default value of this macro, and replace it with
something else.

You can also use templates for adding hooks into the source code.

: function plus() { ..{{{HOOK}}} }

Then, add a preprocessing step that expands ~HOOK~ to whatever you want it to
be.  You added code and diverted the program, but without changing its semantics
(leaving ~HOOK~ empty does not modify the program).

**** Pushing down
When I say "push down", I really see the code as a flat surface in space.  The
surface is ready to be fed to an interpreter, like a fax machine.  But now we
push some parts of this surface down, so they lie parallel to the surface, but
deeper in the third dimension.

#+BEGIN_EXAMPLE
----- ------------ ------
     -            -
#+END_EXAMPLE

This object cannot be faxed any more.  It must be preprocessed and flattened.
The preprocessor flattens every bump in the surface, expands every macro use.

With this metaphor, we see that we need to differentiate the macro use from the
regular code so that the preprocessor catches them.

We can also see pushing down as late binding.  The meaning of the pushed down
values is "up in the air", still not settled.  The rest of the program will not
change after preprocessing: it is fixed.

**** By reifying the program
With preprocessing, we pushed down the definitions we wanted to be able to
change at a later point.

There is another point of view, which is to "push up" the program we are
writing.  This is the monadic interpreter approach.

** [2016-06-28 mar.]
*** Chasing the monadic interpreter
One [[https://gist.githubusercontent.com/avieth/334201aa341d9a00c7fc/raw/56e137ca76f04062bb7b89a9fe87280fd6b61ee0/gistfile1.lhs][boilerplate-heavy approach]] is the Free monad.  The gist goes further than
Swierstra by showing how to combine interpreters as monad transformers.  Most of
the code actually goes over my head.

This [[https://lexi-lambda.github.io/blog/2016/06/12/four-months-with-haskell/][post]] gives a less hairy example of using newtypes to simulate an effect
system.  The upshot is that side effects made the function are visible in its
signature.  And different effect runners can be used (they give the example of a
mock runner for unit testing for instance).

Ah!  But the Free monad is actually just a way to get an AST out of a
do-notation program for free.  It's used for imperative DSL inside Haskell.  But
it's just syntactic sugar that hides the fact that one can build the AST
manually.

This [[https://programmers.stackexchange.com/questions/242795/what-is-the-free-monad-interpreter-pattern][SO answer]] gives the example of a get/set keystore DSL:

#+BEGIN_SRC haskell
p4 :: Free DSL a
p4 = do foo <- get "foo"
        set "bar" foo
        end
#+END_SRC

The Free monad here just threads everything (but we have to define ~get~, ~set~
and ~end~ accordingly) so that the program ~p4~ is just an expression of value:

: Free (Get "foo" $ \foo -> Free (Set "bar" foo (Free end)))

So we have a sequence of instruction in AST form.

The point is, it just builds an AST.  Going into this direction to the extreme,
to "push up" the interpreter we can just quote our program and provide an
interpreter for it.  Given the Lisp interpreter:

#+BEGIN_SRC lisp
(defun eval (...)
   (cond ...
     ))
#+END_SRC

Instead of taking action, we can just return its code:

#+BEGIN_SRC lisp

(defun eval (...)
   '(cond ...
      ))
#+END_SRC

AST for free!  The code is a description of its action, a reification of the
process taken.

Then we can rewrite that code and evaluate it: we've got an interpreter
variant.  Going to the metalevel allows us to treat the code as first class
data.  We don't manipulate the running process of the interpreter, but the code
itself, the code that the programmer works with.

This is more easily done in Lisps, because the structure of the code is very
simple.  Template Haskell would work if we wanted type safety, but we would have
to deal with a more complex AST.

The main takeaway is that, to be able to divert the program, that parts that
must change must first be made /first-class values/.  Quoting makes the code
first-class, then we can change the code.  Using macros makes them first-class
values for the preprocessor.  And dynamic scoping makes the environments
first-class as well.  Kind of obvious in retrospect.  The design space is
defined by what you chose to reify, and what mechanisms you use to reify it.

*** A pure interpreter is easy to divert
Just substitute the ~eval~ function.

When we interpret a program, we are interested in its side effects (putting
things on the screen, writing on the disk, reading from the network...) and
eventual output value.

Abstractly, we can see any interpreter as a mathematical function that takes an
AST and produces a computation:

: interp :: AST -> m a

This suggests that, to /change/ the outcome of the interpreter, we can just
substitute a new ~interp~ function:

: interp' :: AST -> m' a'

For the same program (AST), we can now have two outcomes.

This suggests that whenever our interpreter is actually written as a function,
we can just substitute that function for another.  Of course, the interpreter
function is realized by code.  The mathematics are not as helpful to understand
the issue there.  The crux of the issue is to avoid duplicating the code for the
function, but change only the relevant parts.  Avoid duplication is not easily

** [2016-06-29 mer.]
*** Breaking news: computers steal programming jobs
In the perspectives: programming automation will probably not care about any of
this.  If in the future, computers are able to generate programs based on an
informal specification, or by having a discussion with humans, then modularity
consideration will change.  Modularity is rarely a concern for generated code.
But as long as there are humans who read programs, they will appreciate
clarification.

*** The motivation is writing programs for humans first
In the synthesis: this is tied to writing.  Writing clearly eases communication
between humans.  A program is written by a human to the machine, but also to
other humans (himself included).

*** Strengthening the narrative
Diverting programs is about changing their behavior without directly changing
their code.  But there are a couple of hidden assumptions.  The goal is not to
divert programs because we can, the goal is to divert programs in order to build
programs incrementally.  We don't go back to the code we want to modify: we add
modification instead.  Add-only way of programming.

The benefit of add-only is that you have multiple variants of the program on
hand.  You can toggle variants dynamically.  And you can easily add new ones
without impacting the others.

The goal is, ultimately, to make programs clearer, to convey their intent as
directly as possible.  The instrumentation of Narcissus is best defined as a
/delta/ from the base interpreter.  It follows that the code should also be
expressed as a delta.

*** Building incrementally with version control
Now, this 'delta' idea evokes version control and /patches/.  Indeed, patches
are a unit of modification of the source code.  We can even see the whole
version control history as the program itself, and we can use branches to manage
variants of the program.  Since we can revert earlier changes, and branch to
test changes, break up modifications to keep the good stuff... we can see
that version control allows us to build a program incrementally.

But patches alter the static structure of the code, and are devoid of any
semantic meaning.  What we do to Narcissus in the last chapter can be seen as
dynamic patching, because the activation actually happens at runtime.

*** AOP is diverting, so is open implementation
AOP is diverting.  AOP is basically COMEFROM, and COMEFROM is the epitome of
diversion.  But AOP is a known means of diverting programs, so not as
interesting to pursue.

Open implementation creates a backdoor for changing stuff inside the program.
Same idea.

*** Putting the horses in front of the carriage
Either I present the idea of diverting programs before the context section, or
after it.

Before the context, then we know what to look for: ways to divert programs.  The
context section is more focused.  It is more of a 'related areas'.  We do not
compare solutions closely (because we haven't shown one yet), but we take the
time to appreciate how they all relate to this idea of diverting.  Importantly,
it casts a new light on these well-known works, the light of a new idea:
diverting.  This new light reinforces the purpose of the section.

If we introduce the idea of diversion after the context, well... We might not
buy into it as well.  Reading the context, you would see many threads, and not
necessarily the idea of diversion.  When the conclusion is "these are all about
diversion", it's not a revelation, it's just an interpretation.  A restrictive
interpretation.  The reader feels robbed: what of all the threads I didn't pick
up?

Now, there's a bit of a chicken and egg problem here, because the idea of
diversion actually came from thinking about all of these works.  The idea comes
from them.  By introducing the idea after these works, I'm hoping the reader
comes to the same conclusions as I did.  I present the evidence, the facts, and
they make their own mind.  The other way around feels manipulative.

On the other hand, it /is/ a thesis.  It is an argument.  I have a bias, a
perspective that I want to share.  And I am presenting evidence supporting my
claims.  Disagreeing parties can always refute my interpretation of the
evidence.

So it is agreed: I must introduce the idea of diverting before the context.

But there remains an issue of ensuring the reader has enough background before
introducing the idea.  In other words, can I use the introduction to introduce
and motivate the idea, or do I need the whole problem section?

1) Intro -> Problem (diverting idea) -> Context

or

2) Intro (diverting idea) -> Context -> Problem

I'm leaning towards 1), because the sooner the problem is exposed, the sooner
the document makes sense to the reader (especially to the informed reader).  The
only downside is that I will probably have to introduce a few concepts to ease
the presentation of the problem.

** [2016-07-01 ven.]
*** New abstract about diverting
Programs are hard to write, and harder to maintain.  Programs often are part of
an /ongoing/ development process: they evolve as requirements change, as new
features are added, and as defects are uncovered and fixed.   The main way to
deal with these constant changes is by /refactoring/.

But refactoring can be difficult and time-consuming.  Refactoring entails to
rewrite parts of the source code, but may also require to reorganize the
program's whole architecture.

We propose a lightweight alternative to refactoring: /diverting/.  Diverting a
program is to change its results without directly modifying its source code.
To divert a program, we leverage late-binding mechanisms such as dynamic scoping
and metaprogramming.

To illustrate diverting, we take a simple, non-trivial example of program: an
interpreter.  We start with an interpreter for arithmetic expressions, and
review the techniques used to divert the interpreter to support new terms and
operations; a setting reminiscent of the Expression Problem.  We show how
diverting can help build interpreters incrementally in layers, allowing dynamic
activation of alternative behavior.

Finally, we apply those techniques to divert a full-blown JavaScript interpreter
with multiple dynamic analyses.  There, we find that diverting is a legitimate
alternative to refactoring for this realistic use-case.  Diverting is both
faster and more flexible than rewriting the program, but bears potential costs
in efficiency and program safety.

** [2016-07-06 mer.]
*** Programming and the scientific method
Naur's idea was that the primary product of programming was to build a theory in
the programmer's mind.

When you look at the actual process of programming, there are similarities
with the scientific method.  You start with a specification (a hypothesis), then
you write a program (conduct an experiment), then observe the results, and if
the results are not as expected, you make the requisite modifications to the
program (conduct a new experiment) or specification (change hypothesis).

When conceiving a program, you have a theory of how the underlying machinery
works.  If you have an idea ("i'll make these thingamagins work /that/ way"),
you use your internal theory to write the code.  When the results are not those
expected (i.e., it goes horribly wrong), you have to revisit your understanding.

The scientific method will lead you to refutable theories about how the world
works.  Programs will lead you to refutable theories about how the machine, the
language, data structures, algorithms and libraries work.

** [2016-07-07 jeu.]
*** Viewing a change as a difference or as a redefinition
A change like the faceted evaluation of Narcissus can be seen either:

- as a new definition for JavaScript evaluation.  That is the point of view
  taken by Austin and Flanagan: they present a new specs, derived from the
  first, but totally autonomous.  Same with the instrumented interpreter: it's
  derived from Narcissus, but act as a standalone.

- as a base + differential couple.  This is our point of view.  Narcissus is the
  base, and stay pristine.  Faceted evaluation can be described as a
  differential on top of the base.  The goal is then to take this point of view
  into code.

** [2016-07-20 mer.]
*** Programming is crafting a future experience
Why is tangling even an issue in source code?  One of the reasons might be that
we are /able/ to modify the source code after we have written it, but before it
is executed again.

When we write it at first, maybe it runs as intended.  But we have the option to
move parts of the code around, to reorganize it into neater pieces.  Source code
is malleable.

Now, this is the same with novels, plays, and sheets of music.  These are
written in advance, and going through several incarnations before being
experienced by the audience.  We seldom read a novel, attend a play, or listen
to musical piece that has not been through several drafts before.  And as I
write these words, I too, erase, go back and change sentences, move commas and
break lines.  Why is that?

It seems so natural this process, that to even ask why it is so is like asking
why trees are green.  But, as there is an explanation for the greenness of
trees, chlorophyll and light reflection, so there must be one for this question
as well.

Consider that, even though we draft speeches, or iterate on essays, we do not
necessarily prepare every casual lunch debate—even if we are trying to convince
our peers in all three situations.  In a casual discussion, we might make many
errors of speech: missing words, swapping word order, forgetting an 's' at the
third person, using approximate words, but most importantly, we might not say
what we mean.  In these situations, we are often /discovering/ our thoughts as
we express them.  If you would look at a transcript of such discussion, it would
be full of errors, approximations, and non sequiturs.  But that's actually okay
in this setting, because we can correct ourselves as the discussion happens.
"Sorry" erases the previous word, "What I meant to say was ..." overrides the
previous argument, and so on.

But, given the opportunity to make the same argument in writing, we would get
rid of the hesitations, the speech errors, the roundabout formulations; we would
consider that an /improvement/ because we would get right to the point.  Not
wasting time.

When you think about the experience itself, of reading a novel, of attending a
play, or listening to music, these experiences are all unrolling themselves in
time.  They are continuous from start to end.  When crafting these experiences,
this time is part of the budget.  The experience is what the audience feels, but
the artist describes the experience as a condensed form, taking time into
account.  The sheet of music is not the music, but it describes it sufficiently
enough for a musician to perform it.  When writing the sheet, the artist crafts
the experience like clockwork, adjusting rhythm and harmonies in advance.

A program is the condensed description of the processus.  Like fireworks, the
inert artifact do not resemble the experience when it is activated.

But this disconnect in time may actually be the answer to the question.  We
/can/ fix and enhance the experience before it happens, since we control its
description.  In a discussion, we cannot go back and fix the mistakes when it
happened, so we must invite our interlocutors to forget them after the fact.  In
a program, we can actually go back in time and fix the mistake, albeit for
/future/ executions.

** [2016-07-22 ven.]
*** Merging chapters 3 and 4
After chapter 2, we have two different inquiries:

1. How is the problem of extending Narcissus related to other known /problems/?
2. How is diverting related to known /mechanisms/?

See, Smalltalk and Self introduce mechanisms that can be used to divert
programs, but they certainly don't intend to use them that way.  However, there
is a shared mindset of separating concerns, for code clarity.

Parnas does not introduce any mechanism, but helps setting the mindset.  Ideally
the mindset should help set the context before we jump to the problem.  I want
to ease the reader to think of modularity, clarity and extensibility.  "See,
these great men cared for it, there must be something there".  Then: here we
have a similar situation.

But, knowing the problem first helps focus the reader as well.  You are not just
reading a nice, if biased, history of programming paradigms and software
engineering research.  We are looking throughout history for relevant problems
and solutions.

So putting the related work after the problem is okay.  Might need a section for
motivating the need of multiple dynamic analyses.

Then, in the related work, just state that we are looking for a) related
problems (for their solutions) and b) other mechanisms.

Parnas and Naur might go before the problem, before the preliminaries even.  As
a proper Context section.

** [2016-07-25 lun.]
*** The need for diverting is real
The scenario of extending Narcissus without refactoring it is not convoluted.

My good friends at the end of the corridor are trying to extend OpenStack,
without touching the code base.  They want to provide an extension without
polluting the tree, to test the waters.  That's legitimate.

When I see the V8 team dropping "strong mode" for code complexity reasons, I
can't help but think that a diverting approach would have lessened the burden of
maintaining it.

Diverting is not be an alternative to refactoring where refactoring is
warranted, it is an alternative to refactoring when you want to prototype
without touching the tree.

Using a VCS could also work: use a branch per extension, and create multiple
folders to view into each branch.  Now you have access to all variations, but
they are synced by the VCS system.  Main contributors can work on the main
branch, without caring about experimental branches.  Experimenters on the other
hand can keep hacking on their branch, merging the main patches as they see
fit.  The process is analogous to diverting, but diverting happens at runtime,
whereas this happens statically.  Give git example (in appendix?)

** [2016-07-26 mar.]
*** FOAL chapter and object algebras
Watching a [[https://www.youtube.com/watch?v=snbsYyBS4Bs][video]] of Curry On last night, as one does, object algebras finally
clicked for me.  Especially as it was /eerily/ similar to the FOAL chapter I
just finished writing.  Sigh.

There's an upside though.  It made me realize that I could reuse the AST
expressions if I put them in functions, therefore knocking off one down side of
the approach.

** [2016-07-29 ven.]
*** Content ideas for chapter 5 (variations)
Take the most interesting examples of my variations (from Lamfa), and compare
them.  Try to see how they differ, what mechanisms they use, etc.  It can easily
reach 10-15 pages with code examples, which would be alright for this chapter.

Should have two examples of metaprogramming (Template Haskell at least), because
it's advertised on the cover!

So at least:

- late binding
- dynamic scoping
- template haskell
- aspects

** [2016-08-04 jeu.]
*** One leftover thread is DSLs
That's the logical next stop to this train of thought.  Ever more flexibility,
up to metaprogramming.  Then, next stop: forgo the language, build your own.
That brings us to DSLs, free monads, object algebras and the like.

** [2016-08-08 lun.]
*** Idea for cover
Had this vision of a japanese-style ink painting of a mountainous landscape with
a man ascending in the foreground.

The style of painting I had in mind is called sumi-e.

I found [[https://en.wikipedia.org/wiki/File:SesshuShuutouTou.jpg][this one]] which fits nicely, and is even nearly A4!

I also like [[https://commons.wikimedia.org/wiki/File:Sesshu_-_Haboku-Sansui.jpg][that one]] for the larger and quicker strokes.

** [2016-08-11 jeu.]
*** Emacs indirect buffer
When I want to edit a subtree in the manuscript, I may want to narrow it.  But
if I want to look at other parts in another window for reference, I cannot
narrow it.

Unless I narrow it to an indirect buffer!  There's a command to do just that in
org, org-tree-to-indirect-buffer.

Making a note here.

** [2016-08-16 mar.]
*** Multiple indirect buffers from subtrees
To add on the previous note, if you want /multiple/ such views on the same org
document, pass the prefix argument or customize org-indirect-buffer-display, as
noted in the documentation.

** [2016-08-17 mer.]
*** Planning for the conclusion
First, a recap of what we talked about.

Then specific highlights on what was important: late binding.

Then a more critical look back at the work: why do you want separation of
concerns?  Locality, using git.  Especially: if this really serious?  It does
not seem to ease maintenance.  Familiarity is important, and tricks like ~with~
are surprises waiting to blow up.  I'd say it has its uses when you know what
you are doing, but is not a fits-all solution.

Can we apply it elsewhere?  I don't really believe in one-size-fits-all
solutions.  Every situation is slightly different.  The state of art chapter
showed a bunch of different ways to tackle extensibility.  It's not a bad thing.
There are compromises to be made all the time.  There is not superior solution,
just trade offs to be made.  That's what make it fun.

Then perspectives: what's next if we wanted to go further in this direction?
For the ~with~ trick, eliminating performance disparity is one thing.  Then,
combining multiple analyses need to be looked into.  At the very least, we could
statically warn on problematic overrides.  More generally, we could offer
conflict resolving strategies, like conflicts of advices in AOP: two analyses
override the same method, how do you proceed?  This could be done at runtime for
a dynamic language, or statically if the language provides enough clues.

Ultimately though, the larger goal is to find better ways to write, organize and
maintain programs.  This is still an open problem.  It's also not really a
problem, as what we have is clearly good enough for the millions of programmers
and programs out there, that run on billions of computers every day.  Still, I
guess one could have said the exact same thing 10, 20, or even 50 years ago.  So
if you feel like there have been any improvement in the matter in the past
decades, then I guess it's worth keeping going further.

I believe the problem will stay open as long as there are humans writing
programs and weirdly caring about how it reads.  If humans are taken out of the
equation, and, for instance, computers become good enough at generating programs
from loose directions, then maybe how the programs are actually written will
matter less.

** [2016-08-18 jeu.]
*** Choose your own questions
I should just have written the thesis as questions and answers.  For this kind
of document, it just makes more sense.  I feel that I can answer any questions
about my work really more easily than I can field together a coherent narrative
about it.  There are always dangling threads that do not fit quite well within a
section.  And threading the whole thing is actually lousy for following
inquiries.

As reader of theses and research papers, I do not want to follow a narrative as
much as find answers to specific questions.  At the outset, I always have two
questions: what was your problem?  how did you solve it?

Then, depending on the answers, I can branch out to specific topics I am
interested in.

Papers already follow an informal format, designed to provide answers to the
usual questions a reviewer may ask.  Want to know how we compare to X? ->
related work.  Want to know the specific contributions -> bullet points in the
introduction.

So, why not use these questions for guiding the reader instead?

I could have several entry-points that are explicitly labeled:

- What's your thesis?
- What is the problem you want to solve?
- How does it relate to existing problems?
- What are the solutions your propose?
- What insight did you gain?

These are the parts I already have (as expected).  But then, instead of
formulating an answer in 30 pages, it's just one or two paragraphs at most.
Reading these paragraphs just opens follow-up questions.

Say:

Q: What's your thesis?
A: That we can use /diversion/ to instrument and extend interpreters.
-> Q: What's diversion?
-> Q: What do you mean by instrumenting and extending?
-> Q: What's an interpreter?

Readers follow the threads they are interested in.  Readers with different
backgrounds can trace their own path.  A naive reader would maybe start with the
third question, and then since I know what question they asked, I can do a good
job of providing an answer while not worrying they might know too much on the
topic.  Expect readers can maybe gloss over the answer to the second question,
come back, and follow the first.

One drawback is that you might lose yourself following threads.  Like following
too many links on Wikipedia, you may not know how you ended up where you are.
Bread crumbs could help.  Visual feedback on questions that have been read is
mandatory.  As well as an exhaustion indicator: all follow-up questions have
been read.  Or maybe just an overview of the question graph, to see if you have
missed an interesting branch.

You might ask: but what if none of the questions are even close to what I want
to ask?

Well, the prose format cannot anticipate these questions either, so we are not
/losing/ anything.  But I take it as healthy sign.  Maybe the reader of a
well-woven narrative would not /think/ to ask such questions because narratives
have a tendency to take us on a nice soothing ride.  If they are well done, we
might get the impression that nothing is missing.  If, on the other hand,
readers are required to be actively /seeking/ answers from the start, they will
ask many interesting questions.  Maybe the flaws and shortcomings would be more
obvious.  But wouldn't that be positive side-effect?

You might say: well, isn't that a little patronizing, to require readers to
follow the questions you thought of, rather than providing them whole thing and
letting them diving in?

It might be, for some.  You have to be honest when writing questions.  You could
be sneaky and rig the questions so that flaws are not revealed, and difficult
topics are never raised.  One solution would be to include the readers in the
writing process.  Write a first draft with the questions you think are fair.
The readers will propose more adversarial questions, or want you to elaborate on
a particular point.  Then you can add these questions in, and the answers as
well.  It's not a perfect solution, because you could still ignore these
questions.  But if you are intellectually honest, that would make for an
engaging read.

Also, I realize that the primary role of the PhD defense is to give a chance to
reviewers to ask questions directly to the author.  To have a real discussion.
Sadly, I've rarely witnessed real discussions in these settings because of the
blatant asymmetry: the candidate is being /evaluated/.  The candidate is never
at ease.  And neither is the jury: sometimes they are fishing for questions, in
an unfamiliar environment, in a room full of strangers (and family members for
the candidate).  They are afraid of asking a stupid question (they shouldn't!),
and they basically /always/ nod even if they haven't understood completely.
Like students, they are afraid of saying "Well, I'm not sure I got that right",
especially if the candidate is confident.  The defense is unfortunately an
adversarial situation, rather than a heartfelt meeting of minds.

But it does not matter if the document has a Q&A format.  There will always be
questions left for the defense.

Bummed that I did not realize this sooner.  Because that's actually how,
sometimes consciously, how I wrote the thesis!  By answering implicit and hidden
questions.  Maybe for the defense?

** [2016-08-22 lun.]
*** Open recursion in OCaml
PIM mentioned "open recursion", a pattern for writing several walkers of an AST
without repeating the boilerplate.

[[https://realworldocaml.org/v1/en/html/classes.html#open-recursion][This book]] has an example.  Specifically, here is the AST walker, ~folder~:

#+BEGIN_SRC ocaml
open Core.Std

class ['a] folder = object(self)
  method doc acc = function
  | Heading _ -> acc
  | Paragraph text -> List.fold ~f:self#text_item ~init:acc text
  | Definition list -> List.fold ~f:self#list_item ~init:acc list

  method list_item: 'b. 'a -> 'b list_item -> 'a =
    fun acc {tag; text} ->
      List.fold ~f:self#text_item ~init:acc text

  method text_item acc = function
  | Raw _ -> acc
  | Bold text -> List.fold ~f:self#text_item ~init:acc text
  | Enumerate list -> List.fold ~f:self#list_item ~init:acc list
  | Quote doc -> self#doc acc doc
end
#+END_SRC

And then one specific walker would be:

#+BEGIN_SRC ocaml
class counter = object
  inherit [int] folder as super

  method list_item acc li = acc

  method text_item acc ti =
    let acc = super#text_item acc ti in
    match ti with
    | Bold _ -> acc + 1
    | _ -> acc
end

let count_doc = (new counter)#doc
#+END_SRC

It counts bold tags within text items, but not when the text items are in a
list.  Hence it redefines ~list_item~ to skip the list altogether, and
~text_item~ to actually count the bold tags.

It furiously looks like an object algebra, or visitor.  The interface is defined
by the object: these three methods.  Any alternative walker inherits from this
object and reuses the base functionality through calls to ~super~.  Crucially,
adding a variant to the ~doc~ datatype can be absorbed by the ~folder~ class.

It might even just be an abstract class.  Here is how I think about it in JS:

#+BEGIN_SRC js
var folder = {
  doc(acc, arg) {
    switch (arg.type) {
      case 'Heading':
        return acc

      case 'Paragraph': let {text} = arg
        return text.reduce(this.text_item, acc)

      case 'Definition': let {list} = arg
        return list.reduce(this.list_item, acc)

      default:
        throw `Unknown type: {arg.type}`
    }
  },

  list_item(acc, {tag, text}) {
    return text.reduce(this.text_item, acc)
  },

  text_item(acc, arg) {
    switch (arg.type) {
      case 'Raw':
        return acc

      case 'Bold': let {text} = arg
        return text.reduce(this.text_item, acc)

      case 'Enumerate': let {list} = arg
        return list.reduce(this.list_item, acc)

      case 'Quote': let {doc} = arg
        return this.doc(acc, doc)

      default:
        throw `Unknown type: {arg.type}`
    }
  }
}

var counter = {
  __proto__: folder,

  list_item(acc, li) { return acc },

  text_item(acc, ti) {
    ti //: Object {type:"Raw",a:"Raw text"}
    let acc2 = this.__proto__.text_item.call(this, acc, ti)
    if (acc2.type === 'Bold') {
      return acc2 + 1
    } else {
      return acc2
    }
  }
}

var test_doc = {
  type: 'Heading',
  a: "I'm a heading",
}
counter.doc(0, test_doc) //: 0

var test_doc2 = {
  type: 'Paragraph',
  text: [
    {type: 'Raw', a: "Raw text"},
    {type: 'Bold', text: "bold text"},
  ]
}
counter.doc(0, test_doc2) //: TypeError: this.__proto__.text_item is undefined
#+END_SRC

Now, it shows I do not quite understand what is going on behind the scenes of
this OCaml snippet.  Unfortunately it does not work because ~this~ is not bound
to ~text_item~ when calling ~reduce~.

Well, then we can easily just use ~bind~:

#+BEGIN_SRC js
var folder = {
  doc(acc, arg) {
    switch (arg.type) {
      case 'Heading':
        return acc

      case 'Paragraph': let {text} = arg
        return text.reduce(this.text_item.bind(this), acc)

      case 'Definition': let {list} = arg
        return list.reduce(this.list_item.bind(this), acc)

      default:
        throw `Unknown type: {arg.type}`
    }
  },

  list_item(acc, {tag, text}) {
    return text.reduce(this.text_item, acc)
  },

  text_item(acc, arg) {
    switch (arg.type) {
      case 'Raw':
        return acc

      case 'Bold': let {text} = arg
        text //: [Object {type:"Raw",a:"bold text"}]
        return text.reduce(this.text_item.bind(this), acc)

      case 'Enumerate': let {list} = arg
        return list.reduce(this.list_item.bind(this), acc)

      case 'Quote': let {doc} = arg
        return this.doc(acc, doc)

      default:
        throw `Unknown type: {arg.type}`
    }
  }
}

var counter = {
  __proto__: folder,

  list_item(acc, li) { return acc },

  text_item(acc, ti) {
    ti //: Object {type:"Raw",a:"bold text"}"}]}
    let acc2 = this.__proto__.text_item.call(this, acc, ti)
    if (ti.type === 'Bold') {
      return acc2 + 1
    } else {
      return acc2
    }
  }
}

var test_doc = {
  type: 'Heading',
  a: "I'm a heading",
}
counter.doc(0, test_doc) //: 0

var test_doc2 = {
  type: 'Paragraph',
  text: [
    {type: 'Raw', a: "Raw text"},
    {type: 'Bold', text: [{type: 'Raw', a: "bold text"}]},
  ]
}
counter.doc(0, test_doc2) //: 1
#+END_SRC

Then it works.  So yes, open recursion in this example can be understood as
defining a list of methods that can call themselves back.  That's the "mutual
recursion" part.

#+BEGIN_EXAMPLE
folder o =
 doc { ... o.list_item ... }
 list_item { ... o.text_item ... }
 text_item { ... o.doc ... }
#+END_EXAMPLE

And then you can plug a more specific ~o~ into it, so it will take advantage of
the recursion already in place and ride along.

*** Planning for chapter 4
The main question that chapter should answer is: "Okay, we took a nice tour in
the previous chapter, but how do all of that can apply to your problem?".

So for every paradigm or mechanism mentioned in chapter 3, I should answer in a
few words how this mechanism can apply to extending interpreters, and to
extending Narcissus.

This is best done with a running example of a toy interpreter.

And it's okay if I am not exhaustive and just abruptly say "Well, here it's
unclear how it can applied, any pointers appreciated".

** [2016-08-23 mar.]
*** Full-power GOTO in JavaScript
Is there any way to simulate an arbitrary GOTO using the restricted version of
labels that JavaScript has?  [[https://alexsexton.com/blog/2009/07/goto-dot-js/][GOTO.js]] does it by rewriting, but it does not work
across functions.

*** Q: can you easily divert other interpreters?
The methodology (I have in mind) in chapter 4 is to show off diverting
mechanisms for interpreters I have also built.  Since I build them, why not
build them to be extensible?  Well, I try to build them /without/ considering
extensions first.  The simplest way possible.  Then, I show off how we can
divert them.

While I'm honest, it can be a useful inquiry.

But still, doubt can remain on whether there is value to the approach in a more
general context.  You know, outside of /me/.

One way to squelch this doubt would be to just take random interpreters and try
to modify them at a distance, and see how that works.

** [2016-10-20 jeu.]
*** Reviewers comments
Suggested refs: [[cite:VBG+10][VBG+10]] [[cite:MMC-95][MMC-95]] [[cite:JF-96][JF-96]] [[cite:ALM+09][ALM+09]]

** [2016-10-21 ven.]
*** Reading [[cite:JF-96][JF-96]]
#+BEGIN_QUOTE
We reject as too complicated and inelegant the possibility of dynamically
modifying the procedure objects constituting I_exit.
#+END_QUOTE

Yes you can nicely extend interpreters with reflective towers, but you have to
first build the tower.  The goal was to work with what was at hand; if we turn
Narcissus into a reflective tower we can also refactor it in any way we see fit.

The reflective tower argument can apply to chapter 5, but then again, we show a
/different way/ to build extensible interpreters.  Just because there already
exist solutions does not mean we should stop looking for others.

In any case, reflective towers should appear in chapter 3.

*** Reading [[cite:ALM+09][ALM+09]]
Well, there's no code, so all I see is a vague discussion about the usability of
reflexive systems.  Narcissus is not reflexive, only meta-circular, so while
they cut the subject into nice little categories, I don't see how it applies.

*** Reading [[cite:MMC-95][MMC-95]]
Well, it says it deals with metaobjects composition, which I guess is kinda of
related.  But you have to have a metaobject protocol in the first place.  We
don't.  So I don't see how it applies.

*** Reading [[cite:VBG+10][VBG+10]]
This one is particularly relevant.  In section 4.2, they show how they can
modify their interpreter to implement an Object Flow Analysis, which was
previously implemented by hacking the Pharo VM.  Now, they built a reflexive and
extensible interpreter on top of Pharo, so they don't have to keep hacking the
VM.

Extending the interpreter is pretty simple, it's a visitor over the AST:

#+BEGIN_QUOTE
To construct a new variant of the Pinocchio interpreter it suffices to subclass
the Interpreter class and override a part of its interface.  The Interpreter
class defines a meta-circular interpreter implemented as an AST visitor that
manages its own environment but relies on recursion to automatically manage the
runtime stack.
#+END_QUOTE

So our intents are very much alike.  But, they still built a brand new
interpreter on top of Pharo to be able to extend it however they wanted.

I took the interpreter as-is, and extended it.

** [2016-10-24 lun.]
*** Met with J to discuss defense tactics
Have a slide for every point raised by reviewers.

Among which:

- why is dynamic scoping considered an error, and why is out of so many
  languages if you say it's useful?

Also, did I talk about Swierstra and free monads in the document?  Maybe have a
slide mention the experiments I conducted in that direction.

Aiming for a first rehearsal on [2016-10-28 ven.].

** [2016-10-28 ven.]
*** Rehearsal with J

- Add table of contents

- Missing related work part

  - say AspectJ, COP, reflexive interpreters; just to show we know, but won't
    use them

- Make a clear distinction with "my contribution" and "what exists / what could
  have been used"

- Get rid of "preliminaries"

- Better contrast on faceted evaluation slide

- Give a big picture with boxes (interpreter box we don't touch; analyses boxes
  we do)

- Introduce Narcissus, introduce dynamic analyses/information flow (direct
  flows) directly before jumping into the more complex indirect flow example

- is the indirect flow example right?  doesn't seem so

- Slippery slope from "without code modification" to "minimize changes".  We
  need modification in the Narcissus instrumentation, be clear about that

- Evaluation slide: make a table with "existing instrumentation" with number of
  lines before/after

- Tell us more about the overhead (esp. the overhead of Narcissus faceted
  evaluation vs. raw interp); say you have worked to run the tests, fixed bugs
  in Narcissus

- "After diff" slide: make slide illustrating the problems needing code
  modifications (purple group: sequence problem; yellow group: need to name
  things)

- Say that Nataliia might be interested to have this to prototype different flow
  analyses and measure (perspectives)

- One slide on ~with~, explicit the distinction with dynamic scoping

- Before conclusions, tell us about why ~with~ is evil and why it is useful (and
  needed in the second contrib, but not in the first)

- We could have passed the E object as a parameter to Narcissus, how would that
  be different?

- First contrib is more bottom-up, construct the interpreter in pieces; second
  is top-down: modify the thing from outside.  Say it

- Module is not used in the same sense in both contribs; say it

- Give the legend for reference links/prototype links in diagrams (maybe even
  give more contrast between the two)

- Slide "add operation retroactively": say that adding a method as a property
  dynamically is rather JavaScript-y

- Slide "add operation as module" if you want us to compare the two approaches,
  show the two schemas

- In code slides, put emphasis color on lines that are relevant

- On diff slides, add legend

- Perspectives: how can we apply that to V8?  To compilers?

** [2016-11-08 mar.]
*** Second rehearsal with J

- Terms/operations/modules -> modules.  Too confusing.

- 4 creation patterns of modules used in part 2:
  1. var m = {}
  2. var m = {m1, m2}
  3. (function() { return {f, g} }())
  4. var m = function(base) { return {f, g} }

  Mind the difference, esp. in slide 20 (add show)

- slide 17 -> n = 3 or 1 ?

- njs-base -> njs

- Why JS?  State that JS is important today: popularity charts/numbers in
  intro.

- "portée dynamique" evokes "danger", and does not appropriately describes what
  happens with `with`.  Just say that it allows you to push an environment, and
  it's equivalent to an IIFE, because that's all we use at this point.

- add logos and J and M and ASCOLA on first page

- slide 2: say that project had static and dynamic analyses in mind, we are in
  dynamic part

- slide 5: dynamic analyses are not on source code

- slide 5: say that indirect flows were thought to be static-only, but AF showed
  it could be used dynamically, hence the choice of dynamic analyses and this
  one in particular

- describe dynamic analyses before Narcissus

- slide 7: give better intuition of faceted evaluation; maybe simplify example
  with only one branch

- slide 11: pourquoi 4 analyses?  Plus!

- slide 12: typo: réfléxif

- slide 12: put AOP at the bottom, and just say overhead + just too complex for
  our needs

- slide 17: num is a /reference/ to an object

- slide 33: say that it works because we defined them adequately

- backup slides: about interferences of modules (in Narcissus you have to do
  extra work if you want them to compose)

** [2016-11-10 jeu.]
*** Third rehearsal with some peeps

- slide 3: remove "analyse the *execution* of the program"

- slide 3: the arrow a -> b indicates a *dependency* from b to a

- slide 3: why do we chose dynamic analyses?  Static analyses are not as well
  suited to JS since we get many scripts as they arrive; the language is more
  dynamic so less can be known statically; much easier to use dynamic analyses.

- Why multi-facets?  Need to show that we need to modify the interpreter to
  implement them.  It's a better motivation; otherwise why present the facets
  analysis?

- See paper for the details on faceted evaluation; we are interested in how it's
  implemented on Narcissus.  Remove example and show code in Narcissus instead

- slide 6: say that JS is compiled or interpreted, but interpreters are simpler,
  and we are trying to understand what's going on here so first interpreters

- Need to clearly state the scope: we are in JS, providing a solution in JS.

- slide 12: especially here: the goal is having that in JS

- slide 12: note, just say that "solution pragmatique" means using the parts
  given by the language, not adding anything

- Part 2: state contribution more clearly: what's do you bring on top of
  alternatives?  State at the onset, and after.

- Part 2: have diagrams to show the high-level working of module composition,
  rather than the low-level working of the mechanisms.

- What do you bring on top of AOP?  Maybe a backup slide that extends on top of
  that.  Two different things: use AspectScript on programs (can't instrument
  if); or on Narcissus (there it works, but we are simpler and faster)

- slide 11: state of the art in two parts to prepare the two contributions,
  construction, and modification in second.  Move Rao in second part

- slide 11: call out AspectScript specifically; AOP is fine, the implementation
  is not.  Thus, we offer an alternative which is in the spirit of AOP, but
  simpler.

- slide 13: label the three different forms of modules; and say which are common
  (number 2).  First one is just an object.  Second one is a module: can control
  visibility.  The third one is a functor: takes a module, returns a module.

- slide 18: add a wrapping "base" around num and plus.  Otherwise, use a diagram
  to illustrate the abstract working: show: (m1, m2) -> (show(m1), show(m2))

- remove slide 20

- slide 21: say that the object creates an environment

- slide 28: interpréteurs *modulaires*

- slide 28: say that it's an alternative solution to state of the art,
  specifically tailored to JS using only the most basic mechanisms

- remove slide 31 32

- slide 48: nommer une *valeur*

- part 3: use higher-level schemas for most of the part

- slide 49: say that Narcissus + multi-facettes is with empty program counter

- perspective: separate persp for the two parts

- perspectives: say that there might be a space left to explore between this
  minimalist solution and AspectScript

* Meta-concerns
Notes about the process of writing and organizing the manuscript.  Behind the
drapes stuff that will not be exported.

** Guidelines
Ideally, I want my thesis manuscript to read like a reference on JS security via
language mechanisms.  I want to point people to it and say "This is the only
document I wanted to read when doing my PhD".

As a side note, it would be nice for it to be self-contained: most references
which are not academic may not be easily found in a couple of years (especially
web pages).  I try to save them using the Mozilla Archive Format, but having
them on print (in an appendix) would make the thesis a definitive reference, if
only for historical purposes.  There is the issue of copyright for existing
content though ...

*** Style
Des haiku pour chaque chapitre, un tl;dr

"JavaScript est complexe / Les navigateurs aussi / Nous sommes tous cuits"

*** Examples
Lots of.

I love discussing and arguing style and patterns.  But I often do that based on
examples that I have on my mind.  I need extract them from my mind and give them
to the readers, so they can appreciate the discussion even more.  It won’t seem
so abstract now.

*** Exhaustivity vs. relevance
I’d like the manuscript to be a definitive guide.  But at the same time, this is
an unrealistic goal, both because of time constraints and rapid obsolescence of
the field.

Thus, I should focus on the relevance of the current works, how they relate to
each other, and what insight can we gain from abstracting the examples a little.

Maybe if I can’t do an in-depth analyses of all the related works, I can focus
on the truly relevant for my discourse, and then at least cite all the others.

*** Annotated bibliography
Don’t just put the references here, but add context:
- state how it influenced the manuscript
- describe what the reader may find inside

An example would be:

Clear and simple as the truth ~ Writing classic prose Francis-Noël Thomas, Mark
Turner

On the surface, this book is about the /classic style/ of writing attributed to
French authors like Descartes and de Montesquieu, but which can be found in
authors from other countries, other times and in other languages.  This style’s
virtue is exhibiting the truth in a direct manner, shunning abstract discourse
and hedges.  Authors following this style present the most sophisticated ideas
to the reader as naturally as if they were describing a scene of nature.

I think that the characteristics of classic style apply to programming
languages.

Programming is not the act of typing keys on the keyboard in a certain order.
Programming is describing an often complex process undertaken by the computer,
in a program that should be clear and simple to the human reader.  The
programmer conveys its meaning by combining the features of the chosen
programming language in a way that, ideally, directly reflects the process he is
implementing.  An example would be to use functional composition to implement a
pipeline process; a function is an adequate representation of the pipe, since
both have one output that results from transforming their inputs.

Consider the problem of counting letter frequencies in a text.  Given an input
text in English, give, for each 26 letter of the alphabet, the number of
occurrences of the letter over the total number of letters in the text (the
frequency of the letter relative to the text).

First, the programmer would find an algorithm for the problem: divide the text
in letters, normalize it (remove non alphabetic characters and flatten the
case), count each letter separately, then compute the frequency (n_a / \sum_{i}n_i).

At this stage, we can write this process in code, in an idealized language.
This is a hazardous exercise, as we are trying to find just the right level of
specification to avoid over-specifying, while still being as precise as we can.

#+BEGIN_EXAMPLE
text-freq = normalize | count

normalize = filter not-alpha | lowercase

count nil freqs = freqs
count letter:text freqs = count text (freqs[letter] + 1)

freq letter freqs = freqs[letter] / sum(freqs)
#+END_EXAMPLE

Here, a vertical bar =|= takes the input of the left and feed it to the right.
We rely heavily on Haskell syntax to provide the necessary context in order to
help the reader fill the blanks.  But we do not explicitly give any semantic to
this pseudo-language.  In some ways, it is more helpful than the English
algorithm, but without semantics it can really mean anything.  At this stage, we
are hoping that the reader connects the dots that we have in mind.

Should we write out the definition of =lowercase=, =filter=, or =sum=?  If we
did, should we write out the definition of their constituents as well?  When do
we stop?  Well, we cannot go wrong by specifying /everything/, but we will spend
a lot of time doing so.  Instead, we should stop when the level is evident to an
intelligent reader.  A good criterion would be: give this to any competent
programmer, and if she implements it without asking for clarification, then you
provided sufficient information.  The same dilemma occurs in classic style: when
to explain, and when to take for granted?

Here we are taking for granted =lowercase=, on the basis that it should be a
well-known function to any competent programmer.  Instead of defining it, we can
specify how it should work: applied to any uppercase alphabetical character of
the English alphabet, it should return the corresponding lowercase character,
and be the identify for lowercase characters.  Amusingly, if we had to implement
=lowercase=, we would have the same problem of finding the right level of
description.  Since it is a common function in many languages, we can file it as
trivial.  But we can see that we are only building on top of existing knowledge,
of an existing context: the language and its API; itself built on its compiler;
itself built on machine code; itself built on PC hardware; itself built on
transistors; themselves built on electricity; and we have attained the ground
truth: nature.

It is mesmerizing to contemplate how deep these ramifications go, and how a few
words can trigger a torrent of associations and ideas, and how our minds select
the correct associations depending on the context in which they were written or
read.  But this is also the source of confusion in any communication: assuming
both parties have the same context for the current argument.

At this stage, we should refrain from over-specifying, and state only what is
needed by the problem.  Failing to do that, we might constrain the
implementation unnecessarily.  For instance, in common programming languages we
often specify the order of execution implicitly, even when such order does not
matter, hence forcing a sequential order where parallel execution would lead to
better performance.  The trick to not over-specify is to state only relations
between functions, and not say a word about algorithms or data representation.
The formalism of mathematics is often well-suited for this task.

We would then write:

: occurs(text, letter)
is the number of occurrences of =letter= in =text=

: freq(text, letter)
is the frequency of =letter= in =text= relative to the other 26 letters of the
alphabet.  So,

freq(text, letter) = occurs(text, letter) / (\sum_{l} occurs(text, l))

Leaving us to implement only =occurs=.

#+BEGIN_SRC haskell
  import Data.Char (toLower)

  occurs :: Char -> String -> Int
  occurs _ [] = 0
  occurs l (t:ts)
    | l == normalize t = 1 + rest
    | otherwise        = rest
    where rest = occurs l ts

  normalize :: Char -> Char
  normalize = toLower
#+END_SRC

This example is deceptively simple.

We also use names to trigger contextual associations from the reader.  Names are
useless for the machine: machine code has no name.  The only significance is the
order of bits.  Putting arbitrary names is an effective way to obfuscate a
program (e.g. minifiers for JavaScript, or obfuscators).  Names are tremendously
helpful for humans, but can lead to confusion.  Names are ambiguous.  If a
function is called “filter”, I would expect it to filter its input in some way,
and preferably to behave just like the well-known filter of functional
programming languages.  Likewise, if a function is called “getResults” and
actually brews coffee, the name is misleading.  A short name is better than a
long name, for both reading and writing.  But a short name can only tell so
much, hence short names invite a decomposition of the program into small units.

Related: Rich Hickey’s “Simple made easy” talk, and Bret Victor’s concept of
“direct manipulation”.

** Advice from “How to write a better thesis”
Useful excerpt from [[cite:EGZ-14][EGZ-14]].

*** Link words
Link words indicate the logic flow in a passage of text.  There are two kinds:
conjunctions, which are used to link clauses in a sentence, and transitional
words, which are used to link a sentence to the one that preceded.

- Common conjunctions: but, although, unless, if, as, since, while, where,
  before, after, when, because, for whereas, and, or, nor.
- Transitional words: however, thus, therefore, instead, also, so, moreover,
  indeed, furthermore, now, nevertheless, likewise, similarly, accordingly,
  consequently, finally.
- Transitional phrases: in fact, in spite of, as a result of, for example, for
  instance.

*** The “standard” structure:
1. Introduction, problem statement, aim and scope, thesis overview
2. Background, history, current theory, current practice
3. Core, own work, proposals, results
4. Synthesis, analysis, discussion, conclusions

*** Aim for a narrative
A story that will take the reader along the road to where I want them to go.
This path should be straightforward.

#+BEGIN_QUOTE
You may think to yourself: I have had to fumble, and explore, and make mistakes
to get here, but I am now writing the guidebook that helps the next person to
painlessly come to the same point of view and the same knowledge.
#+END_QUOTE

*** What to put in the appendices, what to discard
Anything that would distract the reader from the main argument should go into
the appendices.

But only put material that a reviewer would want to follow.

*** Introductory chapter
Stick to a single aim.  Do not describe how you intend to achieve this aim;
reserve this for a later chapter.

Establish the scope of your study: time, location, resources or established
boundaries of a field.

*** Background chapter
Provides a map of the territory you intend to cover.

Will lead the reader from where you started (when you began your thesis), to
where you are now.

Write /defensively/: if you think something might confuse an examiner, address
it.

Pick a baseline against which to compare your results.  The “best” baseline in
the scope you have set, of course.

* Musings
Things I think about, that had to come out at some point.  They are
very relevant to how I felt during my thesis, but they might be too tangential
to appear in the manuscript, even though they influenced it.

** What interests me
How to design modular programs using language constructs.  To be concrete, here
is the scenario of instrumenting interpreters.  How to build interpeters that
can be extended without changing a bunch of lines of code.  In essence, how to
design program that are extensible and clear.

The clarity of a program is how well one can understand what the program does by
reading the source code.  I believe clarity can be greatly influenced by how we
choose to construct our program using languages and patterns.  Programmers are
prompt to point out that certain features of programming languages can be
detrimental to clarity (“GOTO considered harmful”), going as far as preaching to
others programmers to avoid whole families of programming languages altogether.
On the other hand, it is frequent to come upon praises of others features or
paradigms (“monads are awesome”, “functional programming makes program
clearer”).  But like natural languages shape thought, programming languages
shape programs, and languages are not all equivalent in clarity when applied to
different problems.

I’m interested in the process of going from the problem, to the mental model, to
the program, and the interactions between these levels.  A clear program is a
perfect match with the conceptual model of the problem: no line of code is
superfluous, and each line can be mapped to a part of the problem.  There is no
accidental complexity (see Moseley’s “Out of the Tar Pit”).  The program is as
complex as the problem is, no more, no less.

To form a mental model is very related to a learning activity described by
Papert and Piaget.  Going from a problem to a mental model is learning the
problem.  The mental model is not formed at once, but is enriched piece by
piece.  Similarly, the program is not written all at once, but line by line,
feature by feature.  When one’s comprehension of the problem expands, the
program must follow.

I see programming as adapting the model of the problem to the model of the
computer.  I have a mental model of how the computer works; I have a model of
how such and such languages work.  When I have constructed a model for a
problem, I can compare it to my models of tools (languages) and see if one fits
better than the other.  It is unlikely that one language fits perfectly, but the
closest will do.  From there, I can use the constructs of the language to
approximate the problem model.

On top of the model for languages, I also have countless models of how to solve
generic problems, built from experience.  These are generally the first models
used when looking for a match to the problem.  “This is a graph problem”, “An
observer pattern would solve that”, “You are missing a semicolon”...

I don’t feel that this process is specific to programming.  Essentially, it’s
just pattern matching.  Matching forms and processes of the real world to the
more abstract ones in our minds.  It’s a human thing, a consequence of how our
brains work.

This view of programming is a running thread for the work I’ve pursued during my
thesis.  Finding just the /right/ solution for the problem, and not a
catch-everything miracle.

** Finding the right program for a problem
Is a fitting problem.  You choose the most appropriate language that fits the
problem, then write the code that maps the flow of your solution perfectly.

Nothing left to add, nothing left to take away.

It’s like those children toys where you have to match shapes, except you have to
build your shapes from small parts.  If your shape is too small, you have not
solved the problem completely.  If your shape is too large, your code does too
much.

Sometimes you change the shape of the problem, because it is much like another
problem you have already solved.

The same idea can be found in the “least upper bound” of sets, or the “tight
necessary and sufficient conditions” of my master thesis.

** Comparing strengths and weaknesses, not playing favourites
It is common to hear programmers argue about their favourite language.  Java is
crap, Scala is better, Clojure is awesome, Haskell is fantastic...  All these
assertions are often rooted in personal sentiment, or signaling membership to a
specific tribe.  When the arguments are based on seemingly objective
observations (“type systems makes you more productive”, “C makes you write buggy
programs”), they are rarely based on reproducible facts, but rather on
subjective experiences and self-confirmation.

The fact is, few serious studies have been done on the benefits and drawbacks of
different programming languages.  The same goes for other tools like text
editors, IDEs, terminal emulators or web browsers.

One thing that I like to do, is to write the same program in different
languages.  Or rather, solve the same problem in different languages.
Often, one language will allow for multiple solutions.  It is then interesting
to compare the benefits of the solution in terms of readability, clarity,
simplicity, and extensibility.

In this thesis, I would like to exhibit a number of ways to write an extensible
interpreter and its extensions.  Hopefully there will be no subjective
statements (elegance of code is in the eye of the beholder), but merely
comparisons of the strengths and weaknesses of different solutions.

I’m not hoping to settle the debate on what language to use.  First, it seems to
me that languages are primarily bundle of features, and these features have
found their way into many languages.  So rather than praising or dissing a
language, we should focus on features.  But second, there won’t be one solution
to satisfy everyone, because each programmer has different needs, even when
considering the same problem.

Instead, I’m hoping that programmers will recognize that language features are
tools, just like algorithms or data structures.  One should always have the most
useful tools in his belt to be able to solve common problems efficiently.  Some
specific problems might require specialized tools, or the knowledge to apply
common tools correctly.  This document provides knowledge to apply common tools
to solve the specific problem of interpreter instrumentation in a modular way.

** A programming language is the interface of a program
Writing a program in a language is giving instructions to the interpreter or
compiler of that language.  To allow a wide range of programs to be written, we
use complex languages rather than merely passing options to gcc, because
languages provide greater orthogonality.

But the language is the input of the interpreter.  The output is the movement of
bits in the machine.

Since we have this level of indirection (why is it required?), this abstraction,
it makes sense to design it in a way that allow for a great and precise control
of the bits’ movement.

The programming language is an interface, and it must be designed following the
guidelines of human-machine interaction.

In this view, a framework gives you one interface that is convenient for some
tasks.  Using a framework is like using a front-end program for ffmpeg when you
just want to rip CDs to MP3s.  The base program is more powerful, but you have
to remember which options to declare, and input them each time.  Using the
front-end program, the task is simpler to express, but the front-end cannot do
everything that ffmpeg does.  The framework provides an interface for the
programming language, in order to simplify the creation of selected programs.

On the other hand, a library essentially enhances your interface, without being
partial to some tasks.  Libraries compose, whereas frameworks rarely do.  Using
a library is like composing programs with a pipe.  Greater re usability.

Programming language designers often tout the regularity of their language, or
its simplicity.  Both properties are important for /learning/ a language, but
can actually hinder its /use/ [[cite:Gru-89][Gru-89]].

Designers tend to favor consistent languages because they are simpler and more
elegant.  However, users favor getting things done, and seldom care about the
internals of the language.

Programming languages are interfaces, and we should design them to be convenient
to use.  We should design them from the problems they aim to solve.  This
document specifically deals with the mechanisms one can use to solve the problem
of modular instrumentation.

** Modularity is a human concern
The end goal of programming is moving bits in the machine to obtain a desired
outcome.

Writing a program is giving a static description of the movement that will take
place when the machine executes it.

The machine does not care about the elegance of the code.  It does not care
about your choice of variable names, or class decomposition.  If anything, all
these abstractions often lead to less efficient machine code.

Modularity and elegant code are targeted to human programmers, human readers.
Finding out what is pleasant to the human brain is harder, less absolute.  Maybe
it’s not much a science.  There are measures we can apply, but we cannot measure
what counts.  The full appreciation of an elegant code is in the intricate
interplay of language features and problem domain.  Very much like a work of
art, there is no formula, and a theory would be ill-advised.

That said, there are choices of language design that promote or hinder
modularity.  What choices, and to what extent do they affect modularity?  That
is what this text is really about.

** Shapes of computation
AOP is a language for altering the control flow, but there is a dissonance
between writing an aspect, and writing the program.  Writing a program is
mapping the problem space to the computer space.  High-level languages encourage
the programmer to think in terms of abstractions higher than registers and
instruction pointers.  The mental model of OO is object + messaging.  The mental
model of FP is function + referential transparency.

AOP forces you to think about the control flow of your application.  Pointcuts
are inherently control-flow related: =call=, =execution=, and of course =cflow=.
You are writing pointcuts using the same identifiers as in your program, but
they actually describe different objects.

Knowing the shape of your computation is crucial to write robust programs [[cite:AS-96][AS-96]].

Is there a programming language which is clearly matched to the shapes it
produces?  I guess it’s not that simple.

In [[cite:PLM-07][PLM-07]], semantics patches are used to pattern match on the syntactic
structure of target programs.  Pointcuts are used to capture joinpoints, and
joinpoints are created by the dynamic control-flow of a program.  However, the
pointcut language is /not/ pattern matching.  You don’t write:

: f(...)

but

: call(f)

And you write

: call(f) & cflow(g)

instead of

: stack[f ... g]

We use pointcuts because we lack the adequate language to describe the runtime
behavior of the program.

Pattern matching on strings, inductive types, or abstract syntax trees, in all
of those we are dealing with text.  So it’s easy to write a pattern in a text
editor.  However, the shapes of computation are very dynamic.  We don’t have a
language to describe them.

** Prescriptivists and descriptivists
Referring to the philosophy of linguistic researchers.

I’m a descriptivist.  I describe what languages exist, how they are built, what
constructs they contain, how they are used by programmers to build programs.

I know of some prescriptivists.  Dijkstra was a prime example.  They prescribe
which languages should be used, and which should not; which constructs are
harmful, which are good.  They assume the Sapir-Whorf hypothesis, and are
baffled that programs written in “badly-designed” languages work.

The machine does not care, but we do.  It’s an amusing phenomenon.

Anyway, if you are a prescriptivist, this manuscript will probably bother you.

Maybe include a few questions for discerning on which side you stand?  “Type
systems are necessary: agree/disagree”, etc.

** Unanticipated extension is just a hack
A catchier term might be /post-hoc extension/, following its use by Benjamin
Lerner’s thesis.

A program is made according to a specific mental model.  In its simplest form,
this model does not account for all reasonable or exotic extensions one could
add to the program.  As a result, the program is made simpler to build,
understand and maintain.

When someone wants to extend the program, they have a renewed mental model of
what the extended program should do.  The problem is: the original program was
/not/ made with the extended model in mind.  Now there are two ways to implement
this renewed mental model:

1. from scratch, as its own program
2. by hacking the first program that was not meant to be used in this fashion

Now, depending on the models themselves, either #1 or #2 may lead to the
simplest program to build and understand.  However, #1 will be a standalone
model, while #2 will be a fragmented one.  In #1, any part of the original model
that is superseded by the extension can be deleted from the model, thus making
the program simpler.  In #2, the model is larger, hence more complex, even if
some parts of the original model are overridden.

Think of a drawing made on paper.  If you only have a pencil and no eraser to
draw, every mistake must be crossed-out.  When the drawing is done, any mistake
will be unwanted noise, a byproduct of the process that obscures the final
result.  If one were to draw a model on paper, without mistakes — a clear
model.  Then #1 would be like taking a new paper and making a new drawing
inspired by the first, while #2 would be like taking the first paper and drawing
on top of it, but without an eraser.  If per chance the additions of the second
drawing do not conflict with any of the preexisting lines, then both ways lead
to a clear picture.  However if any line is superfluous, or even contradicts the
goal of the second model, then it must be crossed-out, and the resulting drawing
is less clear than a fresh one.

Two things spring to mind:

1. That this problem can be found also in drawing, writing or composing music
   suggests that it is not specific to computer science, but rather to all
   creative activities.  Programming is not unique in this regard.  Still, these
   activities are sufficiently different that tentative solutions to the problem
   in one of them may not be applicable to the others.
2. Music composition may be a better analogy than drawing.  When programming,
   you write source code that is meant to be executed.  In music, you write
   musical notation that is meant to be played.  When you want to change a
   particular piece of music, you have to specify the change using musical
   notation.  The analogy is a better mapping than drawing, where the distance
   between the end result and the process is smaller.

Unanticipated extension is just a hack in cases where the original model was
never intended to support such an extension.  This is in fact one of the common
usage of the term “hack”: to cleverly leverage existing facilities in order to
solve a problem these facilities were not intended for.  But when we build
systems, and more importantly when we maintain these systems, we tend to see
hacks as temporary workarounds, rarely as the proper solution.

The trouble with the proper solution is that it makes rethink your whole
approach.  Your are confronted with problem A, you think for a while and build a
model M of this problem that you implement as program P.  Then a bit later,
someones come along and introduces you to problem B, which kinda looks like A.
Since you have built a mental model M of A, you try to fit B to M.  Most of B
fits, and only a few bits stick out.  So you decide to extend program P into Q
which only have a couple of bifurcations in the control flow to deal with the
exceptions brought by problem B.  There, you solved B.  And you re-used your
mental model M, saving you precious cognitive load.

Now someone comes along with problem C (of course), and C kinda looks like B,
which looked like A, but with additional bits sticking out — and not the same
one that made B different from A.  You /could/ stretch your model M to fit C,
but that would make for a messy model with lots of exceptions.  The same goes
for the program Q, which could be turned into a solution for C, but the
resulting program would not make for an easy read.  Still, A, B and C do share
similarities.  You have to work to find the variable parts and the static parts,
then design a model that allows you to easily plug in different variable parts
that work well with the static ones.  Call this model N.  Now A, B and C are all
expressible by model N.  But model N is sufficiently different from M that you
need to overhaul your program Q (or P) into program R.  From R, you can create
with few additional code (really configuration) solutions to A, B and C.

** Why JavaScript?  What’s so special about it?
Well, I guess the main reason is that you have a JS VM in all major browsers,
and that web application are increasingly popular.  So that’s a vast domain of
application right there.  Solve a “problem” in JS, you potentially impact
millions of everyday users.  That’s reason enough for lenders of research
grants.

In addition, it’s not like you have to look far to find security issues in web
applications.  XSS, CSRF, scams, phishing, and now ransomware.  We could all do
without those.

The question is though, is JavaScript /worse/ than other languages in respect to
security?  And that’s a difficult question to answer, in large part because
JavaScript is the only language that has been largely used to create client-side
web applications.  Plenty of variants and alternatives to JS exist, but none
with the same popularity.

And I’d be warry of doing comparisons between languages alone.  Say, mine
TypeScript projects for defects, and compare with comparable projects in JS.  I
would guess that the background of JS devs is on a much larger spectrum than
webapps devs using TypeScript.  If you /chose/ TypeScript, chances are you
already care about the security of your web programs.  Or in the case of Dart (a
different language, not a superset), I suspect the programmers who use Dart are
more skilled in programming than the average JS dev.

And even if you took top programmers in TS (or Dart) and JS, and have them both
build the same application to spec, are you really evaluating the “robustness”
of the language?  How do know they are equally skilled?  What about the
toolchain?  Editors, IDEs, test suites?  What about methodology?  Cascade
vs. agile?  Many variables here that get in the way of evaluating the language
itself.

So if you can’t easily evaluate the language at the top, from the end-user side.
What about at the bottom?  When you consider the language in itself, and not its
applications?

That’s what theoretical types do.

** What’s the ideal secure language?
Ha!  Thanks for asking, because as it happens, there is one such language.  I
call it ‘P’, though not many people are aware of it.  Here are its syntax and
semantics (operational, small-step):

#+BEGIN_EXAMPLE
Syntax: ⟂

                  true
Rules:       ----------------
                   ⟂
#+END_EXAMPLE

All programs in P are guaranteed to terminate.  They are also XSS- and
CSRF-free.  P programs do not leak sensible information, nor can they harm your
browser and computer in any way.  As a matter of fact, P programs cannot do
/anything/.

[[http://channel9.msdn.com/Blogs/Charles/Simon-Peyton-Jones-Towards-a-Programming-Language-Nirvana][Following Simon Peyton Jones]], you can classify all programs along two axes:
usefulness (I interpret that as “easy” to write a program doing X) and safety
(you can control every effects).  C would be useful but unsafe, and Haskell
(jokingly) safe but useless.  Jokingly, because P /is/ the idealized safe but
useless program.

But Peyton-Jones has another insight for us in this video: even if your language
allows you to program all kinds of algorithms, if running the program has no
side-effects in the world, *it is useless*.  You can compute π to the 123rd
decimal, but if you don’t save the result somewhere, what’s the point?
(Arguably, executing the program /is/ a side effect on your machine, but that’s
a side-effect of the interpreter/virtual machine rather than of your program.)

So, what’s the ideal secure language that is also useful?  One with which you
can write any program, but where all the effects of running the program are
under total control of the programmer, or of the user running the program.

** On what you can say about a given program
Take regexps.

#+BEGIN_EXAMPLE
.*
#+END_EXAMPLE

What can you say about strings matching this regexp?  They are of unlimited
length, containing any character.  Basically, the regexp matches /all/ strings
you can create.

What about that one:

#+BEGIN_EXAMPLE
a
#+END_EXAMPLE

It matches exactly one string, the string beginning and ending with “a”.  Given
the regexp, the set of non-matching strings is infinite (it won’t match “b”, it
won’t match “bb”, and so on).

In programming languages land, in order to say what a program can or cannot do,
the regexp becomes the syntax + semantics + environment of the language.

The (somewhat) equivalent examples would be:

#+BEGIN_EXAMPLE
eval(stdin)
#+END_EXAMPLE

Can’t know what the program will do, depends entirely on user input.  Though
this is /partially/ true when you consider what you know about the environment.
You could restrict behavior in the environment itself, rather than in the
program.

The other example:

#+BEGIN_EXAMPLE
return 1
#+END_EXAMPLE

Only one purpose, does not depend on input.  You can say for sure that this
program does not harm your computer (well, depending on the exact semantics,
once again).

Maybe that’s the issue: that you have to take the full stack into account for
giving guarantees.  Or maybe regexps are conceptually simpler.  Is that because
of their lower computational power?

** What work I did during the thesis
- Playing with interpreters written for lambda calculus and trying to change
  their behavior for faceted evaluation.  Without presuming too much about the
  structure of the interpreter to be general.
  - JS protos revolved around manipulating scope and using prototypes, both for
    constructing an interp (LASSY) and for instrumenting it (DLS), WITHOUT
    true dynamic scoping
  - Lisp proto shows dynamic scoping is a solution
  - Haskell prototypes were failed attempts at using monads to define analyses.
    Extending the syntax is brittle (Open Data Types), but allows one limited
    form of extension.

Trying to formulate an understanding of how all these solutions relate to each
other.  What is common idea behind the syntax, grammar and semantics of each,
when trying to solve post-hoc extension?

- Thinking about language mechanisms for solving extensibility issues:
  - OO: inheritance, composition, delegation, design patterns, dynamic and
    multiple dispatch, ...
  - AOP: joinpoints, pointcuts, weaving
  - Dynamic scoping (and delimited dynamic scoping)
  - Macros: static code as data
  - Reflection: dynamic manipulation of language structure/semantics
  - Continuations: linear control flow is easier to instrument
  - Monads: reify computation; dynamic code as data
  - Type systems: mostly working around them as they are antagonistic with
    post-hoc extension

Maybe we can make two categories of mechanisms: those that assume the
interpreter has a specific structure (class-based, linearized, ...), and those
with minimal assumptions (dynamic scoping, macros).

Seeing how these mechanisms can be equivalent when looked through the post-hoc
extension lens.

- Applying this knowledge to Narcissus.  And dynamic analyses.

** I don’t believe in security.
Security is confidence, assurance, certainty, the absence of risks.  Security is
immobility, immutability, predictability.  Security is a never ending game of
building bigger mousetraps, only to find your grain eaten by bigger mouses.  We
won’t have secure programs, or secure computers, since they are all embedded in
the real world.  The real world is always moving, always changing.  Things grow
and die all the time.  Accidents happen.  Tragedy is inevitable.

The human desire for security is a desire to control reality, to control
nature, to control future outcomes.  Secure programs are programs that we have
control over.  We do not want strangers and malicious third parties taking over
our computers, because that is a loss of control.  Leaked data?  Loss of
control.

Of course there are reasonable arguments for secure programs: programs that we
do not control would be barely useful.  What good would be an unreliable sort
algorithm?

How secure is secure enough?  Proponents of secure programs invoke exceptional
events like Ariane 5, or OpenSSH, and they ask for safer languages, safer
tools.  There is a great cost incurred by going to 100% safe, if such a goal is
even attainable.

I believe in adaptability.  The world changes, then we must change with it.
Plan for failure, as you cannot prove that it will not happen.  Crashes happen,
and will continue to happen.  Computers are made of thousands, if not millions,
of components.  They run in warehouses, on electricity, provided by cables.  All
these components are embedded in reality, ready to fail when the next disaster
happens.

Redundancy is more reliable than Coq proofs when it comes to real-world
software.  Things fail.  Robust programs are better than secure, or “safe”
programs.  Robust programs deal with erroneous input; robust programs recover
from error; robust programs continue to operate when the rest of the world
crumbles.

Secure computing is a dream for control-hungry plutocrats, it is running against
the currents of reality.  Adaptability is letting go of the anxiety, accepting
that things will change, things will fail, things will burn.  It’s okay.  We’ll
recover and carry on.

** Fast and forward research vs. slow and backward research
Usually, a thesis is a of deep incursion into unknown territory.  The manuscript
should revolve around new results, with a background section comparing existing
approaches to the problem the thesis aims to solve.  The emphasis is on new,
original results and new insights on a problem that is usually recent, but can
also be an old one.

The thing is, I never feel anything is new.  New problems are just old problems
in different clothes.  New solutions are just old solutions put together and
slightly altered.  From this perspective, no work is new or original; any work
just flows logically from what was known before.

Young researchers are often advised to read recent papers in a field to find new
or “low-hanging” problems.  Papers from 10 years ago are seldom considered
relevant for today’s problems.  There is the issue of bounded time: if you only
have to read all the relevant papers for your thesis that were published in the
last 10 years, that sets a manageable bound on your reading list.  If you
consider any work published in the last century relevant, then you will spend a
lot more time reading, and less time publishing.

I feel we would be better of reading more, and publishing less.  The future is
reflected in the past.  One of the goals of a thesis is providing new insight on
a problem.  Well, there still is considerable insight in past research.  Even in
2015, a 1962 paper is still many years of work condensed into a dozen pages.
Disregarding most of that past effort seems wasteful.

It is true that past research tends to get condensed as it is better understood.
In fact, this is the general process of acquiring knowledge: gather up
individual scenarios, then factorize a common schema out of the scenarios.  The
factorization gives way to a mental model that can be used to quickly learn
“new” situations by matching them to an existing mental pattern.  In papers,
individual contributions do not matter as much as the mental model they evoke in
the reader, the insight they provide.

Doing “fast and forward” research will cause you to ignore what already existed,
and could have been reused.  Reusing source code, reusing thoughts model is a
strong theme in this thesis, and it applies to research as well.  I feel that it
is easier to dismiss all the body of existing work, and instead focus only on
“hot topics”.  But ultimately, publishing without looking back is doing a
disservice to research.  Research works are increasingly produced every year,
and the trend is accelerating.  Publishing without taking the time to put your
work into perspective of what has been done before is disrespectful of the time
spent by reviewers and readers.  The peer-review process is supposed to be
safeguard against redundant works.  Unfortunately, the increasing volume and
specialization of research work against peer-reviews.

Authors should take the time to reflect on how what they have found relate to
existing insight.  This is a tenet of this thesis.

In fact, I don’t feel anything in this thesis will feel new to someone familiar
with programming languages.  What I hope that this thesis does is give a fresh
perspective on what modularity means to programmers, and expose the multiple
ways there are to create modular interpreters.

* Rough draft
Thesis: the modular instrumentation of interpreters can be achieved through
language mechanisms.

** The narrative
Take a program, try to change its semantics and minimize the changes made to the
program source code.

There are many ways to do that, and not all are equivalent.  How do they differ?
Is there a common concept that lie behind them?  These are the initial research
questions.

It seems this process [[Unanticipated extension is just a hack][mirrors]] one involving mental models in the brain.

I’ve started by looking at the case of an interpreter and dynamic program
analyses.  However, this is just the application, as it turns out that the core
of the problem lies in changing what a line means by dynamically changing
bindings.

What I hope this thesis provides, is a mental model to think about extending
programs.  What it means, conceptually, and what it means for the source code.

** What should be in it
- a program is an explanation
- modular is simple
- separation of concerns goes with modularity
- thinking about features: base behavior vs. extensions
- programming incrementally
- instrumentation is like customization
- css is a customization language
- emacs is a customizable platform
- leaving holes for extensions: Lua game engines, visitor pattern
- minikernels vs. monolithic kernels
- preparing for extension vs unanticipated extension
- instrumentation is unanticipated extension
- monkey patching, copy-pasting, AOP
- dynamic scoping, global scopes, namespaces
- Lambda papers
- granularity of extension points
- modifying the control-flow vs modifying the source
- patches are instrumentation (brittle)
- drawbacks to separation: implicit control flow changes hamper reasoning
  (COMEFROM, GOTO)
- JS interpreters + information flow case studies
- JS interpreters + modes of evaluation (strict/strong)
- functional instrumentation: control-flow is data
- throwbacks to Lisp (code is data)
- monads, free monads
- Lassy’15 work
- Swierstra work
- Application to Narcissus

*** What is currently missing from the plan
- instrumentation is like customization
- css is a customization language
- emacs is a customizable platform
- leaving holes for extensions: Lua game engines, visitor pattern
- preparing for extension vs unanticipated extension

** Introduction
I introduce the themes of the thesis, its setting and its goals.

The themes:
- modularity of programs
- modularity is clarity
- separation of concerns
- incremental programming
- code as data is the epitome of modularity

The setting:
- JavaScript security
- instrumentation of interpreters
- information flow analyses
- Narcissus

The goals:
- understand what makes a program modular
- understand the benefits and downsides of modularity (in particular, with
  respect to security)
- understand how we can build modular interpreters for the purpose of
  instrumentation
- give a fresh perspective on languages from different communities: JavaScript,
  Haskell, Lisp, Scala.  Different programming disciplines to solve the same
  problem.

I also describe what the thesis does /not/ cover, in order to give reasonable
expectations to the reader.

** Background
We extend the themes and setting of the introduction to give the reader all the
background necessary to understand the choices made in the thesis.

We show how existing work relate to the problem of modular instrumentation.
Specifically we show that very few work have targeted this particular problem.
Though there are numerous works concerned with modularity in interpreters, most
prominently around the expression problem.

Conclusion should be: modular instrumentation is still a fresh, and legitimate
axis of research.

*** Structure of this section                                          :meta:
Organizing this section is hard.  There are many themes I want to cover, but the
analysis of the related work should stay relevant to the thesis’s goals.

Modularity is an overloaded term, with confusing expectations.  Extensibility is
narrower, prefer the latter.

The contributions all revolve around extensible interpreters.  How to build
them, and what mechanisms to use to extend existing interpreters.

Separation of concerns is closely related to extensibility, and can be seen as a
pre-requisite.  An interpreter is hardly extensible for analyses if the code for
all the analyses is tangled together.  We must know how to achieve it as well.

Modularity, complexity, separation of concerns.  All have been a driving force
in software engineering since assembly.  There is an evident chronological
presentation of extensibility: from assembly to post-object.

There is also a strong methodology/language/tool spectrum of separating
concerns.  I focused on languages, but I now believe more in methodology and
tools.

A third axis is what they were used for: building versus extending.  This one is
fuzzy, and overlaps perhaps too much with the contributions.

I have collected examples of /extensible systems/ (Emacs, IDEs, Browsers).  What
make them extensible is relevant to the thesis.

A nice solution to presenting this session is to start from a map of the process
of creating a computation from the source code of target programs.  From source,
to AST, to bytecode generation and evaluation in the browser.

The browser as well was generated from source code, so there are potential
points to exploit as well.

Each node in this map is a site potential to accept the modifications needed by
the analyses.  Some, however, are more convenient than others.

But most related work can be placed on this map, providing a convenient visual
‘heat chart’ of the field.

*** Setting
The ELI5 section of the thesis.  I explain all of that and foreshadow the
following subsections.  If I you don’t get what is all this stuff and why you
should care, then you can put down the thesis now.

**** What is a program?
**** What is a programming language?
**** What is program modularity?
**** What is an interpreter?
**** What does it mean to instrument an interpreter?
**** Why should this instrumentation be modular?
**** Why should we care about separation of concerns?
**** What is JavaScript?
**** What is an information flow analysis?

*** Modularity background
Modularity has been a concern for programmers for a long time.  Plenty of
solutions abound.  I stick to modularity in programs from a programming
languages point of view.  But you should know there are other ways to build
programs than inputting source code, and there also modularity is a concern.

Should also cover:
- scoping / dynamic scoping
- information hiding
- interfaces
- modules

But when selecting papers, be sure to get all the relevant ones first (the ones
that target instrumentation and interpreters are a priority), then take
“representative” papers about other programming disciplines.

Notion of modularity [[cite:OGK+11][OGK+11]].  Modularity is rooted in classical logic thinking.
Classical logic is inflexible, incompatible with the realities of software.
Especially, information hiding is not the silver bullet.  Approaches to software
development that seem to break information hiding, and even oppose modular
reasoning, have their virtues.  Those can be thought of using nonclassical
logics.

Parnas is usually credited with the notion of modularity, as well as notions of
/separation of concerns/ and /information hiding/ [[cite:Par-72][Par-72]].  Parnas advocates
improving the methodology of programming through up-front planning and critical
analysis of designs.  He does not believe in language solutions to software
modularity [[cite:DBB+03][DBB+03]] [[cite:Par-96][Par-96]], although he is often quoted by proponents of
modularity through languages.

#+BEGIN_QUOTE
My engineering teacher laid down some basic rules:

1. Design before implementing.
2. Document your design.
3. Review and analyze the documented design.
4. Review implementation for consistency with the design.

There rules apply to software as least as much as they do to circuits or
machines.
#+END_QUOTE

**** Structured programming
Argues for a single entry point into procedures, and single exit point.  Not
jumping directly in the middle, or exiting prematurely.

Exemplified by ALGOL, and Pascal [[cite:Wir-74][Wir-74]] [[cite:Wir-74a][Wir-74a]].

Dijkstra notoriously argued against the GOTO statement, as a superfluous control
structure [[cite:Dij-68][Dij-68]].  On grounds of obscuring the “independent coordinates”
implicitly used by programmers to understand the dynamic flow of a program.
“Unbridled use” of GOTO statements makes finding such coordinates “terribly
hard”.  In short, peppering GOTO statements leads to spaghetti code.

The article has a strong prescriptive tone, as usual from Dijkstra, yet it opens
with a reasonable appeal: “to shorten the conceptual gap between the static
program and the dynamic process, to make the correspondence between the program
(spread out in text space) and the process (spread out in time) as trivial as
possible”.

On the legacy front, most programmers are cargo-culting the fear of GOTO (though
Knuth argues that it has its uses [[cite:Knu-74][Knu-74]]).  Few languages in use today propose
it.  However, the discipline of single-exit is more controversial, as most
modern languages offer constructs for early exits from procedures (return
statement) or from loops (break and continue statements, sometimes with
labels).

The fear of GOTO is an example of focusing on the wrong issue: structured
programming is a proposal for clearer programs.  Blindly removing all GOTOs and
labels from an unstructured program does not make it structured.  The focus is
on writing programs that clearly reflect their dynamic process.  As Parnas noted
[[cite:DBB+03][DBB+03]], modularity is solved by improving the design and documentation
processes, not by adding a “module” statement to the language.  The same
situation arises here.

**** Literate programming
Programs are constructed as they are explained.  Knuth, LiterateCoffee, Org
mode.

[[cite:Knu-84][Knu-84]] for the original notion:

#+BEGIN_QUOTE
Instead of imagining that our main task is to instruct a /computer/ what to do,
let us concentrate rather on explaining to /human beings/ what we want to do.
#+END_QUOTE

As usual, Knuth writing is delightfully witty:

#+BEGIN_QUOTE
I must confess that there may also be a bit of malice in my choice of a title.
During the 1970s I was coerced like everybody else into adopting the ideas of
structured programming, because I couldn’t bear to be found guilty of writing
/unstructured/ programs.  Now I have a chance to get even.  By coining the
phrase “literate programming,” I am imposing a moral commitment on everyone who
hears the term; surely nobody wants to admit writing an /illiterate/ program.
#+END_QUOTE

The WEB system allows one to write a TeX + source code document, and then
produce documentation (using the WEAVE program) or complete program (using
TANGLE).  The focus is on documenting first what the program does, then
producing a machine version as a second concern.  The source code can be
presented out-of-order in the document, for expository purposes, using links and
macros.

The WEB way of writing programs is “psychologically correct”, as it reflects the
way in which the program was conceived and elaborated.

#+BEGIN_QUOTE
When I first began to work with the ideas that eventually became the WEB system,
I thought that I would be designing a language for “top-down” programming, where
a top-level description is given first and successively refined.  On the other
hand I knew that I often created major parts of programs in a “bottom-up”
fashion, starting with the definitions of basic procedures and data structures
and gradually building more and more powerful routines.  I had the feeling that
top-down and bottom-up were opposing methodologies: one more suitable for
program exposition and the other more suitable for program creation.

[...] I have come to realize that there is no need to choose once and for all
between top-down and bottom-up, because a program is best thought of as a web
instead of a tree.  [...] A complex piece of software consists of simple parts
and simple relations between those parts; the programmer’s task is to state
those parts and those relationships, in whatever order is best for human
comprehension – not in some rigidly determined order like top-down or
bottom-up.

[...]

Thus the  WEB language allows a person to express programs in a “stream of
consciousness” order.
#+END_QUOTE

An unexpected benefit of WEB is a better separation of concerns.  Although Knuth
does not use the term, each part of a program can be described in its own
section, thus each section can focus on one concern.  He gives the example of
separating error recovery from a simple data structure update routine.

#+BEGIN_QUOTE
While writing the program for [error recovery], a programmer subconsciously
tries to get by with the fewest possible lines of code, since the program for
[updating the structure] is quite short.  If an extensive error recovery is
actually programmed, the subroutine will appear to have error-messages printing
as its main purpose.  But the programmer knows that the error is really an
exceptional case that arises only rarely; therefore a lengthy error recovery
doesn’t look right, and most programmers will minimize it [...] in order to make
the subroutine’s appearance match its intended behavior.  [Programming] with
WEB, the purpose of =update= can be be shown quite clearly, and the possibility
of error recovery can be reduce to a mere mention when =update= is defined.
When another section [related to error recovery] is subsequently written, the
whole point of that section is to do the best error recovery, and it becomes
quite natural to write a better program.
#+END_QUOTE

Knuth notes that the target programming language can impact the writing of WEB
programs.  Having to declare variables at the start of a program leads to
appending to the same “Local variables” program section.

Taking the time to document the code as you write it is not free, but is
beneficial in the long run.

#+BEGIN_QUOTE
I had known for a long time that the programs I construct for publication in a
book, or the programs that I construct in front of a class, have tended to be
comparatively free of errors, because I am forced to clarify my thoughts as I do
the programming.  By contrast, when writing for myself alone, I have often taken
shortcuts that proved later to be dreadful mistakes.  It’s harder for me to fool
myself in such ways when I’m writing a WEB program, because I’m in “expository
mode” (analogous to classroom lecturing) whenever a WEB is being spun.  Ergo,
less debugging time.
#+END_QUOTE

#+BEGIN_QUOTE
WEB may be only for the subset of computer scientists who like to write and to
explain what they are doing.
#+END_QUOTE

Noweb is a language-agnostic syntax and implementation of WEB, which is used in
Org-mode.

***** Mechanisms for extension
The idea of documenting as you program is important, as is the focus on writing
“what the human meant to do”.

The mechanisms of including and referencing code snippets allows one to
structure the program as they see fit.  Especially, it allows to separate
concerns through quantification.

**** Object-oriented programming
Objects impose another structure.  Design patterns are recipes for building
modular object-oriented programs.  Meta-object protocols let you manipulate
message dispatching for a great flexibility.
**** Functional programming
Pure functions are easier to compose.  Referential transparency, local
reasoning.

Monads and side-effects as computation.
**** Aspect-Oriented Programming
Manipulation of static and runtime code.  Joinpoints reifie extension points.
Pointcuts give powerful quantification over joinpoints.  Aspects promote
separation of concerns.

Treats the code as an implicit interface.  Runtime code is data.  Obliviousness
both a blessing and a curse.  COMEFROM destroys local reasoning or referential
transparency.
**** Context-Oriented Programming
Expressive separation of concerns when behavior can change depending on the
context in which the program is executed.  Composition of programs by layers.
**** Feature-Oriented Programming
Promise of high-level programming, where features are built standalone, and
interaction between them are dealt with separately.
**** Flow-based programming
No side effects?  I don’t know if this is a discipline or just a toy.  But if it
should help build modular programs, it fits.
**** Model-driven development
You build meta-models that encompass all variations of the solution space.

[[cite:HT-06][HT-06]] makes some good points about the promises and reality of MDD (in 2006).
The distinction between the three categories of sketchers, blueprinters and
model programmers in the modeling community is relevant in order to not
amalgamate different intentions.

**** Domain-specific languages
Greater control for language designer.  Gives a constrained playground for
programmers.

Downsides include tooling, development time, unfamiliarity and competition with
general-purposes languages.

Monads can be seen as DSLs (but this is an insight better saved for later).

*** Modularity of interpreters
How to build an interpreter from composing blocks.  And how this /not quite/
instrumentation, because these approaches do not consider modification to
language.

**** Expression problem
Wadler, Odersky, Krishnamurthi, Oliveira (expression families) ...

**** Building from modules
Findler & Flatt, Newspeak

**** Building with monads
Wadler, Steele, Spinoza, Swierstra, Rúnar, ...

Free algebras, free monads.  Basically reify data in a way that is accepted by
the type system of the underlying language to allow unanticipated extension.

[[cite:OC-12][OC-12]] gives Java code with generics for solving the expression problem using
/object algebras/.  Object algebras are akin to a free algebra.  Instead of
locking down the actual objects used as expressions too early, they leave them
open using abstract factories.  Providing a factory when evaluating the
expression gives you either integer evaluation, or pretty-printing.

Their solution is applicable to Java with generics, without significant
syntactic overhead (less than related work).  And, they leverage the type system
to capture erroneous composition.

**** Stratego/Spoofax
Peter Moses.

Gives you an interpreter and tool support (IDE, syntax highlighting) from just a
grammar.  What about composing languages?  Spin-offs languages (instrumentation)?

**** Partial evaluation
Partial evaluation is the partial application of an interpreter to a program.
If some input to the program is known before runtime, then one can specialize
the program to this input.  The result is a semantically-equivalent program
which accepts the rest of the (dynamic) input.

Exactly like the partial application of a function:

: interp(a, b, c, ...)(a,b) = interp’(c, ...)

[[cite:Fut-99][Fut-99]] describes how to apply partial evaluation to a interpreter, and by
successive application of partial evaluation, how to obtain a compiler
generator (a point made clearer in [[cite:FNT-91][FNT-91]]).

#+BEGIN_EXAMPLE
interpret(program, input) = \alpha(interpret, program)(input)
                          = \alpha(\alpha, interpret)(program)(input)
                          = \alpha(\alpha, \alpha)(interpret)(program)(input)
#+END_EXAMPLE

If ‘interpret’ is an interpreter, then the partial evaluation (\alpha) of this
interpreter for a known program yields a specialized interpreter \alpha(interpret,
program).  Applying the partial evaluation again, we obtain \alpha(\alpha, interpret),
a specialized compiler of the programs of the language targeted by ‘interpret’.
Then, ‘currying’ along, we obtain \alpha(\alpha, \alpha), a compiler generator.

This can be understood again by looking at the types:

#+BEGIN_EXAMPLE
interpret :: (Program, Input) -> Computation
\alpha(interpret, program) :: Input -> Computation
\alpha(\alpha, interpret) :: Program -> Input -> Computation
\alpha(\alpha, \alpha) :: ((Program, Input) -> Computation) -> Program -> Input -> Computation
#+END_EXAMPLE

Each projection yields a more abstract function, up to the last one which can
take an interpret as input.

We can derive the type of \alpha by line 2:

#+BEGIN_EXAMPLE
\alpha(interpret, program) :: Input -> Computation
\alpha :: (((Program, Input) -> Computation), Program) -> Input -> Computation
#+END_EXAMPLE

and by abstracting the types, we obtain the signature of the polymorphic \alpha:

: \alpha :: (((a, b) -> c), a) -> b -> c

Applying the projections to the polymorphic variant yields:

#+BEGIN_EXAMPLE
f       :: (a, b) -> c
\alpha(f, a) :: b -> c
\alpha(\alpha, f) :: a -> b -> c
\alpha(\alpha, \alpha) :: ((a, b) -> c) -> a -> b -> c
#+END_EXAMPLE

The signature of the compiler-compiler is the same as that of the well-known
currying function!  Seems there is an evident relation between partial
evaluation and currying in functional programming.  Laziness might even give you
partial evaluation for free.

Work by Futamura is light in technical details of the practicality of these
projections, especially the higher-order compiler-compiler.  [[cite:JGS-93][JGS-93]] has examples
of practical partial evaluators for subsets of Scheme and C.

A basic example of partial evaluation is the compiler optimization known as
constant-folding.  Partial evaluation is generalized constant folding.

The advantage of partially evaluating a program is a potential gain in
performance: operations evaluated at partial evaluation time need not be done at
runtime.  However, there are two immediate troubles: termination is undecidable,
and side-effects need to happen at runtime.

Since partial evaluation operates with partial knowledge of the input, it must
make assumptions for the unknown inputs.  This leads to a potential static
analysis of branches, which can degenerate into infinite loops.  Knowing in
advance if the program will terminate its partial evaluation is a variant of the
halting problem.  However, I suppose there is always the option to bail out of
the partial evaluation process, trading potential optimization benefit for
bounded-time partial evaluation.

Side-effects, like printing to the console or writing to disk, cannot be
generated from the partial interpreter: they need to happen at runtime.  I
suppose, here again, that side-effects can be left in place.  But getting values
from the outside world probably means that the partial interpreter cannot
evaluate further.

[[cite:JGS-93][JGS-93]] makes the point that partial evaluation allows one to write
highly-parametrized programs, while retaining efficiency.  The modular program
is specialized by the partial evaluator, and you get an efficient equivalent
program.

The remark that interpreters are usually smaller and easier to write.  They
ascribe that to the simplicity of the former: interpreters only deal with
execution time (execution of the target program is the same happens at the
execution of the interpreter), with only one language, and they provide (or
match) an operational semantics.  But compilers are efficient.

*** Instrumentation of interpreters
**** Bytecode instrumentation
Ansaloni.  Targets bytecode, which is low-level code.

Jinliner [[cite:TSN+02][TSN+02]] can insert code into the bytecode of a Java program.  Allows to
alter the behavior of a program with no access to its source code.  Inserts code
after/before point of interest.


[[cite:BRG+14][BRG+14]] instruments the bytecode interpreter of WebKit to enable information flow
tracking.  Bytecode instrumentation is difficult, because you lose high-level
details of the source code like “when does an if block ends”.  They have to
build a control-flow graph to know when to discard program counters used by the
information flow analysis.  Also, instrumenting the bytecode is specific to the
bytecode compiler of WebKit (there is no standard, unlike Java).

**** Using aspects
FlowR.  Not much insight regarding the structure; artifact not available.

*** Extensibility
Emacs and Smalltalk are two examples of dynamic environments that can be
extended by the user, incrementally and continuously.  They both achieve that by
exposing code to the user.

The languages used are different: Lisp with dynamic scoping, Smalltalk with
object-orientation.

Extensions in Eclipse and web browsers are fundamentally different as they are
not written in only one language, but several (description in XML, views in XUL,
logic in Java/JavaScript).  They are much more cumbersome to write, without
better guarantees.

**** Open Implementation                                        :methodology:
Before AOP, there was the concept of Open Implementation [[cite:Rao-91][Rao-91]] [[cite:Kic-96][Kic-96]] [[cite:MLM+97][MLM+97]]
[[cite:KLL+97][KLL+97]].

[[cite:Rao-91][Rao-91]] introduces the concept of a system with /open implementation/, which has
two interfaces: the base level interface and the metalevel interface that
reveals parts of the implementation of the base level.  They use reflection to
customize the behavior of a window system for writing a spreadsheet.  They find
that OO languages have advantages:
1. Object-centered specification closely maps the domain (here, a window
   system).
2. Polymorphism allows multiple implementation to coexist.
3. Inheritance allows reuse and differential programming.

Reflection is only one mechanism, that may not be optimal for clients of the
meta level interface (can be complex).  They believe in a more declarative
approach to meta level interfaces.

#+BEGIN_QUOTE
An Open Implementation of a software module exposes facets of its internal
operation to client control in a principled way.  They key assumption behind
Open Implementation is that software modules can be more reusable if they can be
designed to accommodate a range of implementation strategies.  Since no
implementation strategy is adequate for all clients, the module should support
several implementation strategies and allow clients to help select the strategy
actually used.
#+END_QUOTE
[[cite:MLM+97][MLM+97]]

The first sentence does not give the full picture.  Open Implementation is not
just about exposing an alternate interface.  The primary concern is to allow
client code to select different implementation strategies (to answer different
performance needs, for instance).

Metaobject protocols [[cite:KRB-91][KRB-91]] are given as an example of open implementation, for
object-oriented systems.

#+BEGIN_QUOTE
The goals of any Open Implementation are to ensure that suitable implementation
strategies are available for a range of clients, to ensure that the appropriate
strategy may be selected for or by a client, and to ensure that the benefits
associated with black-box abstraction are not unreasonably compromised.
#+END_QUOTE
[[cite:MLM+97][MLM+97]]

One key tenet of OI is “give control to the client in a disciplined way”.  That
means, some structure should be in place, otherwise the client is free to mess
with the implementation in any way.

#+BEGIN_QUOTE
Whereas black-box modules hide all aspects of their implementation, open
implementation modules allow clients some control over selection of their
implementation strategy, while still hiding many true details of their
implementation.
#+END_QUOTE
[[cite:KLL+97][KLL+97]]

The paper is broad: it considers what solution should a module implementer chose
for open implementation, depending on the client requirements.  It provides a
methodology for designing an open module.

They define 4 styles of open interface:
1. Client has no control: the module adapts its implementation by observing the
   client.
2. Client declares its usage pattern, module selects a strategy.
3. Client specifies the strategy among the predefined ones.
4. Client provides the strategy.

Style 4 is the one we want for modular instrumentation.  It is also recommended
in half the cases they consider, though it “might be difficult to engineer”.
They note that style 4 cannot be used when the integrity of the module must not
be compromised.

[[cite:KLL+97][KLL+97]] describes the four styles further.  Style 4 subsumes styles 1 and 3 (and
could be adapted to style 2), and is said to be /layered/, in the sense that
clients can choose the style better suited for their needs.

#+BEGIN_QUOTE
When there is a simple interface that can describe strategies that will satisfy
a significant fraction of clients, but it is impractical to accommodate all
important strategies in that interface, then the interfaces should be layered.
#+END_QUOTE
[[cite:KLL+97][KLL+97]]

From the set of client requirements, the module implementer should refine the
open interface in stages, until all requirements can be expressed.

***** Mechanisms for open implementation
Sadly, the language mechanisms for open implementation are not covered.

#+BEGIN_QUOTE
While the implementation techniques that support theses interfaces are crucial,
they are beyond the scope of this paper.  [footnote:] Many of the implementation
techniques are straightforward, and will be apparent simply from looking at the
interface design.  Others are more subtle, and involve recently developed
techniques in language and system implementation [[cite:KRB-91][KRB-91]] [[cite:CU-91][CU-91]] [[cite:Chi-95][Chi-95]].  There is,
as yet, no unified presentation of these techniques; a separate paper describing
this is in preparation.
#+END_QUOTE
[[cite:KLL+97][KLL+97]]

Could not find a trace of this paper in preparation.

The Strategy pattern comes to mind [[cite:GHJ+94][GHJ+94]] (though they actually cite [[cite:HO-87][HO-87]] for
the specific case of selecting algorithms with different space/time trade-offs).

Open Module [[cite:Ald-05][Ald-05]] does not mention Open Implementation, although they certainly
fit the description of style 4.

[[Reflection]] is another mechanism.

**** Aspect-Oriented Programming                       :methodology:language:
Did the initial vision of AOP covered the problem of extensibility?

[[cite:KLM+97][KLM+97]]
Motivation of AOP is a better match between design processes and programming
language mechanisms.

#+BEGIN_QUOTE
A design process and a programming language work well together when the
programming language provides abstraction and composition mechanisms that
cleanly support the kinds of units the design process breaks the system into.
#+END_QUOTE

OO languages, procedural languages, functional languages all provide a
/generalized procedure/ as key abstraction mechanism.  Design processes for a GP
language decompose systems into units of behavior.

First example of tangling: an efficient image filter system.  A filter loops on
all the pixels of the input image, and produces a new image.  Higher-level
filters (‘horizontal-edge’) are defined by composing lower-level ones (‘or’,
‘and’).  This is elegant, but inefficient as temporary images are created and
deleted, and many loops are made where only one sufficed.

The alternate solution is to code the higher-level filters explicitly with only
one loop.  Then the code is tangled.  Their actual system is 768 lines when
implemented “cleanly”, but the efficient version is 35213 lines.

The language only supports one kind of composition, the functional one, while
there is a need to also specify the fusion of loops, which is a composition of
data flow.

They distinguish /components/ from /aspects/:
- Components :: encapsulate cleanly a feature through a “generalized procedure”
                (object, method, procedure, API).  Components tend to be the
                unit of functional decomposition of the system.
- Aspects :: for features that cannot be cleanly encapsulated through a
             generalized procedure.  Aspects tend to be features orthogonal to
             the functionality of the system: data representation,
             synchronization constraints ...

The goal of AOP is to provide mechanisms to cleanly separate components from
aspects, components from components, and aspects from aspects.  GP languages
only provide mechanisms to separate components from each other.

They feel that dynamic scoping and catch/throw already help implementing
/aspects/, since they provide a complementary composition mechanism.

Error handling and performance issues are often aspects because they cross-cut
the components decomposition.

An AOP implementation has: a component language, an aspect language, and an
aspect weaver.  For example, in AspectJ the component language is Java, the
aspect language is the pointcuts/advice language provided by AspectJ.  But the
component language does not have to be a vanilla language — it can be a specific
one.

In the image filter example, the component language is procedural and allows
high-level filters to be defined cleanly, using a DSL for describing loops.  The
aspect language is also procedural, but allows to specify loop fusion.  The
weaver then creates a data-flow graph from the components, runs aspects on them,
and produces efficient C code.

They significantly improved the performance of the clean version by adding 352
lines of aspects (not counting the size of the weaver).  Though the manually
optimized version is still more efficient.

#+BEGIN_QUOTE
[...] the aspect languages must address different issues than the component
languages.
#+END_QUOTE

The second example is a book repository.  The component language is (a subset
of) Java, and the aspect language is a meta-program which captures method
invocation using compile-time reflective techniques.

[[Reflection]] can be used to write aspects, but may be too powerful a tool (hence,
a costly one).  A reflective system provides a component language and a
low-level aspect language, as well as the weaving mechanism.  The reflected
structures provide join points.  Reflective systems are general-purpose, and in
the paper they aim for more declarative aspects.

AspectJ is more limited than reflection, but still general.  Domain-specific
aspect languages are recommended to write aspects while retaining static
control.

#+BEGIN_QUOTE
AOP is a goal, for which reflection is a powerful tool.
#+END_QUOTE

[[cite:KHH+01][KHH+01]] presents the AspectJ AOP system.  It contains an intuitive footnote about
the separation of concerns.

#+BEGIN_QUOTE
When we say “separation of concerns” we mean the idea that it should be possible
to work with the design or implementation of a system in the natural units of
concern – concept, goal, team structure etc. – rather than in units imposed on
us by the tools we are using.  We would like the modularity of a system to
reflect the way “we want to think about it” rather than the way the language or
other tools force us to think about it.  In software, Parnas is generally
credited with this idea [[cite:Par-72][Par-72]] [[cite:Par-74][Par-74]].
#+END_QUOTE

The shift from domain-specific to general-purpose AOP is motivated by a desire
for adoption: providing an alternative paradigm for all Java programmers.

#+BEGIN_QUOTE
AspectJ is intended to be a practical AOP language that provides, in a Java
compatible package, a solid and well-worked-out set of AOP features.
#+END_QUOTE

They describe the joinpoints, pointcuts, and advice of AspectJ, as well as
the rules of advice precedence, and sketch the compilation strategy.

Advice declarations in AspectJ, through CLOS [[cite:KRB-91][KRB-91]], owe much to Flavors
[[cite:Can-03][Can-03]].

[[cite:MK-03][MK-03]] provides models and scheme implementations of four AOP systems; the
Pointcuts-Advice model for AspectJ in particular.

[[cite:FF-04][FF-04]] wants to answer the question “when are we looking at an AOP system?”.
They find two essential traits of AOP systems: quantification and obliviousness.

They describe AOP as the desire to make statements of the form

: In programs P, whenever condition C arises, perform action A.

suggesting three axes of choices for AOP systems:
1. What kinds of conditions can we specify? (Quantification)
2. How do actions interact with programs and with each other? (Interface)
3. How will the system mix the execution of programs and actions? (Weaving)

For quantification, they distinguish between static (conditions on the source
code structure) and dynamic (conditions on the runtime behavior).  Furthermore,
black-box systems quantify over the public interface of components (e.g.,
functions or object methods), and clear-box systems quantify over the internal
structure of the code (AST).

They note that rule-based systems (Prolog, OPS-5 [[cite:BFK+85][BFK+85]]) would not need AOP.
However

#+BEGIN_QUOTE
But by and large, people don’t program with rule-based systems.  This is because
rule-based systems are notoriously difficult to code.  They’ve destroyed the
fundamental sequentiality of almost everything.  The sequential, local, unitary
style is really very good for expressing most things.  The cleverness of
classical AOP is augmenting conventional sequentiality with quantification,
rather than supplanting it wholesale.
#+END_QUOTE

The paper has an interesting stance on the evolution of programming languages
with respect to /local/ and /unitary/ statements (\sect2.2).

#+BEGIN_QUOTE
The earliest computer machine-language programs had a strict correspondence
between the program text and the execution pattern.  Generally, each programming
language statement was both /unitary/ and /local/ — unitary in that it ended up
having effect in precisely /one/ place in the elaborated program, and local in
that it was almost always proximate to the statements executing around it.
#+END_QUOTE

They point out that adding code to a base class that has multiple subclasses is
a form of quantification.

[[cite:Ste-06][Ste-06]] questions the success of AOP by opposing the AOP vision to the actual
mechanisms provided.  Quoting [[cite:FF-04][FF-04]]:

#+BEGIN_QUOTE
Understanding something involves both understanding how it works (mechanism) and
what it’s good for (methodology).  In computer science, we’re rarely shy about
grandiose methodological claims (see, for example, the literature of AI or the
Internet).  But mechanism is important – appreciating mechanisms leads to
improved mechanisms, recognition of commonalities and isomorphisms, and plain
old clarity about what’s actually happening.
#+END_QUOTE

AOP has the issue of /fragile pointcuts/: sensitive to changes in the target
program.

AOP is detrimental to Parnas’s notion of modularity because of the strong
coupling between an aspect and the target program.  Independent development
cannot continue.

Interestingly, Parnas considers modularity as a design issue, not a language
one.  Confusing the two is harmful: using the module functionality of a language
does not mean the system is modular in the sense meant by Parnas.  Each task is
a single module with a clear interface, and implementation-specific information
is not shared across modules.

They suggest that AOP use should be restricted to applications where programmers
do not have to see it; e.g., generated code.  They do not regard AOP as
a “new paradigm”, especially they do not find convincing applications for it.

AOP promotes the localization of concerns (bringing tangled code in one place),
but this actually breaks the locality of code (executed statements are not
together in the source code).

I find strange that a critique of AOP does not even mention the original AOP
paper [[cite:KLM+97][KLM+97]].  This critique is focused on the AOP mechanism as realized by
AspectJ, mostly.  But the original paper focused on domain-specific aspect
languages, which /hid/ the weaver, joinpoints and pointcuts.  The original
contribution was also in formulating the goal of separating components from
aspects.  AspectJ is just one way to achieve this goal, but it might not be best
one, depending on the domain.

Overall, it is a critique of one mechanism for AOP, rather than a critique of
the methodology (separating aspects from components).

Aspects and monads are sometimes both viewed as mechanisms to achieve modularity
in software [[cite:DBB+03][DBB+03]] [[cite:HO-07][HO-07]] [[cite:Meu-97][Meu-97]].

AOP is [[https://encrypted.google.com/patents/US6467086][patented]] since 2002 by XEROX (US6467086 B1).

***** Mechanisms for instrumentation
The distinction between aspects and components is the most important
contribution of AOP.  Though it is unclear whether ‘aspects’ are inevitable
because of the complexity of the problem domain, or if they are accidental
artifacts created by the chosen programming model (like most design patterns are
motivated by the lack of first-class functions).

Java + AspectJ is only one aspect system: useful for tracing, logging, but
cumbersome for more specific needs.  The pointcuts/advice model is the
underlying formalism of AspectJ, but not necessarily of the AOP methodology.

Like Parnas’s modules, aspects are a design-time issue.  Solving the module
issues with language mechanisms was, according to Parnas [[cite:DBB+03][DBB+03]], a mistake.
Maybe the same can be said of aspects.

Is AOP useful for the instrumentation problem?  First, the initial use case of
AOP, like open implementation, is tangential concerns: algorithmic complexity,
choice of data representation, optimizations, etc.  Post-hoc extension is not
exactly a tangential concern: changing the behavior of the interpreter is a
primary concern.

Second, we have to consider separately the usefulness of the AOP methodology,
and of the AOP mechanisms.

The methodology of separating components from aspects is applicable if our
analyses are tangential.  They are not.  The problem we deal with is that
extensibility was not considered when designing the interpreter, and solutions
must be built on the implementation.

Preserving locality is a guiding tenet of the AOP methodology (avoiding
tangling).  It is also a motivation for writing modular analyses: we want the
analysis code to be in one place.  However, by regrouping the analysis code, we
are sacrificing locality of code execution: statements executed at runtime are
not next to each other in the source code.  Satisfying both notions of locality
would lead to duplication in the code, which is a worse state of affairs.
Solutions to this duplication must come from the tools used to write and browse
code, since the textual format we use offer none.  An editor can maintain two
views of the same unit of code: changes in one view will affect both places.
That way, both notions of locality can coexist.

The second notion of locality, the one from [[cite:FF-04][FF-04]], is one manifestation of the
more general need of a match between runtime behavior and static program
description.  The program source should tell readers what it does, and
navigating through dynamically-bound method calls and oblivious advices hinders
the reading.

Organization of the code should reflect the design decisions: what is primary is
explained first, then exceptions or tangential concerns are relegated to
appendices.  Literate programming [[cite:Knu-84][Knu-84]] can help organize the code in a such
way.

The mechanisms of AOP may serve to extend the interpreter with analyses, without
necessarily obeying the component/aspect decomposition.  Though without editor
support, using AOP mechanisms will only satisfy one notion of locality.

**** Emacs                                                         :language:
See Emacs Manual, [[cite:Sta-81][Sta-81]], [[cite:Hal-88][Hal-88]].  Emacs is an example of an extensible system.
The mechanisms: global namespace, dynamic scoping, and a simple aspect system.

In [[cite:Sta-81][Sta-81]], it is said that the TECO language was instrumental for the
extensibility of the EMACS system.  An interpreter should be available all the
time, and compiled languages often lack this functionality.

#+BEGIN_QUOTE
A system written in PL/I or PASCAL can be modified and recompiled, but such an
extension becomes a separate version of the entire program. The user must
choose, before invoking the program, which version he wants. Combining two
independent extensions requires comparing and merging the source files.  These
obstacles usually suffice to discourage all extension.
#+END_QUOTE

Especially they list “Language features for extensibility”:
1. Global variables.  They can be queried, referred to, and redefined.
2. [[Dynamic binding]].  Useful for redefining binding on the fly.
3. File-local variables.  Good for customization, but really they give a
   file-local value for a global variable.
4. Hooks.  They give points in the control flow to insert extension code.
   Especially when redefining assembly or C functions, which cannot be
   reinterpreted.
5. Error handling.  Throwing the debugger helps discover and recover from
   unexpected situations.
6. Non-local transfers.  Gives an example to exit an infinite loop.


In the related work, Multics EMACS [[cite:Gre-80][Gre-80]] is mentioned as being more flexible,
as it is written in MacLisp directly.  [[Smalltalk]] [[cite:Ing-78][Ing-78]] is also said to be
“oriented toward writing extensible programs”.

(The Augment editor demoed by Engelbart [[cite:EE-68][EE-68]] is also mentioned, though nothing
is said of its extensibility.)

[[cite:NS-01][NS-01]] proposes a dynamic scope analysis, to translate Emacs Lisp code using
dynamic binding to lexical binding.

*****  Mechanisms for extension
Global variables, dynamic binding, hooks.

Though hooks are more a convention than a first-class mechanism.

**** Dynamic binding                                               :language:
Introduced by McCarthy’s LISP [[cite:McC-60][McC-60]] as a bug.  Can be emulated by passing a
dynamic environment in lexical binding [[cite:Que-03][Que-03]].

Implicit parameters [[cite:LLM+00][LLM+00]] provide dynamic scoping for Haskell (though they lose
their first-class privileges).

[[cite:Mor-98][Mor-98]] gives a syntactic theory of dynamic binding, and prove that dynamic
binding adds expressiveness to a purely functional language.  They give examples
in Perl, TeX, Common Lisp and Bash.

[[cite:Tan-09a][Tan-09a]] generalizes dynamic and static binding by making explicit the two
dimensions of propagation of bindings (call stack and delayed lambdas), and
offering a filter function to toggle the activation of a propagated binding.

Some use-cases are mentioned, but none are demonstrated in the paper.  The
proposal is not motivated enough by concrete applications that would be
difficult to solve using existing mechanisms.  Also, the work is really focused
on the binding semantics of Scheme, which reduce its applicability.

**** Smalltalk                                                     :language:
Design and Implementation [[cite:Ing-78][Ing-78]].

Opens with a definition of modularity:
#+BEGIN_QUOTE
No part of a complex system should depend on the internal details of any other
part.

[...]

Objects are created and manipulated by sending messages.  The communication
metaphor supports the principle of modularity, since any attempt to examine or
alter the state of an object is sent as a message to that object, and the sender
need never know about internal representation.

[...]

The class is the natural unit of modularity, as it describes all the external
messages understood by its instances, as well as all the internal details about
methods for computing responses to messages and representation of data in the
instances.
#+END_QUOTE

Smalltalk is designed with modularity, as classes encapsulate object
descriptions and methods, and can only interact through messages.

An example of extending the system: adding new objects and a printer for them.
Similar to the [[Expression problem][expression problem]].

#+BEGIN_QUOTE
Adding a new class of data to a programming system is soon followed by the need
to print objects of that class. In many extensible languages, this can be a
difficult task at a time when things should be easy.  One is faced with having
to edit the system print routine which (a) is difficult to understand because it
is full of details about the rest of the system, (b) was written by someone else
and may even be in another language, and (c) will blow the system to bits if you
make one false move.  Fear of this often leads to writing a separate print
routine with a different name which then must be remembered.

In our object-oriented system, on the other hand, printing is always effected by
sending the message =printon: s= (where s is a character stream) to the object
in question.  Therefore the only place where code is needed is right in the new
class description.  If the new code should fail, there is no problem; the
existing system is unmodified, and can continue to provide support.
#+END_QUOTE

Changing a field inside =Rectangle= does not need to change code external to the
object, and global recompilation is avoided.

Additional story on the vision of Smalltalk can be found in [[cite:Kay-93][Kay-93]]; a larger
perspective is given in [[cite:Mul-15][Mul-15]].

***** Mechanisms for extension
Subclassing, and reflection.  Everything as an object, so message dispatch is
just a method on the meta class, and can be altered.

**** Self                                                          :language:
The power of simplicity [[cite:US-91][US-91]].

Pure object-oriented language.  No variables, but slots containing objects that
return themselves.

No classes.  No control structure.

The absence of distinction may not be a good thing in practice:
#+BEGIN_QUOTE
The absence of class-instance distinction may make it too hard to understand
which objects exist solely to provide shared information for other objects.
Perhaps SELF programmers will create entirely new organizational structures.  In
any case, SELF’s flexibility poses a challenge to the programming environment;
it will have to include navigational and descriptive aids.

[later, in the conclusion]

Reducing the number of basic concepts in a language can make the language easier
to explain, understand, and use.  However, there is a tension between making the
language simpler and making the organization of a system manifest.  As the
variety of constructs decreases, so does the variety of linguistic clues to a
system’s structure.
#+END_QUOTE

They cite [[cite:UCC+91][UCC+91]] for pointers on structuring programs in SELF.

Classes are abstract description of objects, but prototypes are always
concrete.  Each object is an example, and can be easily cloned.  Class
hierarchies are hard, and impose a structure; prototypes less so.

Classes forces you to create a template, even when you deal with several objects
with unique behavior.

Activation records for methods inherit from the receiver object, so the receiver
is on the chain for binding lookup.

They note that they could build “class-like” objects that hold code to create
new clones, and also hold the shared behavior, though they “do not believe this
is the best way to construct a system”.

In [[cite:UCC+91][UCC+91]], the following organization is described:
- Traits object for methods (shared by all instances of an object).
- A prototype object with a default implementation.
- Instances are created from cloning the prototype.

Abstract objects dispense of the prototype, and singleton objects contain
methods and state without providing a copy method.

They note that OO supports “differential programming”, which is to define new
data types as differences from existing data types.  In Smalltalk, differential
programming is achieved through subclassing.  In SELF, they call it “refining
traits objects”, but the mechanism is delegation through the parent link.

An oddity: they state that parent links are constant, though the introduce a
=dataParent= setter in figure 3.  Later they say that parent slot are like other
data slots, assignable.

Prototypes allow for multiple behavior modes, through dynamic inheritance
switching.  Behavior modes enhance the clarity of the code, though they do not
comment on the potential performance costs.

***** Mechanisms for extension
Prototypes, and message passing.  Prototypes are more general and simpler than
inheritance.

However prototypes do not originate with SELF [[cite:Bor-86][Bor-86]] [[cite:Lie-86][Lie-86]].

[[cite:Lie-86][Lie-86]] makes a good case for prototypes as being a simpler model to learn, as
well as being more intuitive.  Humans derive general concepts from examples, not
the other way around.  Class-based languages require you to commit to the
concepts first.

Prototype-based and class-based languages provide different mechanisms for
realizing differential programming,

Is differential programming sufficient to solve the problem of modular
instrumentation?  In the case of Narcissus, it was not, since the interpreter
was not OO.  But the open scope pattern might be equivalent, dynamically, to
inheritance.

**** Prototype-based programming                                   :language:
A collection of (at least) the following papers (or revisions of them):
- [[cite:Tai-97][Tai-97]]
- [[cite:DMB-98a][DMB-98a]] (mostly a translation of [[cite:DMB-98][DMB-98]])
- [[cite:GBO+98][GBO+98]]
- [[cite:Bor-86][Bor-86]]
- [[cite:SU-95][SU-95]]
- [[cite:Smi-95][Smi-95]]
- [[cite:MMM+98][MMM+98]]
- [[cite:Bla-91][Bla-91]] & [[cite:Bla-94][Bla-94]]
- [[cite:Wol-96][Wol-96]]
- [[cite:Moo-96][Moo-96]]
- [[cite:Nob-01][Nob-01]]
- [[cite:DeM-98][DeM-98]]

[[cite:Tai-97][Tai-97]] is a philosophical take on the basis for class-based and prototype-based
languages.  Ascribing to classes is following the school of Plato and Aristotle
[[cite:Pla-98][Pla-98]] [[cite:Ari-35][Ari-35]].  Plato distinguished between /forms/, the ideal description of
things, and /instances/ of these forms.  He regarded forms as being more real
than instances.  Aristotle believed in “a single correct taxonomy of all natural
things”, and classified things using the following rule:
: essence = genus + differential
which mirrors class creation in class-based languages.

Classification has been criticized, notably by Wittgenstein [[cite:Wit-53][Wit-53]], as being
subjective.  Some concepts are difficult to define by intension – through a list
of common properties that all instances must share.  Rather, Wittgenstein
proposes the notion of /family resemblance/.  Meaning is not determined by a
definition, but by similarity to representative prototypes.

This philosophical heritage has a few implications for programming:
- there are no optimal class hierarchies
- in a class hierarchy, the middle classes are often the best representatives.
  Higher classes are too abstract; lower classes too specific.
- prototypes may map better to the usual human process: iterate from examples.

Designers of class-based or prototype-based languages are seldom aware of the
philosophical issues of both models, but focus more on technical matters.  Kevo
[[cite:Tai-93][Tai-93]] is a prototype-based language with a notion of family
resemblance. [[cite:Tai-93b][Tai-93b]] offer similar insights on the notion of object.

[[cite:DMB-98][DMB-98]] tries to classify prototype-based languages (ironically).  Prototype-base
d languages are advantageous for describing exceptional instances, multiple
points of view of the same entity, and incomplete objects.

They identify the following mechanisms common to prototype-based languages:
- message passing
- 3 ways of creating objects (ex nihilo, cloning, and extension)
- delegation
- dynamic dispatch

Prototype-based languages also introduce new issues:
- Fragmented entities.  Since objects are described differentially, no single
  object in the system reify the complete entity.  To clone it completely, we
  would need to clone all its parts, but they are not reified (e.g., traits
  objects are only conventions, not language primitives).
- Sharing between clones of the same object.
- Sharing between clones of different objects.

**** Eclipse and other IDEs                                            :tool:
As noted by [[cite:Ler-11][Ler-11]], the Eclipse platform is extensible, and built using
plugins.  Each plugin states its dependencies (the hooks needed to function),
and its extension points (for other plugins).

Eclipse plugins are compiled, though they can be loaded dynamically (if they are
written properly).  Symptomatically of Java, writing plugins needs lot of
boilerplate code and XML (which Eclipse can generate for you, I understand).

***** Mechanisms for extension
The mechanisms for extension seems to revolve around the observer pattern: a
host plugin raises events which can be intercepted by extensions [[cite:Bol-03][Bol-03]].

So, a lot of convention.

**** Web browsers                                                      :tool:
Many extensions are written for web browsers.  The mechanisms are heavy,
comparable to the effort of writing an Eclipse plug-in.

In fact, ZaphodFacets was an extension to change the JavaScript interpreter used
by the browser.

***** Mechanisms for extension
Convention.  Write manifest, and define the agreed-upon functions (install,
startup).

**** Lua                                                           :language:
An extensible extension language [[cite:IFF-96][IFF-96]].

Extensible systems comprise of a kernel and a configuration.  The kernel is the
core of the system, the parts that cannot change, and is usually compiled for
speed and efficiency.  The configuration part is written in an interpreted,
flexible language, which can interact with the kernel.

Another take, in the conclusion, is that the kernel is a virtual machine for
programs written in the configuration language.

Note that if performance can suffer, writing the whole system as a configuration
gives even greater flexibility.

Configuration languages can be simple: .ini files, X11 resource files, but they
can have more features (scripting languages).  Also called /extension
languages/.

Five requirements for extension languages:
1. good data structures (key-value maps for configuration)
2. simple syntax for amateur programmers
3. lightweight
4. not static type checking or exception handling, as only small programs are
   written in them
5. should be extensible

Requirement 4 is actually an absence of requirement.  Unfortunately, people
/will/ write large systems in it, especially if the language is easy to pick up.
Arguably, the cost of such features may conflict with requirement 3.  Otherwise,
this list looks more like a checklist for Lua.

On a related note, [[cite:Bla-82][Bla-82]] devotes a whole thesis against exceptions.

Extension programs have no =main=.

Associative arrays are a powerful data structure which make plenty of algorithms
trivial (free hashtables), and more efficient to implement than lists.

Amusingly, the associative array syntax was inspired by BibTeX.

Associative arrays + first-class functions = classes.

No error handling, but errors can be raised.  To catch them, we can define
/fallback/ functions.

Setting a fallback on the “index” event allows to define a custom delegation
mechanism between tables.

Compared to Lisp, Lua is portable and has easier syntax.  Tcl is slow and has
strange syntax.  Python is not embeddable, and is already too complex (modules
and exception handling).

At the time of writing, Lua is 20 times slower than C (this factor is said to be
“typical for interpreted languages”, and cites “Java, The Language” for this
assertion).

The latest numbers on the [[http://benchmarksgame.alioth.debian.org][benchmarks game]] show Lua being 5 to 79 times slower,
while consuming more memory.

In the conclusion, they allude at extending web browsers with Lua.  A follow-up
seems to be [[cite:HBI-98][HBI-98]], which proposes Lua as a target for CGI on web servers.

[[cite:IFF-07][IFF-07]] goes over the history of Lua, up to version 5.1 released in 2006.

One tenet of Lua is “Mechanisms, not policy”: provide language mechanisms and
let programmers code the way they want to with them.  An example is message
dispatch: rather than using a class construct, Lua programmers can use fallbacks.

Though they regret not stating a policy when it comes to modules, since everyone
is doing its thing, without agreeing on a common protocol.

*****  Mechanisms for extension
The kernel+configuration, as seen in EMACS.  Mechanisms over policy shares our
philosophy and provides programmers with tools to solve their problems in their
own way.

To extend Lua, bindings from C can be added, and custom data structure as well.
Changing the interpreter does not seem possible, even from C.

**** Scripting languages                                           :language:
Tcl 1988, Python 1991, Lua 1993, VBA 1993, JS 1995.  Scripting languages are an
early ‘90s phenomenon.  Dealing with low-level languages was deemed too heavy,
but writing your whole system in a high-level language was too costly.  The
compromise was to write the kernel in C, and the rest in a scripting language.

With sufficiently efficient high-level languages, the kernel+configuration
approach might be unneeded.

JavaScript being a scripting language for the browser, as well as an object used
in the [[Core]], it might be adequate to have a dedicated background section to it.

**** Reflection                                                    :language:
[[cite:Tan-09][Tan-09]] gives a nice survey of reflection and its uses.  Useful distinctions are
made between /introspection/, /introcession/, /structural reflection/, and
/behavioral reflection/; also between a program (a textual description) and a
/computational system/ (a running process described by a program).

A interesting observation on binding is quoted from [[cite:MJD-96][MJD-96]]:
#+BEGIN_QUOTE
The general trend in the evolution of programming languages has been to postpone
formal binding times towards the running of programs, but to use more and more
sophisticated analysis and implementation techniques to bring actual times back
to the earlier stages.
#+END_QUOTE
Later binding = more runtime flexibility, but also less guarantees and less
performance.  The DLS submission is a perfect example.

[[cite:DS-01][DS-01]] give a general method to reify selected parts of a meta-circular
interpreter.

[[cite:Ste-94a][Ste-94a]] studies object-oriented languages which support open implementation.
The open implementation of a language (the interpreter) is itself written in one
language called the /implementation language/, and its meta-level interface
allows the system to interpret a range of /engendered languages/.

[[cite:SW-96][SW-96]] describe three approaches to code non-functional requirements while
preserving the separation of concerns: systems-based, language-based, and
MOP-based.  They find that MOP-based solutions are more flexible, especially as
they can be applied to other domains without modifying the code.  However, they
consider non-functional requirements like persistence and atomicity.

Reflection for dynamic adaptation [[cite:DSC+99][DSC+99]].  Dynamic adaptation echoes the
motivation of open implementation: an application should adapt dynamically to
the need of the users, thereby enhancing performance.  This is mostly a concern
in systems software, operating systems and middlewares.  They use a memory
allocator example and compare using design patterns, DLLs and reflection.
Essentially, reflection is more flexible, but also less efficient.

[[cite:RC-02][RC-02]] illustrates how unanticipated dynamic adaptation can be achieved using
MOPs in Java.

Unifying AOP and OOP [[cite:RS-09a][RS-09a]].

[[cite:ADF-11][ADF-11]] proposes a proxy protocol for values.  A /virtual value/ is wrapped by a
proxy which has a handful of traps that are useful to override: when the value
is called as a function, when the value is used as a record, when the value is
used as an index in an array, when the value is used in a binary operation ...

They exhibit several scenarios where virtual values are useful: lazy evaluation,
revocable membranes, and tainting.  They modified Narcissus (again!) to add
their virtual values extension, but the implementation seems incomplete
regarding all operations available in JavaScript.

They motivate virtual values as a nice way to extend languages without having to
touch the interpreter.  Though they do not talk at all of the limitations of
this approach: can you write any extension that you would write by modifying the
interpreter with virtual values?  The only downsides they acknowledge are
performance hits and potential breakage of JS invariants (‘x*x’ returning a
negative number, or ‘x === x’ returning false).

It seems evident that virtual values are only hooks for values.  So you cannot
override any other part of the module which is not explicitly given by a trap.
Getting a trace of the interpreter execution is out.  Also, you need to specify
your analysis from the point of view of handler on values, not by altering the
interpreter semantics.

[[cite:KT-13][KT-13]] implements access control on JS objects through ES6 proxies.  Improves a
previous implementation which used code transformation; better performance, less
maintenance.

**** Software product lines                       :tool:language:methodology:
[[cite:ABK+13][ABK+13]] provides a well-rounded survey of the field.

An engineering methodology to create and maintain variants of a software
product, with optional features (analogy with car assembly lines, which allow
for adding optional features while reusing the same assembly process).

Inspired by the similar evolution in the mass production of consumer goods.
From handcrafting to mass production, to mass customization: product lines that
cover a spectrum of variations.  Examples abound: cars, multi-flavored
detergent, phones, Subway sandwiches ...  Software product lines are the
realization of mass customization for software products (yeah!).

A product line engineering platform combines all the artifacts, documentation
and methodologies of a family of products.  The goal of PLE is to manage the
/commonality/ and /variability/ of a product family.  PLE is not specific to
software.

Properties of a SPL:
- binding time (composition can happen at compile-time, load-time or run-time)
- language solution vs. tool based
- annotation (think C preprocessor) vs. composition (features in their own unit)
- preplanning effort (can you add features without designing for it?)
- feature traceability (mapping between feature model to solution space)
- separation of concerns
- information hiding
- uniformity

Software product lines mechanisms include:
- global parameters
- design patterns (observer, strategy, decorator)
- frameworks
- components

Using version control branches to manage variability is also discussed.  Each
branch correspond to a product, and code sharing is provided by the version
control tool.  However, version control manages /products/ rather than
/features/.  Features are not apparent independently of the base code, except
when looking at diffs.

Feature-oriented programming allows the decomposition of a program into features
first.  Jak is a Java extension that supports FOP [[cite:BSR-04][BSR-04]].  A feature corresponds
to a layer, and each layer can contain multiple classes that implement the
feature.  Further layers can /refine/ the classes of previous layer, and refer
to their implementation via the =original= keyword.

FeatureHouse [[cite:AKL-13][AKL-13]] is akin to [[Semantic patches]], in that it uses a reduced
syntax tree in order to transform code.  One writes a base program, then another
program can be superimposed on it by matching their reduced syntax trees.  The
base program code can be called using the =original= keyword.  Three-way merges
are also possible, and resolved like in version control systems.  The model of
reduced syntax trees of FeatureHouse is language independent, as are the
composition mechanisms.  Language plugins can be written to tell
FeatureHouse how to generate, compose, and pretty-print reduced syntax trees.

#+BEGIN_EXAMPLE
public class A {
  private int foo() { return 0; }
}

public class A {
  private int foo() { original(); return 1; }
}
#+END_EXAMPLE

FeatureHouse also supports quantification.  Mixins and traits mechanisms are
essentially instances of superimposition.

FOP is well-suited to implementing /heterogeneous concerns/ (one variation per
join point), while AOP is better for /homogeneous concerns/ (one variation,
multiple join points). [[cite:MO-04][MO-04]] illustrates the compromises of each approaches (and
presents [[Caesar]] as the superior solution).

If you cannot maintain a separation of concerns in the code itself, you can
emulate it through views.  /Virtual separation of concerns/ is using tools to
provide coherent views of features that are scattered in the code [[cite:AK-09][AK-09]].

Virtual separation of concerns has few downsides and many benefits: simplicity
and flexibility being the chief advantages.

Handling feature interactions is an open problem.  Detecting them also.

***** Mechanisms for instrumentation
FOP implementations presented here are static organization of code into
features.  Much like design patterns or frameworks, they require the programmer
to design for extensibility beforehand.  AspectJ allows extending an existing
code base (unlike the original AOP vision, which emphasized the design decision
of separating components from aspects).

The notion of superimposition is nice.  Recognizing that inheritance, mixins and
traits are all instances of superimposition is a powerful insight.

Virtual separation of concerns makes some good points.  If the primary
decomposition is tyranny, then we have no hope of organizing the physical code
into features.  However, we can leverage editing tools to re-arrange and view
the code in any way we like.  One physical representation, many views.  Each
view can provide different information about the system.

The motivation behind all such mechanisms is a desire to organize snippets of
code, to structure modules, and avoid repetitions.  The ultimate conclusion of
that trend is a language-agnostic manipulation syntax based on hypertext.  Each
snippet has a name, and tags (for marking membership of a feature, but mostly
for non-hierarchical organization).  Any snippets can be referenced by another
(for documentation), and can be included for execution.  Snippets can be
referenced to by name, or by tags.  Tags and wildcards allow quantification.

Tags also allow to view the program through different lenses.  Snippets can have
parameters, hence are a form a macros.

Links are two way, and kept in sync by the programming system (editor): this
prevents obliviousness.

**** Caesar
[[cite:AGM+06][AGM+06]]

CaesarJ regroups virtual classes, mixins, pointcut-advice and binding classes.
All these mechanisms are brought together to allow composition along many axis.

But overall, I failed to see the problems that it solved.  Both papers [[cite:AGM+06][AGM+06]]
[[cite:MO-02][MO-02]] are dense and opaque; the examples are too complicated to make sense of
the benefit brought by the new mechanisms.

**** Hyper/J                                               :methodology:tool:
[[cite:TOH+99][TOH+99]] argues for a multi-dimensional separation of concerns.  First, they note
that modern software technologies provide mechanisms for the /decomposition/ and
/composition/ of source code, in order to cut the code into manageable pieces,
and put the pieces back together to produce the running program.

Existing software formalisms provide decomposition and composition mechanisms,
but typically support a single dominant dimension of decomposition.  They dub
this phenomenon the “tyranny of the dominant decomposition”.

A class hierarchy is insufficient for anticipating all the evolutions of an
expression language (see Expression problem).  Subclassing and design patterns
require pre-planning.

There are many concerns we need to manage simultaneously, and the dominant
decomposition typically sacrifices some of those concerns for the benefit of
others.  Thus, we are in presence of a multi-dimensional artifact, and each
decomposition gives only a lower-dimensional view of said artifact.

They propose /hyperslices/ as way to organize artifacts along all desired
dimensions of concern.  An hyperslice contains all the units of change related
to one concern.  Units of change can appear in multiple hyperslices, and thus
hyperslices can overlap.  In the expression example, one slice for the kernel
language, one slice for the pretty-printing, one slice for syntax checking, etc.

Composition of hyperslices must be specified manually, though a default strategy
can be installed.  They suggest one strategy based on name matching for merging
classes together (akin to superimposition).

Throughout the paper, they only use hyperslices on UML diagrams, not source
code.  Hyperslices can be applied to specification, design documents and code.
Though they do not highlight a way to link the related parts from those
different artifacts together, other than putting them in the same hyperslice.
There does not seem to be a way to deal with duplicates.

Compared to AOP, where components are the primary decomposition and aspects
gravitate around them, hyperslices do not impose a dominant structure (though it
may often appear in practice, e.g., the kernel slice of the expression
language).

There are no descriptive papers of Hyper/J, but there is a manual [[cite:TO-00][TO-00]].  The
manual gives details on how to implement the example of expression language.
Using Hyper/J requires to write three files describing: the hyperspace (all
classes that Hyper/J will care about), the concern mappings (which
package/class/method/field maps to which concern), and the hypermodules (which
features are part of a module, and how composition happens).  Running the
Hyper/J tool will compose all the hypermodules using the specified rules (merge
by name) to produce the final program.

Hyper/J simplifies the multi-dimensional concept by mapping units of change to
exactly one feature.  No overlap between hyperslices.

***** Mechanisms for instrumentation
Realization that the tyranny of the dominant decomposition is a manifestation of
looking at a multi-dimensional object through low-dimensional projections.  All
projections are unsatisfactory as they sacrifice one or more dimensions.

The Hyper/J solution is basically superimposition.

**** Others?
***** Software evolution through runtime method transformation
[[cite:Zdu-04][Zdu-04]]

***** Direct manipulation
Beyond programming languages [[cite:Shn-83][Shn-83]].  Argues that computer applications are
better learned and used if the user can directly manipulate the high-level
domain objects.

Users have semantic knowledge of how to solve a task, how to decompose it into
smaller, lower-level semantic goals (copy a sentence = move cursor to sentence
start, select until sentence end, issue copy command).  Semantic knowledge is
mostly application-independent (most text editors support the previous semantic
actions).  Syntactic knowledge comprises the actual commands needed to effect a
low-level semantic action; syntactic knowledge is application-specific, and
mostly arbitrary (‘K’ will copy text in one editor, but delete in another).

Novices begin with zero semantic knowledge, and must build it upward, from
low-level actions to high-level domain planning.  They build this knowledge by
learning the syntax of the application: the commands it provides.  At first,
they can only solve tasks that are straight applications of syntactic
knowledge.  Building higher-level semantic knowledge must be done by analogy
with similar domains, or by following examples.

Shneiderman suggests that novice manuals should then not be lists of the
application’s commands, but should provide examples of common high-level domain
goals, and first describe how to decompose these goals into lower-level semantic
tasks, before providing the solutions using commands.

He also frames the benefit of direct manipulation as leaning toward re-using
semantic knowledge, rather than asking the user to digest the arbitrary
syntactic knowledge of a commands language.

Example of systems exhibiting direct manipulation: (early) video games like
Pong, Breakout, Space Invaders, Donkey Kong; Visicalc; display editors (Emacs,
Vi).

***** Delta-oriented programming
[[cite:SBB+10][SBB+10]]

Describes delta of code; can remove methods, which is unusual for step-wise
programming.

***** Information transparency                                         :tool:
[[cite:Gri-01][Gri-01]]

Tools for capturing the similarity of code across modules.  Tangled code should
be similar, according to the principle of consistency.  Hence, capturing similar
code should help gather and organize concerns.  E.g., changing the behavior of
the parsing of a =while= statement by grepping for ‘while’ in the source.

Principle of consistency:

#+BEGIN_QUOTE
Things that look similar should /be/ similar; things that /are/ different should
look different.
#+END_QUOTE
[[cite:Mac-87][Mac-87]]

First principle of information transparency:

#+BEGIN_QUOTE
Code elements likely to be changed together as part of a complete, consistent
change should look similar, and code elements unlikely to be changed together
should look different.
#+END_QUOTE

If a code base obeys this principle, it can be easily refactored using standard
tools like grep.

A second principle promotes using variable names to indicate implementation
choices.  Hungarian notation is given as an example.

#+BEGIN_QUOTE
The unmodularized code elements relating to a changeable design decision should
contain recognizable tags uniquely identifying the design decision.
#+END_QUOTE

Locality can be managed by tools.  They exhibit tools a bit more powerful than
grep, with knowledge of the target language AST, or matching on typos.

#+BEGIN_QUOTE
Both tools [Aspect Browser and Seesoft] embody the concept that, by leveraging
the human visual system, identifiable /symbols/ are a viable alternative to
/locality/ as a way of managing changes to software.
#+END_QUOTE

***** JastAdd: extensible Java compiler
Used for writing “declarative” static flow analyses with reference attribute
grammars [[cite:SEH+13][SEH+13]].

Attribute grammars are already a declarative language for specifying the
semantics of a language [[cite:Knu-68][Knu-68]].

*** Code transformation
**** Semantic patches                                                  :tool:
[[cite:PLM-07][PLM-07]].  A solution to /collateral evolution/.  When a library function changes
name, or gains an argument, client code must makes the necessary changes.  The
changes in client code are collateral.

In a semantic patch, one describes the pattern of collateral changes needed to
adapt client code.

#+BEGIN_EXAMPLE
@ rule2 @
identifier proc_info_func;
identifier hostptr;
@@
proc_info_func (
+ struct Scsi_Host *hostptr,
- int hostno
) {
  ...
- struct Scsi_Host *hostptr;
  ...
- hostptr = scri_host_hn_get(hostno);
  ...
- if (!hostptr) { ... return ...; }
  ...
- scsi_host_put(hostptr);
  ...
}
#+END_EXAMPLE

Identifiers are declared in the header with a syntactic class.  They are matched
in the target code according to the context where they appear in the body of the
semantic path.

The dots =...= are an operator to match any sequence of code between two lines.
There is a mention of the dots matching the /control-flow/ of the code, though
nothing indicates that =spatch= interprets the target code in any way.

[[cite:JH-07][JH-07]] demystifies the tool by giving a denotational semantics.  Indeed, the dots
only match the syntax.

The related work section of [[cite:JH-07][JH-07]] has a few surveys on software evolution, and
in particular the Journal of Software Maintenance and Evolution.

All around a nice idea, though you still have to write the semantic patches from
scratch for every change.

The (unintended) idea of source transformation based on dynamic control flow is
interesting.  See [[Shapes of computation]].

***** Mechanisms for extension
It’s another approach, transforming code to alleviate the maintenance cost.

However, it’s a crutch.  We would prefer not having to have to make those
changes in the first place, even if the kernel libraries are updated.

The concept of /collateral evolution/ is certainly related.  When interpreters
evolve, collateral changes are needed on the analyses.  Previous work [[cite:PLM-06][PLM-06]] was
more focused on introducing the collateral evolution problem, with plenty of
examples from the Linux kernel.

** Core
Here I show, with extensive details, how one can build a modular interpreter
amenable to instrumentation in JavaScript.

The building blocks come from the background section, but as the problem is
slightly different, so are the way they are put together.  This leads to many
opportunities to discuss the benefits and downsides of each solution and the
mechanisms they leverage.

This part ends with an illustration of an interpreter capable of executing code
in different ways, by changing only a small amount of code each time.

I know I’m supposed to lay a straightforward narrative, but here I believe there
is equivalent insight to be gained from /failed/ experiments than from the
successes.  Negative results may not be publishable, but they can appear in a
thesis document.

Finally, I apply the insights gained from prototypes to Narcissus, showing that
they scale, and give examples with information flow instrumentation, showing
that they can be used for security purposes.

*** Building from scratch vs. extending existing interpreters
When we think of instrumentation, we think mostly of the latter.  The two
approaches map to mental models: see [[*Unanticipated%20extension%20is%20just%20a%20hack][Unanticipated extension is just a hack]].

Extending has its limits; it’s not the proper way to do things.

On the other hand, it’s unclear how to build modular interpreters amenable to
extension from building blocks.

*** Instrumenting the program vs. instrumenting the interpreter
Show how they are equivalent, with examples.  But not quite, since the
interpreter can do things the rewriting can’t.

Also, security concerns of leaking information to program if not careful, which
does not happen by instrumenting the interpreter, as they run in separate
environments.

However, interesting insight of full control over the AST given by treating code
as data.  Will pop up again later.

*** Dynamic scoping and global namespace (Lisp/JS)
[[cite:SS-78][SS-78]]

*** The aspect-oriented interpreter (Java/JS?)
*** Once again with types (Haskell/Scala)
*** Application to Narcissus

** Synthesis
Here I take another look at the solutions proposed in Core, and relate them to
existing work.  I take the time to discuss the higher-level schema behind all of
them and what this means for modularity.  I am explicit about the limits of the
approach, and what are the directions for improvement.

** Acknowledgements
Raganwald for a Game of Life implementation showing off literate programming and
AOP, and getting me interested in finding better ways to structure programs.

Bret Victor for the realization of the dissonance between textual programming
languages and the dynamic processes they describe.  Also, for inspiration.
